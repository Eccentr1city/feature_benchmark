{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getting_examples import *\n",
    "import pprint\n",
    "from predict_activations import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run an experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hyperparameters': {'binary_class': True,\n",
      "                     'debug': False,\n",
      "                     'neg_type': 'others',\n",
      "                     'num_completions': 1,\n",
      "                     'randomize_pos': False,\n",
      "                     'seed': 42,\n",
      "                     'show_max_token': True,\n",
      "                     'show_neg': 0,\n",
      "                     'show_pos': 0,\n",
      "                     'test_neg': 3,\n",
      "                     'test_pos': 3},\n",
      " 'num_features': 3,\n",
      " 'results': [{'description': 'timestamps or dates in a specific format',\n",
      "              'feature_index': 521,\n",
      "              'gpt_predictions': [(1, 0.0),\n",
      "                                  (1, 0.0),\n",
      "                                  (1, 0.0),\n",
      "                                  (0, 0.0),\n",
      "                                  (0, 0.0),\n",
      "                                  (0, 0.0)],\n",
      "              'highest_activation': 51.19043350219727,\n",
      "              'show_sentences': [],\n",
      "              'test_sentences': [{'max_token': ' posted',\n",
      "                                  'max_value': 'high',\n",
      "                                  'max_value_token_index': 8,\n",
      "                                  'sentence_string': ' health, '\n",
      "                                                     'australiaĊĊFirst '\n",
      "                                                     \"posted<|endoftext|>CARLTON'S \"\n",
      "                                                     'Dylan Buckley',\n",
      "                                  'tokens': [' health',\n",
      "                                             ',',\n",
      "                                             ' aust',\n",
      "                                             'ral',\n",
      "                                             'ia',\n",
      "                                             'Ċ',\n",
      "                                             'Ċ',\n",
      "                                             'First',\n",
      "                                             ' posted',\n",
      "                                             '<|endoftext|>',\n",
      "                                             'CAR',\n",
      "                                             'L',\n",
      "                                             'TON',\n",
      "                                             \"'\",\n",
      "                                             'S',\n",
      "                                             ' Dylan',\n",
      "                                             ' Buckley'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             51.19043350219727,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_token': ' posted',\n",
      "                                  'max_value': 'high',\n",
      "                                  'max_value_token_index': 8,\n",
      "                                  'sentence_string': ' sport, hondurasĊĊFirst '\n",
      "                                                     'posted<|endoftext|>Just '\n",
      "                                                     'after a midnight '\n",
      "                                                     'deadline that could',\n",
      "                                  'tokens': [' sport',\n",
      "                                             ',',\n",
      "                                             ' h',\n",
      "                                             'ond',\n",
      "                                             'uras',\n",
      "                                             'Ċ',\n",
      "                                             'Ċ',\n",
      "                                             'First',\n",
      "                                             ' posted',\n",
      "                                             '<|endoftext|>',\n",
      "                                             'Just',\n",
      "                                             ' after',\n",
      "                                             ' a',\n",
      "                                             ' midnight',\n",
      "                                             ' deadline',\n",
      "                                             ' that',\n",
      "                                             ' could'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             50.3966178894043,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_token': ' posted',\n",
      "                                  'max_value': 'high',\n",
      "                                  'max_value_token_index': 8,\n",
      "                                  'sentence_string': ', melbourne-3000ĊĊFirst '\n",
      "                                                     'posted<|endoftext|>You '\n",
      "                                                     'should know this about '\n",
      "                                                     'offensive line',\n",
      "                                  'tokens': [',',\n",
      "                                             ' mel',\n",
      "                                             'bourne',\n",
      "                                             '-',\n",
      "                                             '3000',\n",
      "                                             'Ċ',\n",
      "                                             'Ċ',\n",
      "                                             'First',\n",
      "                                             ' posted',\n",
      "                                             '<|endoftext|>',\n",
      "                                             'You',\n",
      "                                             ' should',\n",
      "                                             ' know',\n",
      "                                             ' this',\n",
      "                                             ' about',\n",
      "                                             ' offensive',\n",
      "                                             ' line'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             50.26014709472656,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_value': 'low',\n",
      "                                  'sentence_string': ' the disease remains '\n",
      "                                                     'largely unknown, '\n",
      "                                                     'genetic, viral, hormonal '\n",
      "                                                     'and environmental '\n",
      "                                                     'factors have been so',\n",
      "                                  'tokens': [' the',\n",
      "                                             ' disease',\n",
      "                                             ' remains',\n",
      "                                             ' largely',\n",
      "                                             ' unknown',\n",
      "                                             ',',\n",
      "                                             ' genetic',\n",
      "                                             ',',\n",
      "                                             ' viral',\n",
      "                                             ',',\n",
      "                                             ' hormonal',\n",
      "                                             ' and',\n",
      "                                             ' environmental',\n",
      "                                             ' factors',\n",
      "                                             ' have',\n",
      "                                             ' been',\n",
      "                                             ' so'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_value': 'low',\n",
      "                                  'sentence_string': ' of these are the ones '\n",
      "                                                     'associated with '\n",
      "                                                     'bacterial virulence ( 9 '\n",
      "                                                     'âĢĵ 11 ). The most',\n",
      "                                  'tokens': [' of',\n",
      "                                             ' these',\n",
      "                                             ' are',\n",
      "                                             ' the',\n",
      "                                             ' ones',\n",
      "                                             ' associated',\n",
      "                                             ' with',\n",
      "                                             ' bacterial',\n",
      "                                             ' vir',\n",
      "                                             'ulence',\n",
      "                                             ' (',\n",
      "                                             ' 9',\n",
      "                                             ' âĢĵ',\n",
      "                                             ' 11',\n",
      "                                             ' ).',\n",
      "                                             ' The',\n",
      "                                             ' most'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_value': 'low',\n",
      "                                  'sentence_string': ' height at some '\n",
      "                                                     'international '\n",
      "                                                     'summit.ĊĊOr is it '\n",
      "                                                     'because of these deeply '\n",
      "                                                     'controversial statements',\n",
      "                                  'tokens': [' height',\n",
      "                                             ' at',\n",
      "                                             ' some',\n",
      "                                             ' international',\n",
      "                                             ' summit',\n",
      "                                             '.',\n",
      "                                             'Ċ',\n",
      "                                             'Ċ',\n",
      "                                             'Or',\n",
      "                                             ' is',\n",
      "                                             ' it',\n",
      "                                             ' because',\n",
      "                                             ' of',\n",
      "                                             ' these',\n",
      "                                             ' deeply',\n",
      "                                             ' controversial',\n",
      "                                             ' statements'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]}]},\n",
      "             {'description': 'references to locations or events related to the '\n",
      "                             'state of Florida',\n",
      "              'feature_index': 737,\n",
      "              'gpt_predictions': [(1, 0.0),\n",
      "                                  (1, 1.0),\n",
      "                                  (1, 1.0),\n",
      "                                  (0, 0.0),\n",
      "                                  (0, 0.0),\n",
      "                                  (0, 0.0)],\n",
      "              'highest_activation': 26.77979850769043,\n",
      "              'show_sentences': [],\n",
      "              'test_sentences': [{'max_token': ' Flor',\n",
      "                                  'max_value': 'high',\n",
      "                                  'max_value_token_index': 8,\n",
      "                                  'sentence_string': ' like Rio, Sao Paulo, '\n",
      "                                                     'Salvador, Florianopolis '\n",
      "                                                     'and such. Try going to',\n",
      "                                  'tokens': [' like',\n",
      "                                             ' Rio',\n",
      "                                             ',',\n",
      "                                             ' Sao',\n",
      "                                             ' Paulo',\n",
      "                                             ',',\n",
      "                                             ' Salvador',\n",
      "                                             ',',\n",
      "                                             ' Flor',\n",
      "                                             'ian',\n",
      "                                             'opolis',\n",
      "                                             ' and',\n",
      "                                             ' such',\n",
      "                                             '.',\n",
      "                                             ' Try',\n",
      "                                             ' going',\n",
      "                                             ' to'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             26.77979850769043,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_token': ' Flor',\n",
      "                                  'max_value': 'high',\n",
      "                                  'max_value_token_index': 8,\n",
      "                                  'sentence_string': ' this because I went '\n",
      "                                                     'there recently with the '\n",
      "                                                     'Floridians, also '\n",
      "                                                     'known<|endoftext|> '\n",
      "                                                     'Deborah to',\n",
      "                                  'tokens': [' this',\n",
      "                                             ' because',\n",
      "                                             ' I',\n",
      "                                             ' went',\n",
      "                                             ' there',\n",
      "                                             ' recently',\n",
      "                                             ' with',\n",
      "                                             ' the',\n",
      "                                             ' Flor',\n",
      "                                             'id',\n",
      "                                             'ians',\n",
      "                                             ',',\n",
      "                                             ' also',\n",
      "                                             ' known',\n",
      "                                             '<|endoftext|>',\n",
      "                                             ' Deborah',\n",
      "                                             ' to'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             26.34127616882324,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_token': ' Flor',\n",
      "                                  'max_value': 'high',\n",
      "                                  'max_value_token_index': 8,\n",
      "                                  'sentence_string': ' so this trip was about '\n",
      "                                                     'making sure the '\n",
      "                                                     'Floridians saw as much '\n",
      "                                                     'as possible in',\n",
      "                                  'tokens': [' so',\n",
      "                                             ' this',\n",
      "                                             ' trip',\n",
      "                                             ' was',\n",
      "                                             ' about',\n",
      "                                             ' making',\n",
      "                                             ' sure',\n",
      "                                             ' the',\n",
      "                                             ' Flor',\n",
      "                                             'id',\n",
      "                                             'ians',\n",
      "                                             ' saw',\n",
      "                                             ' as',\n",
      "                                             ' much',\n",
      "                                             ' as',\n",
      "                                             ' possible',\n",
      "                                             ' in'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             25.96149253845215,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_value': 'low',\n",
      "                                  'sentence_string': ' the disease remains '\n",
      "                                                     'largely unknown, '\n",
      "                                                     'genetic, viral, hormonal '\n",
      "                                                     'and environmental '\n",
      "                                                     'factors have been so',\n",
      "                                  'tokens': [' the',\n",
      "                                             ' disease',\n",
      "                                             ' remains',\n",
      "                                             ' largely',\n",
      "                                             ' unknown',\n",
      "                                             ',',\n",
      "                                             ' genetic',\n",
      "                                             ',',\n",
      "                                             ' viral',\n",
      "                                             ',',\n",
      "                                             ' hormonal',\n",
      "                                             ' and',\n",
      "                                             ' environmental',\n",
      "                                             ' factors',\n",
      "                                             ' have',\n",
      "                                             ' been',\n",
      "                                             ' so'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_value': 'low',\n",
      "                                  'sentence_string': ' of these are the ones '\n",
      "                                                     'associated with '\n",
      "                                                     'bacterial virulence ( 9 '\n",
      "                                                     'âĢĵ 11 ). The most',\n",
      "                                  'tokens': [' of',\n",
      "                                             ' these',\n",
      "                                             ' are',\n",
      "                                             ' the',\n",
      "                                             ' ones',\n",
      "                                             ' associated',\n",
      "                                             ' with',\n",
      "                                             ' bacterial',\n",
      "                                             ' vir',\n",
      "                                             'ulence',\n",
      "                                             ' (',\n",
      "                                             ' 9',\n",
      "                                             ' âĢĵ',\n",
      "                                             ' 11',\n",
      "                                             ' ).',\n",
      "                                             ' The',\n",
      "                                             ' most'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_value': 'low',\n",
      "                                  'sentence_string': ' height at some '\n",
      "                                                     'international '\n",
      "                                                     'summit.ĊĊOr is it '\n",
      "                                                     'because of these deeply '\n",
      "                                                     'controversial statements',\n",
      "                                  'tokens': [' height',\n",
      "                                             ' at',\n",
      "                                             ' some',\n",
      "                                             ' international',\n",
      "                                             ' summit',\n",
      "                                             '.',\n",
      "                                             'Ċ',\n",
      "                                             'Ċ',\n",
      "                                             'Or',\n",
      "                                             ' is',\n",
      "                                             ' it',\n",
      "                                             ' because',\n",
      "                                             ' of',\n",
      "                                             ' these',\n",
      "                                             ' deeply',\n",
      "                                             ' controversial',\n",
      "                                             ' statements'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]}]},\n",
      "             {'description': 'mentions of the name \"Christian\" along with '\n",
      "                             'various contexts like sports, events, and other '\n",
      "                             'individuals',\n",
      "              'feature_index': 740,\n",
      "              'gpt_predictions': [(1, 1.0),\n",
      "                                  (1, 1.0),\n",
      "                                  (1, 1.0),\n",
      "                                  (0, 0.0),\n",
      "                                  (0, 0.0),\n",
      "                                  (0, 0.0)],\n",
      "              'highest_activation': 33.55897903442383,\n",
      "              'show_sentences': [],\n",
      "              'test_sentences': [{'max_token': 'Christian',\n",
      "                                  'max_value': 'high',\n",
      "                                  'max_value_token_index': 8,\n",
      "                                  'sentence_string': ' (Brad Evans 69), Andy '\n",
      "                                                     'Rose (Christian Tiffert '\n",
      "                                                     '69), Mauro Rosales',\n",
      "                                  'tokens': [' (',\n",
      "                                             'Brad',\n",
      "                                             ' Evans',\n",
      "                                             ' 69',\n",
      "                                             '),',\n",
      "                                             ' Andy',\n",
      "                                             ' Rose',\n",
      "                                             ' (',\n",
      "                                             'Christian',\n",
      "                                             ' Tiff',\n",
      "                                             'ert',\n",
      "                                             ' 69',\n",
      "                                             '),',\n",
      "                                             ' Mau',\n",
      "                                             'ro',\n",
      "                                             ' Ros',\n",
      "                                             'ales'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             33.55897903442383,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_token': 'Christian',\n",
      "                                  'max_value': 'high',\n",
      "                                  'max_value_token_index': 8,\n",
      "                                  'sentence_string': ' a scene in which '\n",
      "                                                     'Patrick Bateman '\n",
      "                                                     '(Christian Bale) gives a '\n",
      "                                                     'critique of the Hue',\n",
      "                                  'tokens': [' a',\n",
      "                                             ' scene',\n",
      "                                             ' in',\n",
      "                                             ' which',\n",
      "                                             ' Patrick',\n",
      "                                             ' Bat',\n",
      "                                             'eman',\n",
      "                                             ' (',\n",
      "                                             'Christian',\n",
      "                                             ' Bale',\n",
      "                                             ')',\n",
      "                                             ' gives',\n",
      "                                             ' a',\n",
      "                                             ' critique',\n",
      "                                             ' of',\n",
      "                                             ' the',\n",
      "                                             ' Hue'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             30.13565444946289,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_token': 'Christian',\n",
      "                                  'max_value': 'high',\n",
      "                                  'max_value_token_index': 8,\n",
      "                                  'sentence_string': ' Stanley Cup title in '\n",
      "                                                     'next.ĊĊChristian Ehrhoff '\n",
      "                                                     'scored at 2:41',\n",
      "                                  'tokens': [' Stanley',\n",
      "                                             ' Cup',\n",
      "                                             ' title',\n",
      "                                             ' in',\n",
      "                                             ' next',\n",
      "                                             '.',\n",
      "                                             'Ċ',\n",
      "                                             'Ċ',\n",
      "                                             'Christian',\n",
      "                                             ' E',\n",
      "                                             'hr',\n",
      "                                             'hoff',\n",
      "                                             ' scored',\n",
      "                                             ' at',\n",
      "                                             ' 2',\n",
      "                                             ':',\n",
      "                                             '41'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             29.58237266540527,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_value': 'low',\n",
      "                                  'sentence_string': ' the disease remains '\n",
      "                                                     'largely unknown, '\n",
      "                                                     'genetic, viral, hormonal '\n",
      "                                                     'and environmental '\n",
      "                                                     'factors have been so',\n",
      "                                  'tokens': [' the',\n",
      "                                             ' disease',\n",
      "                                             ' remains',\n",
      "                                             ' largely',\n",
      "                                             ' unknown',\n",
      "                                             ',',\n",
      "                                             ' genetic',\n",
      "                                             ',',\n",
      "                                             ' viral',\n",
      "                                             ',',\n",
      "                                             ' hormonal',\n",
      "                                             ' and',\n",
      "                                             ' environmental',\n",
      "                                             ' factors',\n",
      "                                             ' have',\n",
      "                                             ' been',\n",
      "                                             ' so'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_value': 'low',\n",
      "                                  'sentence_string': ' of these are the ones '\n",
      "                                                     'associated with '\n",
      "                                                     'bacterial virulence ( 9 '\n",
      "                                                     'âĢĵ 11 ). The most',\n",
      "                                  'tokens': [' of',\n",
      "                                             ' these',\n",
      "                                             ' are',\n",
      "                                             ' the',\n",
      "                                             ' ones',\n",
      "                                             ' associated',\n",
      "                                             ' with',\n",
      "                                             ' bacterial',\n",
      "                                             ' vir',\n",
      "                                             'ulence',\n",
      "                                             ' (',\n",
      "                                             ' 9',\n",
      "                                             ' âĢĵ',\n",
      "                                             ' 11',\n",
      "                                             ' ).',\n",
      "                                             ' The',\n",
      "                                             ' most'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_value': 'low',\n",
      "                                  'sentence_string': ' height at some '\n",
      "                                                     'international '\n",
      "                                                     'summit.ĊĊOr is it '\n",
      "                                                     'because of these deeply '\n",
      "                                                     'controversial statements',\n",
      "                                  'tokens': [' height',\n",
      "                                             ' at',\n",
      "                                             ' some',\n",
      "                                             ' international',\n",
      "                                             ' summit',\n",
      "                                             '.',\n",
      "                                             'Ċ',\n",
      "                                             'Ċ',\n",
      "                                             'Or',\n",
      "                                             ' is',\n",
      "                                             ' it',\n",
      "                                             ' because',\n",
      "                                             ' of',\n",
      "                                             ' these',\n",
      "                                             ' deeply',\n",
      "                                             ' controversial',\n",
      "                                             ' statements'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]}]}],\n",
      " 'timestamp': 1715377220.0112782}\n"
     ]
    }
   ],
   "source": [
    "with open(f'feats.json', 'r') as file:\n",
    "    feature_data = json.load(file)\n",
    "\n",
    "results = run_experiments(\n",
    "    num_features=3, \n",
    "    feature_data=feature_data,\n",
    "    test_pos=3, # Experiment with\n",
    "    test_neg=3, # Experiment with\n",
    "    show_pos=0, # Experiment with\n",
    "    show_neg=0, # Experiment with\n",
    "    neg_type='others', # Experiment with\n",
    "    binary_class=True, # Experiment with\n",
    "    show_max_token=True, # Experiment with\n",
    "    num_completions=1, # Experiment with\n",
    "    debug=False, \n",
    "    randomize_pos=False, \n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "# the run_experiments function automatically saves results to results/exp_{timestamp}.json\n",
    "pprint.pprint(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a past result file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hyperparameters': {'binary_class': True,\n",
      "                     'debug': False,\n",
      "                     'neg_type': 'others',\n",
      "                     'num_completions': 1,\n",
      "                     'randomize_pos': True,\n",
      "                     'seed': 42,\n",
      "                     'show_max_token': False,\n",
      "                     'show_neg': 0,\n",
      "                     'show_pos': 0,\n",
      "                     'test_neg': 3,\n",
      "                     'test_pos': 3},\n",
      " 'num_features': 1,\n",
      " 'results': [{'description': 'phrases related to direct confrontation or '\n",
      "                             'comparison',\n",
      "              'feature_index': 3111,\n",
      "              'gpt_predictions': [[1, 0.0],\n",
      "                                  [1, 0.0],\n",
      "                                  [1, 1.0],\n",
      "                                  [0, 0.0],\n",
      "                                  [0, 0.0],\n",
      "                                  [0, 0.0]],\n",
      "              'highest_activation': 44.37568664550781,\n",
      "              'show_sentences': [],\n",
      "              'test_sentences': [{'max_token': '-',\n",
      "                                  'max_value': 'high',\n",
      "                                  'max_value_token_index': 8,\n",
      "                                  'sentence_string': ' go with Bisping in a '\n",
      "                                                     'point-fighting '\n",
      "                                                     'situation. But I can see '\n",
      "                                                     'G',\n",
      "                                  'tokens': [' go',\n",
      "                                             ' with',\n",
      "                                             ' B',\n",
      "                                             'isp',\n",
      "                                             'ing',\n",
      "                                             ' in',\n",
      "                                             ' a',\n",
      "                                             ' point',\n",
      "                                             '-',\n",
      "                                             'fighting',\n",
      "                                             ' situation',\n",
      "                                             '.',\n",
      "                                             ' But',\n",
      "                                             ' I',\n",
      "                                             ' can',\n",
      "                                             ' see',\n",
      "                                             ' G'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             33.91120910644531,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_token': '-',\n",
      "                                  'max_value': 'high',\n",
      "                                  'max_value_token_index': 8,\n",
      "                                  'sentence_string': ' doesn<|endoftext|> huge '\n",
      "                                                     'market share in '\n",
      "                                                     \"'point-of-care rapid \"\n",
      "                                                     'tests used in hospitals',\n",
      "                                  'tokens': [' doesn',\n",
      "                                             '<|endoftext|>',\n",
      "                                             ' huge',\n",
      "                                             ' market',\n",
      "                                             ' share',\n",
      "                                             ' in',\n",
      "                                             \" '\",\n",
      "                                             'point',\n",
      "                                             '-',\n",
      "                                             'of',\n",
      "                                             '-',\n",
      "                                             'care',\n",
      "                                             ' rapid',\n",
      "                                             ' tests',\n",
      "                                             ' used',\n",
      "                                             ' in',\n",
      "                                             ' hospitals'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             2.900025844573975,\n",
      "                                             39.18094635009766,\n",
      "                                             0,\n",
      "                                             7.738319396972656,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_token': '-',\n",
      "                                  'max_value': 'high',\n",
      "                                  'max_value_token_index': 8,\n",
      "                                  'sentence_string': ', as<|endoftext|> the '\n",
      "                                                     'Sun Devils go '\n",
      "                                                     'head-to-head with the '\n",
      "                                                     'Blue Devils of',\n",
      "                                  'tokens': [',',\n",
      "                                             ' as',\n",
      "                                             '<|endoftext|>',\n",
      "                                             ' the',\n",
      "                                             ' Sun',\n",
      "                                             ' Devils',\n",
      "                                             ' go',\n",
      "                                             ' head',\n",
      "                                             '-',\n",
      "                                             'to',\n",
      "                                             '-',\n",
      "                                             'head',\n",
      "                                             ' with',\n",
      "                                             ' the',\n",
      "                                             ' Blue',\n",
      "                                             ' Devils',\n",
      "                                             ' of'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             33.09887313842773,\n",
      "                                             0,\n",
      "                                             1.359877228736877,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_value': 'low',\n",
      "                                  'sentence_string': 'Ċ1 cup waterĊĊ1 cup '\n",
      "                                                     'milkĊĊ1 teaspoon saltĊĊ5',\n",
      "                                  'tokens': ['Ċ',\n",
      "                                             '1',\n",
      "                                             ' cup',\n",
      "                                             ' water',\n",
      "                                             'Ċ',\n",
      "                                             'Ċ',\n",
      "                                             '1',\n",
      "                                             ' cup',\n",
      "                                             ' milk',\n",
      "                                             'Ċ',\n",
      "                                             'Ċ',\n",
      "                                             '1',\n",
      "                                             ' teaspoon',\n",
      "                                             ' salt',\n",
      "                                             'Ċ',\n",
      "                                             'Ċ',\n",
      "                                             '5'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_value': 'low',\n",
      "                                  'sentence_string': ': a six-month '\n",
      "                                                     'jet-setting junket of '\n",
      "                                                     'television and dance '\n",
      "                                                     'studios, A',\n",
      "                                  'tokens': [':',\n",
      "                                             ' a',\n",
      "                                             ' six',\n",
      "                                             '-',\n",
      "                                             'month',\n",
      "                                             ' jet',\n",
      "                                             '-',\n",
      "                                             'setting',\n",
      "                                             ' junk',\n",
      "                                             'et',\n",
      "                                             ' of',\n",
      "                                             ' television',\n",
      "                                             ' and',\n",
      "                                             ' dance',\n",
      "                                             ' studios',\n",
      "                                             ',',\n",
      "                                             ' A'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_value': 'low',\n",
      "                                  'sentence_string': 'owment.ĊĊBut U.S. '\n",
      "                                                     'interest in Chinese has '\n",
      "                                                     'fallen precipitously',\n",
      "                                  'tokens': ['owment',\n",
      "                                             '.',\n",
      "                                             'Ċ',\n",
      "                                             'Ċ',\n",
      "                                             'But',\n",
      "                                             ' U',\n",
      "                                             '.',\n",
      "                                             'S',\n",
      "                                             '.',\n",
      "                                             ' interest',\n",
      "                                             ' in',\n",
      "                                             ' Chinese',\n",
      "                                             ' has',\n",
      "                                             ' fallen',\n",
      "                                             ' precip',\n",
      "                                             'it',\n",
      "                                             'ously'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]}]}],\n",
      " 'timestamp': 1715322008.339694}\n"
     ]
    }
   ],
   "source": [
    "json_data = load_json_results('results/exp_1715322008.339694.json')\n",
    "pprint.pprint(json_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get a simpler idea by printing the json tree (or just by opening a results file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{\n",
      "hyperparameters\n",
      "    {\n",
      "    test_pos\n",
      "    test_neg\n",
      "    show_pos\n",
      "    show_neg\n",
      "    binary_class\n",
      "    neg_type\n",
      "    show_max_token\n",
      "    num_completions\n",
      "    debug\n",
      "    randomize_pos\n",
      "    seed\n",
      "    }\n",
      "\n",
      "num_features\n",
      "results []\n",
      "        {\n",
      "        feature_index\n",
      "        gpt_predictions []\n",
      "                .\n",
      "                .\n",
      "                .\n",
      "            .\n",
      "            .\n",
      "            .\n",
      "        description\n",
      "        test_sentences []\n",
      "                {\n",
      "                max_value\n",
      "                max_value_token_index\n",
      "                sentence_string\n",
      "                max_token\n",
      "                tokens []\n",
      "                    .\n",
      "                    .\n",
      "                    .\n",
      "                values []\n",
      "                    .\n",
      "                    .\n",
      "                    .\n",
      "                }\n",
      "\n",
      "            .\n",
      "            .\n",
      "            .\n",
      "        show_sentences []\n",
      "        highest_activation\n",
      "        }\n",
      "\n",
      "    .\n",
      "    .\n",
      "    .\n",
      "timestamp\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print_json_tree(json_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do analysis on loaded json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_json_results(fetch_feature_data(1), 'feat1.json')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Older things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "predict_activations() got an unexpected keyword argument 'test_number'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# custom = [custom_accuracy(data, eps = 0.1) for data in all_data]\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m---> 19\u001b[0m     run()\n",
      "Cell \u001b[0;32mIn[4], line 6\u001b[0m, in \u001b[0;36mrun\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m():\n\u001b[0;32m----> 6\u001b[0m     data \u001b[38;5;241m=\u001b[39m get_predictions(\u001b[38;5;241m991\u001b[39m) \u001b[38;5;66;03m#806\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# for i in range(len(all_data)):\u001b[39;00m\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;66;03m# data = all_data[i]\u001b[39;00m\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;66;03m# print(feature_nums[i])\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m()\n",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m, in \u001b[0;36mget_predictions\u001b[0;34m(feature_num)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_predictions\u001b[39m(feature_num):\n\u001b[0;32m----> 2\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m predict_activations(feature_num, test_number\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, show_examples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m)\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m predictions\n",
      "\u001b[0;31mTypeError\u001b[0m: predict_activations() got an unexpected keyword argument 'test_number'"
     ]
    }
   ],
   "source": [
    "def get_predictions(feature_num):\n",
    "    predictions = predict_activations(feature_num, test_number=10, show_examples=8)\n",
    "    return predictions\n",
    "\n",
    "def run():\n",
    "    data = get_predictions(991) #806\n",
    "    # for i in range(len(all_data)):\n",
    "        # data = all_data[i]\n",
    "        # print(feature_nums[i])\n",
    "    print()\n",
    "    pprint.pprint(data)\n",
    "    custom = custom_accuracy(data)\n",
    "\n",
    "    print(custom)\n",
    "\n",
    "    # custom = [custom_accuracy(data, eps = 0.1) for data in all_data]\n",
    "\n",
    "for _ in range(1):\n",
    "    run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "### Losses\n",
    "def mse(data, normalize = False):\n",
    "    values = ([((elem[0]-elem[1])/(elem[0] if normalize else 1))**2 for elem in data])\n",
    "    return sum(values)/len(values)\n",
    "\n",
    "def nll_variant(data, eps = 1e-1):\n",
    "    values = ([np.log((min(elem) + eps)/(max(elem) + eps)) for elem in data])\n",
    "    return -sum(values)/len(values)\n",
    "\n",
    "def l1(data, normalize = True, eps = 0.1):\n",
    "    values = ([((eps + abs(elem[0]-elem[1]))/((max(elem) if normalize else 1) + eps))  for elem in data])\n",
    "    return sum(values)/len(values)\n",
    "\n",
    "### Plots\n",
    "def plot_mses_cdf(mses):\n",
    "    # Plotting the Mean Squared Errors (MSE) for each dataset\n",
    "    mses_sorted = np.sort(mses)\n",
    "    cdf = np.arange(1, len(mses_sorted)+1) / len(mses_sorted)\n",
    "    plt.plot(mses_sorted, cdf)\n",
    "    plt.title('Cumulative Distribution Function of MSEs')\n",
    "    plt.xlabel('MSE')\n",
    "    plt.ylabel('CDF')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def plot_probability_distribution(data, bins='auto', density=True, title = \"Default Title\"):\n",
    "    \"\"\"\n",
    "    Plots the probability distribution of the given data using a histogram.\n",
    "\n",
    "    Parameters:\n",
    "    - data (list or numpy array): The floating point numbers whose distribution you want to plot.\n",
    "    - bins (int, sequence or str, optional): The method for calculating histogram bins. Default is 'auto'.\n",
    "    - density (bool, optional): If True, the histogram is normalized to form a probability density,\n",
    "                                i.e., the area under the histogram will sum to 1. Default is True.\n",
    "    \"\"\"\n",
    "    # Calculate the histogram\n",
    "    counts, bin_edges = np.histogram(data, bins=bins, density=density)\n",
    "\n",
    "    # Calculate bin centers\n",
    "    bin_centers = 0.5 * (bin_edges[:-1] + bin_edges[1:])\n",
    "\n",
    "    # Plotting the histogram\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.bar(bin_centers, counts*np.diff(bin_edges), align='center', width=np.diff(bin_edges), edgecolor='black', alpha=0.7)\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Probability Density')\n",
    "    plt.title('Probability Distribution of Data')\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def analyze_data(all_data):\n",
    "    mses = [mse(data, normalize = False) for data in all_data]\n",
    "    nlls = [nll_variant(data) for data in all_data]\n",
    "    l1s = [l1(data, normalize = True) for data in all_data]\n",
    "\n",
    "    print('l1s', sorted(l1s))\n",
    "    plot_probability_distribution(mses, title = \"Distribution of MSEs\")\n",
    "    plot_probability_distribution(nlls, title = \"Distribution of NLL variant\")\n",
    "    plot_probability_distribution(l1s, title = \"Distribution of l1s variant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'concurrent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[74], line 9\u001b[0m\n\u001b[1;32m      5\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m predict_activations(feature_num, test_number\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, show_examples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m)\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m predictions\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m concurrent\u001b[38;5;241m.\u001b[39mfutures\u001b[38;5;241m.\u001b[39mThreadPoolExecutor() \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[1;32m     10\u001b[0m     all_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(executor\u001b[38;5;241m.\u001b[39mmap(get_predictions, feature_nums))\n\u001b[1;32m     12\u001b[0m mses \u001b[38;5;241m=\u001b[39m [mse(data, normalize \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m all_data]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'concurrent' is not defined"
     ]
    }
   ],
   "source": [
    "feature_nums = [806]#random.sample(range(0, 1000), 10)\n",
    "\n",
    "def get_predictions(feature_num):\n",
    "    predictions = predict_activations(feature_num, test_number=10, show_examples=8)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    all_data = list(executor.map(get_predictions, feature_nums))\n",
    "\n",
    "mses = [mse(data, normalize = False) for data in all_data]\n",
    "nlls = [nll_variant(data) for data in all_data]\n",
    "l1s = [l1(data, normalize = True) for data in all_data]\n",
    "\n",
    "# print('l1s', sorted(l1s))\n",
    "plot_probability_distribution(mses, title = \"Distribution of MSEs\")\n",
    "plot_probability_distribution(nlls, title = \"Distribution of NLL variant\")\n",
    "plot_probability_distribution(l1s, title = \"Distribution of l1s variant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_accuracy(data):\n",
    "    eps = max([elem[0] for elem in data]) / 10\n",
    "    values = []\n",
    "    for elem in data:\n",
    "        true, pred = elem\n",
    "        ## Add eps to avoid zero case\n",
    "        true, pred = true + eps, pred + eps\n",
    "        # Scale values\n",
    "        true, pred = true ** 0.75, pred ** 0.75\n",
    "        # Calculate difference\n",
    "        difference = abs(true - pred)\n",
    "        # Take ratio\n",
    "        error = difference / max(true, pred)\n",
    "        \n",
    "        accuracy = 1 - error\n",
    "        values.append(accuracy)\n",
    "    return sum(values)/len(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'feature_nums' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m feature_nums\n",
      "\u001b[0;31mNameError\u001b[0m: name 'feature_nums' is not defined"
     ]
    }
   ],
   "source": [
    "feature_nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('You are evaluating an english description of an autoencoder feature. The '\n",
      " 'description should correspond to sentences which result in high activation. '\n",
      " 'The english description of the feature is: \" past tense verbs\"\\n'\n",
      " 'Here are 8 examples of sentences and their corresponding activations:\\n'\n",
      " ' Example: \" economy\\'s cooled off enough, but it wasn\\'t always so. Back in '\n",
      " 'the mid\", Activation: 19.96\\n'\n",
      " 'Example: \" NL<|endoftext|>,\" Watts said.ĊĊRubio\\'s disclosure sheds new '\n",
      " 'light on his\", Activation: 0.00\\n'\n",
      " 'Example: \" in their NL<|endoftext|>,\" Watts said.ĊĊRubio\\'s disclosure sheds '\n",
      " 'new light\", Activation: 0.00\\n'\n",
      " 'Example: \"ĊĊRubio\\'s disclosure sheds new light on his comments in October, '\n",
      " 'when he\", Activation: 0.00\\n'\n",
      " 'Example: \" be sure to add a great feel and glitz to any game. These '\n",
      " 'wonderful futuristic\", Activation: 0.00\\n'\n",
      " 'Example: \" their NL<|endoftext|>,\" Watts said.ĊĊRubio\\'s disclosure sheds '\n",
      " 'new light on\", Activation: 0.00\\n'\n",
      " 'Example: \" of California, proved that prawns eat the mollusc hosts. He '\n",
      " 'shared\", Activation: 0.00\\n'\n",
      " 'Example: \" songs,\" Willow said. \"These songs were created through characters '\n",
      " 'I have developed within my\", Activation: 16.54\\n'\n",
      " 'Use the provided samples and the provided description to predict the '\n",
      " 'activation on a new sentence.\\n'\n",
      " 'You MUST respond with ONLY a number and NO OTHER content.')\n",
      "[{'activation': 21.34770965576172,\n",
      "  'max_index': 8,\n",
      "  'sentence_string': ' the surface area of the land. This was the allocation '\n",
      "                     'that was specified in the plan'},\n",
      " {'activation': 21.08387565612793,\n",
      "  'max_index': 8,\n",
      "  'sentence_string': \" to their online bordcasting but they didn't treated \"\n",
      "                     'well who play in the NASL'},\n",
      " {'activation': 15.00903415679932,\n",
      "  'max_index': 8,\n",
      "  'sentence_string': ' industry for add-ons to applications that were not '\n",
      "                     'meant to be extendible.<|endoftext|>'},\n",
      " {'activation': 0,\n",
      "  'max_index': 0,\n",
      "  'sentence_string': ' Central American folklore. It is a shape-changing '\n",
      "                     'spirit that typically takes the form of'},\n",
      " {'activation': 24.55836486816406,\n",
      "  'max_index': 8,\n",
      "  'sentence_string': ' Great Value, and other brands. It was John Henry Heinz, '\n",
      "                     'in fact,'},\n",
      " {'activation': 0,\n",
      "  'max_index': 0,\n",
      "  'sentence_string': ' comments in October, when he warned his fellow '\n",
      "                     'Republicans against trying to make political hay out'},\n",
      " {'activation': 0,\n",
      "  'max_index': 0,\n",
      "  'sentence_string': \"Rubio's disclosure sheds new light on his comments in \"\n",
      "                     'October, when he warned his'},\n",
      " {'activation': 0,\n",
      "  'max_index': 0,\n",
      "  'sentence_string': ' sheds new light on his comments in October, when he '\n",
      "                     'warned his fellow Republicans against trying'},\n",
      " {'activation': 0,\n",
      "  'max_index': 0,\n",
      "  'sentence_string': \"ĊRubio's disclosure sheds new light on his comments in \"\n",
      "                     'October, when he warned'},\n",
      " {'activation': 12.68032932281494,\n",
      "  'max_index': 8,\n",
      "  'sentence_string': ' disrupt the discourse with other people, who were '\n",
      "                     'either supportive or who were neutral and just'}]\n",
      "[(21.34770965576172, 0.0),\n",
      " (21.08387565612793, 0.0),\n",
      " (15.00903415679932, 0.0),\n",
      " (0, 0.0),\n",
      " (24.55836486816406, 0.0),\n",
      " (0, 15.23),\n",
      " (0, 0.0),\n",
      " (0, 0.0),\n",
      " (0, 0.0),\n",
      " (12.68032932281494, 0.0)]\n",
      "0.4124388582827179\n"
     ]
    }
   ],
   "source": [
    "def run():\n",
    "    data = get_predictions(806)\n",
    "    # for i in range(len(all_data)):\n",
    "        # data = all_data[i]\n",
    "        # print(feature_nums[i])\n",
    "    pprint.pprint(data)\n",
    "    custom = custom_accuracy(data)\n",
    "    print(custom)\n",
    "\n",
    "    # custom = [custom_accuracy(data, eps = 0.1) for data in all_data]\n",
    "\n",
    "for _ in range(1):\n",
    "    run()\n",
    "\n",
    "# plot_probability_distribution(custom, title = \"Distribution of custom accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[80], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m analyze_data(all_data)\n",
      "File \u001b[0;32m~/Desktop/GitHub/feature_benchmark/prediction_analysis.py:57\u001b[0m, in \u001b[0;36manalyze_data\u001b[0;34m(all_data)\u001b[0m\n",
      "File \u001b[0;32m~/Desktop/GitHub/feature_benchmark/prediction_analysis.py:7\u001b[0m, in \u001b[0;36mmse\u001b[0;34m(data, normalize)\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "analyze_data(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
