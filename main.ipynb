{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'prediction_analysis'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mprediction_analysis\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgetting_examples\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_activation_data_for_feature\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mconcurrent\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfutures\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'prediction_analysis'"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from prediction_analysis import *\n",
    "from getting_examples import get_activation_data_for_feature\n",
    "import concurrent.futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "import re\n",
    "import pprint\n",
    "from getting_examples import get_activation_data_for_feature\n",
    "\n",
    "def find_first_number(text):\n",
    "    # Return the first number in a string\n",
    "    match = re.search(r'\\b\\d+(\\.\\d+)?', text)\n",
    "    return float(match.group(0)) if match else None\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "def predict_activations(feature_index, test_number=20, show_examples=0):\n",
    "    # Get and parse JSON data from the url corresponding to the requested feature\n",
    "    url = f\"https://www.neuronpedia.org/api/feature/gpt2-small/9-res-jb/{feature_index}\"\n",
    "    data = get_activation_data_for_feature(url)\n",
    "    explanation = data['explanations'][0]['description']\n",
    "\n",
    "    assert (len(data['examples']) >= (test_number + show_examples))\n",
    "\n",
    "    # Randomly select some sentences to use as examples and test data\n",
    "    random_indices = np.random.choice(len(data['examples']), size=test_number + show_examples, replace=False)\n",
    "    sentences = [\n",
    "        {\n",
    "            'sentence_string': ''.join(data['examples'][i]['tokens']), \n",
    "            'activation':  data['examples'][i]['maxValue'], \n",
    "            'max_index': data['examples'][i]['maxValueTokenIndex'],\n",
    "            'max_token': data['examples'][i]['tokens'][data['examples'][i]['maxValueTokenIndex']]\n",
    "         } \n",
    "        for i in random_indices]\n",
    "    example_sentences = sentences[:show_examples]\n",
    "    test_sentences = sentences[show_examples:] \n",
    "\n",
    "    highest_activation = data['examples'][0]['maxValue']\n",
    "\n",
    "    # Create a system prompt dependning on how many example sentences are provided\n",
    "    # system_prompt = f'You are evaluating an english description of an autoencoder feature. The description should correspond to sentences which result in high activation. The english description of the feature is: \"{explanation}\"\\n'\n",
    "    system_prompt = f'You are evaluating an english description of an autoencoder feature. The description should correspond to sentences which result in high activation. The english description of the feature is: \"{explanation}\"\\n'\n",
    "\n",
    "    if show_examples:\n",
    "        system_prompt += f'Here are {show_examples} examples of sentences and their corresponding activations:\\n '\n",
    "        for sentence in example_sentences:\n",
    "            sentence_string = sentence['sentence_string']\n",
    "            activation = sentence['activation']\n",
    "            system_prompt += f'Example: \"{sentence_string}\" had an activation of {activation:.2f} on token \"{sentence[\"max_token\"]}\"\\n'\n",
    "        system_prompt += 'Use the provided samples and the provided description to predict the activation on a new sentence.'\n",
    "\n",
    "    else:\n",
    "        system_prompt += f'The value of the highest activation on the dataset is {highest_activation:.2f}. You must predict the activation on a new sentence based off of the provided description. If the description matches the provided sentence, the activation may be closer to {highest_activation:.2f}, while if it does not match the activation will be nearly 0.'\n",
    "\n",
    "    system_prompt += '\\nYou MUST respond with ONLY a number and NO OTHER content.'\n",
    "\n",
    "    predictions = []\n",
    "\n",
    "    # Have the model predict activations on each test sentence\n",
    "    for sentence in test_sentences:\n",
    "        sentence_string = sentence['sentence_string']\n",
    "        user_message = f'Please predict the activation on this sentence, responding with a number between 0 and {highest_activation:.2f}.\\n\\nSentence: \"{sentence_string}\" Remember, the english description of the feature is: \"{explanation}\" and the most common words that resulted in a high activation were {\", \".join([\"\\\"\" + s[\"max_token\"] + \"\\\"\" for s in example_sentences])}'\n",
    "        print(user_message)\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_message}\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        predicted = find_first_number(completion.choices[0].message.content)\n",
    "        # (true, pred)\n",
    "        predictions.append((sentence['activation'], predicted))\n",
    "\n",
    "    pprint.pprint(system_prompt)\n",
    "    # pprint.pprint(test_sentences)\n",
    "\n",
    "    for i in range(len(predictions)):\n",
    "        print(predictions[i], test_sentences[i]['sentence_string'])\n",
    "\n",
    "    return predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please predict the activation on this sentence, responding with a number between 0 and 7.48.\n",
      "\n",
      "Sentence: \" the words \"I'll set this plane on fire\" and a seat number.Ċ\" Remember, the english description of the feature is: \"words related to technical processes or actions\" and the most common words that resulted in a high activation were \" on\", \" and\", \" If\", \" is\", \" a\", \"'s\", \" knowledge\", \" to\"\n",
      "Please predict the activation on this sentence, responding with a number between 0 and 7.48.\n",
      "\n",
      "Sentence: \",\" Watts said.ĊĊRubio's disclosure sheds new light on his comments in\" Remember, the english description of the feature is: \"words related to technical processes or actions\" and the most common words that resulted in a high activation were \" on\", \" and\", \" If\", \" is\", \" a\", \"'s\", \" knowledge\", \" to\"\n",
      "Please predict the activation on this sentence, responding with a number between 0 and 7.48.\n",
      "\n",
      "Sentence: \".ĊĊRubio's disclosure sheds new light on his comments in October, when\" Remember, the english description of the feature is: \"words related to technical processes or actions\" and the most common words that resulted in a high activation were \" on\", \" and\", \" If\", \" is\", \" a\", \"'s\", \" knowledge\", \" to\"\n",
      "Please predict the activation on this sentence, responding with a number between 0 and 7.48.\n",
      "\n",
      "Sentence: \" comments in October, when he warned his fellow Republicans against trying to make political hay out\" Remember, the english description of the feature is: \"words related to technical processes or actions\" and the most common words that resulted in a high activation were \" on\", \" and\", \" If\", \" is\", \" a\", \"'s\", \" knowledge\", \" to\"\n",
      "Please predict the activation on this sentence, responding with a number between 0 and 7.48.\n",
      "\n",
      "Sentence: \" disclosure sheds new light on his comments in October, when he warned his fellow Republicans against\" Remember, the english description of the feature is: \"words related to technical processes or actions\" and the most common words that resulted in a high activation were \" on\", \" and\", \" If\", \" is\", \" a\", \"'s\", \" knowledge\", \" to\"\n",
      "Please predict the activation on this sentence, responding with a number between 0 and 7.48.\n",
      "\n",
      "Sentence: \". It will be something you will have to look up in an early 20th-\" Remember, the english description of the feature is: \"words related to technical processes or actions\" and the most common words that resulted in a high activation were \" on\", \" and\", \" If\", \" is\", \" a\", \"'s\", \" knowledge\", \" to\"\n",
      "Please predict the activation on this sentence, responding with a number between 0 and 7.48.\n",
      "\n",
      "Sentence: \" time out. You could tell. Peer totally got you out of nowhere.\"ĊĊ\" Remember, the english description of the feature is: \"words related to technical processes or actions\" and the most common words that resulted in a high activation were \" on\", \" and\", \" If\", \" is\", \" a\", \"'s\", \" knowledge\", \" to\"\n",
      "Please predict the activation on this sentence, responding with a number between 0 and 7.48.\n",
      "\n",
      "Sentence: \" to move beyond the fork and the knife and think about inhaling food,\" he says\" Remember, the english description of the feature is: \"words related to technical processes or actions\" and the most common words that resulted in a high activation were \" on\", \" and\", \" If\", \" is\", \" a\", \"'s\", \" knowledge\", \" to\"\n",
      "Please predict the activation on this sentence, responding with a number between 0 and 7.48.\n",
      "\n",
      "Sentence: \" thing connected and seamless. My dream is to type something once, just one time,\" Remember, the english description of the feature is: \"words related to technical processes or actions\" and the most common words that resulted in a high activation were \" on\", \" and\", \" If\", \" is\", \" a\", \"'s\", \" knowledge\", \" to\"\n",
      "Please predict the activation on this sentence, responding with a number between 0 and 7.48.\n",
      "\n",
      "Sentence: \" try out hazardous chemical reactions. âĢľWe had fun demonstrating the idea by colliding\" Remember, the english description of the feature is: \"words related to technical processes or actions\" and the most common words that resulted in a high activation were \" on\", \" and\", \" If\", \" is\", \" a\", \"'s\", \" knowledge\", \" to\"\n",
      "('You are evaluating an english description of an autoencoder feature. The '\n",
      " 'description should correspond to sentences which result in high activation. '\n",
      " 'The english description of the feature is: \"words related to technical '\n",
      " 'processes or actions\"\\n'\n",
      " 'Here are 8 examples of sentences and their corresponding activations:\\n'\n",
      " ' Example: \". Where news only sort of properly existed on the web for me. '\n",
      " 'ThatâĢĻ\" had an activation of 1.45 on token \" on\"\\n'\n",
      " 'Example: \" that has a 90 percent of breaking even and a 10 percent chance of '\n",
      " 'earning you $\" had an activation of 1.51 on token \" and\"\\n'\n",
      " 'Example: \"s populated in all the right places. If Rigo changed his jersey '\n",
      " 'size IâĢ\" had an activation of 6.23 on token \" If\"\\n'\n",
      " 'Example: \" âĢľIf you look at pizza, what is it? ItâĢĻs high\" had an '\n",
      " 'activation of 6.52 on token \" is\"\\n'\n",
      " 'Example: \" Buckley, had \"sold a million in product.\" A CNC table router. '\n",
      " 'What\" had an activation of 3.79 on token \" a\"\\n'\n",
      " 'Example: \"\\'s disclosure sheds new light on his comments in October, when he '\n",
      " 'warned his fellow Republicans\" had an activation of 0.00 on token \"\\'s\"\\n'\n",
      " 'Example: \" secondĊĊThe worldâĢĻs knowledge at our fingertipsĊĊA seemingly '\n",
      " 'infinite\" had an activation of 4.43 on token \" knowledge\"\\n'\n",
      " 'Example: \" go into butter - now it\\'s up to Â£2.50, Â£2.\" had an activation '\n",
      " 'of 6.51 on token \" to\"\\n'\n",
      " 'Use the provided samples and the provided description to predict the '\n",
      " 'activation on a new sentence.\\n'\n",
      " 'You MUST respond with ONLY a number and NO OTHER content.')\n",
      "(1.727350354194641, 1.29)  the words \"I'll set this plane on fire\" and a seat number.Ċ\n",
      "(0, 0.0) ,\" Watts said.ĊĊRubio's disclosure sheds new light on his comments in\n",
      "(0, 3.25) .ĊĊRubio's disclosure sheds new light on his comments in October, when\n",
      "(0, 2.19)  comments in October, when he warned his fellow Republicans against trying to make political hay out\n",
      "(0, 0.0)  disclosure sheds new light on his comments in October, when he warned his fellow Republicans against\n",
      "(7.41083812713623, 0.0) . It will be something you will have to look up in an early 20th-\n",
      "(2.793524742126465, 1.12)  time out. You could tell. Peer totally got you out of nowhere.\"ĊĊ\n",
      "(6.407825946807861, 6.23)  to move beyond the fork and the knife and think about inhaling food,\" he says\n",
      "(6.859172821044922, 4.18)  thing connected and seamless. My dream is to type something once, just one time,\n",
      "(2.314035415649414, 6.34)  try out hazardous chemical reactions. âĢľWe had fun demonstrating the idea by colliding\n",
      "\n",
      "[(1.727350354194641, 1.29),\n",
      " (0, 0.0),\n",
      " (0, 3.25),\n",
      " (0, 2.19),\n",
      " (0, 0.0),\n",
      " (7.41083812713623, 0.0),\n",
      " (2.793524742126465, 1.12),\n",
      " (6.407825946807861, 6.23),\n",
      " (6.859172821044922, 4.18),\n",
      " (2.314035415649414, 6.34)]\n",
      "0.6522477291298594\n"
     ]
    }
   ],
   "source": [
    "def get_predictions(feature_num):\n",
    "    # activation_data = get_activation_data_for_feature(f\"https://www.neuronpedia.org/api/feature/gpt2-small/9-res-jb/{feature_num}\")\n",
    "    predictions = predict_activations(feature_num, test_number=10, show_examples=8)\n",
    "    return predictions\n",
    "\n",
    "def run():\n",
    "    data = get_predictions(991) #806\n",
    "    # for i in range(len(all_data)):\n",
    "        # data = all_data[i]\n",
    "        # print(feature_nums[i])\n",
    "    print()\n",
    "    pprint.pprint(data)\n",
    "    custom = custom_accuracy(data)\n",
    "\n",
    "    print(custom)\n",
    "\n",
    "    # custom = [custom_accuracy(data, eps = 0.1) for data in all_data]\n",
    "\n",
    "for _ in range(1):\n",
    "    run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "### Losses\n",
    "def mse(data, normalize = False):\n",
    "    values = ([((elem[0]-elem[1])/(elem[0] if normalize else 1))**2 for elem in data])\n",
    "    return sum(values)/len(values)\n",
    "\n",
    "def nll_variant(data, eps = 1e-1):\n",
    "    values = ([np.log((min(elem) + eps)/(max(elem) + eps)) for elem in data])\n",
    "    return -sum(values)/len(values)\n",
    "\n",
    "def l1(data, normalize = True, eps = 0.1):\n",
    "    values = ([((eps + abs(elem[0]-elem[1]))/((max(elem) if normalize else 1) + eps))  for elem in data])\n",
    "    return sum(values)/len(values)\n",
    "\n",
    "### Plots\n",
    "def plot_mses_cdf(mses):\n",
    "    # Plotting the Mean Squared Errors (MSE) for each dataset\n",
    "    mses_sorted = np.sort(mses)\n",
    "    cdf = np.arange(1, len(mses_sorted)+1) / len(mses_sorted)\n",
    "    plt.plot(mses_sorted, cdf)\n",
    "    plt.title('Cumulative Distribution Function of MSEs')\n",
    "    plt.xlabel('MSE')\n",
    "    plt.ylabel('CDF')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def plot_probability_distribution(data, bins='auto', density=True, title = \"Default Title\"):\n",
    "    \"\"\"\n",
    "    Plots the probability distribution of the given data using a histogram.\n",
    "\n",
    "    Parameters:\n",
    "    - data (list or numpy array): The floating point numbers whose distribution you want to plot.\n",
    "    - bins (int, sequence or str, optional): The method for calculating histogram bins. Default is 'auto'.\n",
    "    - density (bool, optional): If True, the histogram is normalized to form a probability density,\n",
    "                                i.e., the area under the histogram will sum to 1. Default is True.\n",
    "    \"\"\"\n",
    "    # Calculate the histogram\n",
    "    counts, bin_edges = np.histogram(data, bins=bins, density=density)\n",
    "\n",
    "    # Calculate bin centers\n",
    "    bin_centers = 0.5 * (bin_edges[:-1] + bin_edges[1:])\n",
    "\n",
    "    # Plotting the histogram\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.bar(bin_centers, counts*np.diff(bin_edges), align='center', width=np.diff(bin_edges), edgecolor='black', alpha=0.7)\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Probability Density')\n",
    "    plt.title('Probability Distribution of Data')\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def analyze_data(all_data):\n",
    "    mses = [mse(data, normalize = False) for data in all_data]\n",
    "    nlls = [nll_variant(data) for data in all_data]\n",
    "    l1s = [l1(data, normalize = True) for data in all_data]\n",
    "\n",
    "    print('l1s', sorted(l1s))\n",
    "    plot_probability_distribution(mses, title = \"Distribution of MSEs\")\n",
    "    plot_probability_distribution(nlls, title = \"Distribution of NLL variant\")\n",
    "    plot_probability_distribution(l1s, title = \"Distribution of l1s variant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'concurrent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[74], line 9\u001b[0m\n\u001b[1;32m      5\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m predict_activations(feature_num, test_number\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, show_examples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m)\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m predictions\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m concurrent\u001b[38;5;241m.\u001b[39mfutures\u001b[38;5;241m.\u001b[39mThreadPoolExecutor() \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[1;32m     10\u001b[0m     all_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(executor\u001b[38;5;241m.\u001b[39mmap(get_predictions, feature_nums))\n\u001b[1;32m     12\u001b[0m mses \u001b[38;5;241m=\u001b[39m [mse(data, normalize \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m all_data]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'concurrent' is not defined"
     ]
    }
   ],
   "source": [
    "feature_nums = [806]#random.sample(range(0, 1000), 10)\n",
    "\n",
    "def get_predictions(feature_num):\n",
    "    # activation_data = get_activation_data_for_feature(f\"https://www.neuronpedia.org/api/feature/gpt2-small/9-res-jb/{feature_num}\")\n",
    "    predictions = predict_activations(feature_num, test_number=10, show_examples=8)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    all_data = list(executor.map(get_predictions, feature_nums))\n",
    "\n",
    "mses = [mse(data, normalize = False) for data in all_data]\n",
    "nlls = [nll_variant(data) for data in all_data]\n",
    "l1s = [l1(data, normalize = True) for data in all_data]\n",
    "\n",
    "# print('l1s', sorted(l1s))\n",
    "plot_probability_distribution(mses, title = \"Distribution of MSEs\")\n",
    "plot_probability_distribution(nlls, title = \"Distribution of NLL variant\")\n",
    "plot_probability_distribution(l1s, title = \"Distribution of l1s variant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_accuracy(data):\n",
    "    eps = max([elem[0] for elem in data]) / 10\n",
    "    values = []\n",
    "    for elem in data:\n",
    "        true, pred = elem\n",
    "        ## Add eps to avoid zero case\n",
    "        true, pred = true + eps, pred + eps\n",
    "        # Scale values\n",
    "        true, pred = true ** 0.75, pred ** 0.75\n",
    "        # Calculate difference\n",
    "        difference = abs(true - pred)\n",
    "        # Take ratio\n",
    "        error = difference / max(true, pred)\n",
    "        \n",
    "        accuracy = 1 - error\n",
    "        values.append(accuracy)\n",
    "    return sum(values)/len(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'feature_nums' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m feature_nums\n",
      "\u001b[0;31mNameError\u001b[0m: name 'feature_nums' is not defined"
     ]
    }
   ],
   "source": [
    "feature_nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('You are evaluating an english description of an autoencoder feature. The '\n",
      " 'description should correspond to sentences which result in high activation. '\n",
      " 'The english description of the feature is: \" past tense verbs\"\\n'\n",
      " 'Here are 8 examples of sentences and their corresponding activations:\\n'\n",
      " ' Example: \" economy\\'s cooled off enough, but it wasn\\'t always so. Back in '\n",
      " 'the mid\", Activation: 19.96\\n'\n",
      " 'Example: \" NL<|endoftext|>,\" Watts said.ĊĊRubio\\'s disclosure sheds new '\n",
      " 'light on his\", Activation: 0.00\\n'\n",
      " 'Example: \" in their NL<|endoftext|>,\" Watts said.ĊĊRubio\\'s disclosure sheds '\n",
      " 'new light\", Activation: 0.00\\n'\n",
      " 'Example: \"ĊĊRubio\\'s disclosure sheds new light on his comments in October, '\n",
      " 'when he\", Activation: 0.00\\n'\n",
      " 'Example: \" be sure to add a great feel and glitz to any game. These '\n",
      " 'wonderful futuristic\", Activation: 0.00\\n'\n",
      " 'Example: \" their NL<|endoftext|>,\" Watts said.ĊĊRubio\\'s disclosure sheds '\n",
      " 'new light on\", Activation: 0.00\\n'\n",
      " 'Example: \" of California, proved that prawns eat the mollusc hosts. He '\n",
      " 'shared\", Activation: 0.00\\n'\n",
      " 'Example: \" songs,\" Willow said. \"These songs were created through characters '\n",
      " 'I have developed within my\", Activation: 16.54\\n'\n",
      " 'Use the provided samples and the provided description to predict the '\n",
      " 'activation on a new sentence.\\n'\n",
      " 'You MUST respond with ONLY a number and NO OTHER content.')\n",
      "[{'activation': 21.34770965576172,\n",
      "  'max_index': 8,\n",
      "  'sentence_string': ' the surface area of the land. This was the allocation '\n",
      "                     'that was specified in the plan'},\n",
      " {'activation': 21.08387565612793,\n",
      "  'max_index': 8,\n",
      "  'sentence_string': \" to their online bordcasting but they didn't treated \"\n",
      "                     'well who play in the NASL'},\n",
      " {'activation': 15.00903415679932,\n",
      "  'max_index': 8,\n",
      "  'sentence_string': ' industry for add-ons to applications that were not '\n",
      "                     'meant to be extendible.<|endoftext|>'},\n",
      " {'activation': 0,\n",
      "  'max_index': 0,\n",
      "  'sentence_string': ' Central American folklore. It is a shape-changing '\n",
      "                     'spirit that typically takes the form of'},\n",
      " {'activation': 24.55836486816406,\n",
      "  'max_index': 8,\n",
      "  'sentence_string': ' Great Value, and other brands. It was John Henry Heinz, '\n",
      "                     'in fact,'},\n",
      " {'activation': 0,\n",
      "  'max_index': 0,\n",
      "  'sentence_string': ' comments in October, when he warned his fellow '\n",
      "                     'Republicans against trying to make political hay out'},\n",
      " {'activation': 0,\n",
      "  'max_index': 0,\n",
      "  'sentence_string': \"Rubio's disclosure sheds new light on his comments in \"\n",
      "                     'October, when he warned his'},\n",
      " {'activation': 0,\n",
      "  'max_index': 0,\n",
      "  'sentence_string': ' sheds new light on his comments in October, when he '\n",
      "                     'warned his fellow Republicans against trying'},\n",
      " {'activation': 0,\n",
      "  'max_index': 0,\n",
      "  'sentence_string': \"ĊRubio's disclosure sheds new light on his comments in \"\n",
      "                     'October, when he warned'},\n",
      " {'activation': 12.68032932281494,\n",
      "  'max_index': 8,\n",
      "  'sentence_string': ' disrupt the discourse with other people, who were '\n",
      "                     'either supportive or who were neutral and just'}]\n",
      "[(21.34770965576172, 0.0),\n",
      " (21.08387565612793, 0.0),\n",
      " (15.00903415679932, 0.0),\n",
      " (0, 0.0),\n",
      " (24.55836486816406, 0.0),\n",
      " (0, 15.23),\n",
      " (0, 0.0),\n",
      " (0, 0.0),\n",
      " (0, 0.0),\n",
      " (12.68032932281494, 0.0)]\n",
      "0.4124388582827179\n"
     ]
    }
   ],
   "source": [
    "def run():\n",
    "    data = get_predictions(806)\n",
    "    # for i in range(len(all_data)):\n",
    "        # data = all_data[i]\n",
    "        # print(feature_nums[i])\n",
    "    pprint.pprint(data)\n",
    "    custom = custom_accuracy(data)\n",
    "    print(custom)\n",
    "\n",
    "    # custom = [custom_accuracy(data, eps = 0.1) for data in all_data]\n",
    "\n",
    "for _ in range(1):\n",
    "    run()\n",
    "\n",
    "# plot_probability_distribution(custom, title = \"Distribution of custom accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[80], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m analyze_data(all_data)\n",
      "File \u001b[0;32m~/Desktop/GitHub/feature_benchmark/prediction_analysis.py:57\u001b[0m, in \u001b[0;36manalyze_data\u001b[0;34m(all_data)\u001b[0m\n",
      "File \u001b[0;32m~/Desktop/GitHub/feature_benchmark/prediction_analysis.py:7\u001b[0m, in \u001b[0;36mmse\u001b[0;34m(data, normalize)\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "analyze_data(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
