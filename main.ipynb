{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getting_examples import *\n",
    "from predict_activations import *\n",
    "from utils import *\n",
    "import json\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run an experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<<<<<<< local\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens: 268\n",
      "tokens: 268\n",
      "tokens: 272\n",
      "tokens: 261\n",
      "tokens: 261\n",
      "tokens: 263\n",
      "{'hyperparameters': {'binary_class': True,\n",
      "                     'debug': False,\n",
      "                     'neg_type': 'others',\n",
      "                     'num_completions': 1,\n",
      "                     'randomize_pos': True,\n",
      "                     'seed': 42,\n",
      "                     'show_max_token': True,\n",
      "                     'show_neg': 3,\n",
      "                     'show_pos': 0,\n",
      "                     'test_neg': 3,\n",
      "                     'test_pos': 3},\n",
      " 'num_features': 1,\n",
      " 'results': [{'description': 'timestamps or dates in a specific format',\n",
      "              'feature_index': 521,\n",
      "              'gpt_predictions': [(1, 1.0),\n",
      "                                  (1, 1.0),\n",
      "                                  (1, 0.0),\n",
      "                                  (0, 0.0),\n",
      "                                  (0, 0.0),\n",
      "                                  (0, 0.0)],\n",
      "              'highest_activation': 51.19043350219727,\n",
      "              'show_sentences': [{'max_value': 'low',\n",
      "                                  'sentence_string': 'endangered primates '\n",
      "                                                     'being killed by humans. '\n",
      "                                                     'Orangutans are faced '\n",
      "                                                     'with extinction from',\n",
      "                                  'tokens': ['end',\n",
      "                                             'angered',\n",
      "                                             ' primates',\n",
      "                                             ' being',\n",
      "                                             ' killed',\n",
      "                                             ' by',\n",
      "                                             ' humans',\n",
      "                                             '.',\n",
      "                                             ' Or',\n",
      "                                             'ang',\n",
      "                                             'ut',\n",
      "                                             'ans',\n",
      "                                             ' are',\n",
      "                                             ' faced',\n",
      "                                             ' with',\n",
      "                                             ' extinction',\n",
      "                                             ' from'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_value': 'low',\n",
      "                                  'sentence_string': ' is a function of how '\n",
      "                                                     'much sugar is in the '\n",
      "                                                     'food. Zume Pizza is half',\n",
      "                                  'tokens': [' is',\n",
      "                                             ' a',\n",
      "                                             ' function',\n",
      "                                             ' of',\n",
      "                                             ' how',\n",
      "                                             ' much',\n",
      "                                             ' sugar',\n",
      "                                             ' is',\n",
      "                                             ' in',\n",
      "                                             ' the',\n",
      "                                             ' food',\n",
      "                                             '.',\n",
      "                                             ' Z',\n",
      "                                             'ume',\n",
      "                                             ' Pizza',\n",
      "                                             ' is',\n",
      "                                             ' half'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_value': 'low',\n",
      "                                  'sentence_string': 'ĿĊĊDonald Trump has '\n",
      "                                                     'captured the imagination '\n",
      "                                                     'of the American people '\n",
      "                                                     'like no other since',\n",
      "                                  'tokens': ['Ŀ',\n",
      "                                             'Ċ',\n",
      "                                             'Ċ',\n",
      "                                             'Donald',\n",
      "                                             ' Trump',\n",
      "                                             ' has',\n",
      "                                             ' captured',\n",
      "                                             ' the',\n",
      "                                             ' imagination',\n",
      "                                             ' of',\n",
      "                                             ' the',\n",
      "                                             ' American',\n",
      "                                             ' people',\n",
      "                                             ' like',\n",
      "                                             ' no',\n",
      "                                             ' other',\n",
      "                                             ' since'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]}],\n",
      "              'test_sentences': [{'max_token': ' Published',\n",
      "                                  'max_value': 'high',\n",
      "                                  'max_value_token_index': 8,\n",
      "                                  'sentence_string': ' demonetization '\n",
      "                                                     'issue.ĊĊFirst Published: '\n",
      "                                                     'Dec 07, 2016 18:57',\n",
      "                                  'tokens': [' demon',\n",
      "                                             'et',\n",
      "                                             'ization',\n",
      "                                             ' issue',\n",
      "                                             '.',\n",
      "                                             'Ċ',\n",
      "                                             'Ċ',\n",
      "                                             'First',\n",
      "                                             ' Published',\n",
      "                                             ':',\n",
      "                                             ' Dec',\n",
      "                                             ' 07',\n",
      "                                             ',',\n",
      "                                             ' 2016',\n",
      "                                             ' 18',\n",
      "                                             ':',\n",
      "                                             '57'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             46.11051177978516,\n",
      "                                             12.55528831481934,\n",
      "                                             2.171714067459106,\n",
      "                                             6.039763450622559,\n",
      "                                             5.608806133270264,\n",
      "                                             5.501268863677979,\n",
      "                                             4.540326595306396,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_token': ' posted',\n",
      "                                  'max_value': 'high',\n",
      "                                  'max_value_token_index': 8,\n",
      "                                  'sentence_string': 'ladesh, asiaĊĊFirst '\n",
      "                                                     'posted<|endoftext|>When '\n",
      "                                                     'Sergeant Clay Hunt left '\n",
      "                                                     'the U',\n",
      "                                  'tokens': ['l',\n",
      "                                             'adesh',\n",
      "                                             ',',\n",
      "                                             ' as',\n",
      "                                             'ia',\n",
      "                                             'Ċ',\n",
      "                                             'Ċ',\n",
      "                                             'First',\n",
      "                                             ' posted',\n",
      "                                             '<|endoftext|>',\n",
      "                                             'When',\n",
      "                                             ' Sergeant',\n",
      "                                             ' Clay',\n",
      "                                             ' Hunt',\n",
      "                                             ' left',\n",
      "                                             ' the',\n",
      "                                             ' U'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             49.49526596069336,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_token': ' posted',\n",
      "                                  'max_value': 'high',\n",
      "                                  'max_value_token_index': 8,\n",
      "                                  'sentence_string': ' markets, '\n",
      "                                                     'australiaĊĊFirst '\n",
      "                                                     'posted<|endoftext|>FARMINGTON '\n",
      "                                                     'âĢĶ A former counselor',\n",
      "                                  'tokens': [' markets',\n",
      "                                             ',',\n",
      "                                             ' aust',\n",
      "                                             'ral',\n",
      "                                             'ia',\n",
      "                                             'Ċ',\n",
      "                                             'Ċ',\n",
      "                                             'First',\n",
      "                                             ' posted',\n",
      "                                             '<|endoftext|>',\n",
      "                                             'F',\n",
      "                                             'ARM',\n",
      "                                             'INGTON',\n",
      "                                             ' âĢĶ',\n",
      "                                             ' A',\n",
      "                                             ' former',\n",
      "                                             ' counselor'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             45.93206405639648,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_value': 'low',\n",
      "                                  'sentence_string': ' the National Security '\n",
      "                                                     'Agency of providing '\n",
      "                                                     'false information in a '\n",
      "                                                     'fact sheet about its '\n",
      "                                                     'spying programs,',\n",
      "                                  'tokens': [' the',\n",
      "                                             ' National',\n",
      "                                             ' Security',\n",
      "                                             ' Agency',\n",
      "                                             ' of',\n",
      "                                             ' providing',\n",
      "                                             ' false',\n",
      "                                             ' information',\n",
      "                                             ' in',\n",
      "                                             ' a',\n",
      "                                             ' fact',\n",
      "                                             ' sheet',\n",
      "                                             ' about',\n",
      "                                             ' its',\n",
      "                                             ' spying',\n",
      "                                             ' programs',\n",
      "                                             ','],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_value': 'low',\n",
      "                                  'sentence_string': ' do an episode about '\n",
      "                                                     'women in comedy, or have '\n",
      "                                                     'a sex-positive guest '\n",
      "                                                     'star of',\n",
      "                                  'tokens': [' do',\n",
      "                                             ' an',\n",
      "                                             ' episode',\n",
      "                                             ' about',\n",
      "                                             ' women',\n",
      "                                             ' in',\n",
      "                                             ' comedy',\n",
      "                                             ',',\n",
      "                                             ' or',\n",
      "                                             ' have',\n",
      "                                             ' a',\n",
      "                                             ' sex',\n",
      "                                             '-',\n",
      "                                             'positive',\n",
      "                                             ' guest',\n",
      "                                             ' star',\n",
      "                                             ' of'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_value': 'low',\n",
      "                                  'sentence_string': ' positive person was on '\n",
      "                                                     'treatment, had a viral '\n",
      "                                                     'load below 40 and had no '\n",
      "                                                     'other ST',\n",
      "                                  'tokens': [' positive',\n",
      "                                             ' person',\n",
      "                                             ' was',\n",
      "                                             ' on',\n",
      "                                             ' treatment',\n",
      "                                             ',',\n",
      "                                             ' had',\n",
      "                                             ' a',\n",
      "                                             ' viral',\n",
      "                                             ' load',\n",
      "                                             ' below',\n",
      "                                             ' 40',\n",
      "                                             ' and',\n",
      "                                             ' had',\n",
      "                                             ' no',\n",
      "                                             ' other',\n",
      "                                             ' ST'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]}]}],\n",
      " 'timestamp': 1715381659.085987}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=======\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted 1 of 2 tasks. Been running for 0 seconds\n",
      "Submitted 2 of 2 tasks. Been running for 0 seconds\n",
      "{'hyperparameters': {'binary_class': True,\n",
      "                     'debug': False,\n",
      "                     'neg_type': 'others',\n",
      "                     'num_completions': 1,\n",
      "                     'randomize_pos': True,\n",
      "                     'seed': 42,\n",
      "                     'show_max_token': False,\n",
      "                     'show_neg': 0,\n",
      "                     'show_pos': 0,\n",
      "                     'test_neg': 5,\n",
      "                     'test_pos': 5},\n",
      " 'num_features': 2,\n",
      " 'results': [{'description': 'timestamps or dates in a specific format',\n",
      "              'feature_index': 521,\n",
      "              'gpt_predictions': [(1, 1.0),\n",
      "                                  (1, 0.0),\n",
      "                                  (1, 0.0),\n",
      "                                  (1, 0.0),\n",
      "                                  (1, 0.0),\n",
      "                                  (0, 0.0),\n",
      "                                  (0, 0.0),\n",
      "                                  (0, 0.0),\n",
      "                                  (0, 0.0),\n",
      "                                  (0, 0.0)],\n",
      "              'highest_activation': 51.19043350219727,\n",
      "              'show_sentences': [],\n",
      "              'test_sentences': [{'max_token': ' Published',\n",
      "                                  'max_value': 'high',\n",
      "                                  'max_value_token_index': 8,\n",
      "                                  'sentence_string': ' demonetization '\n",
      "                                                     'issue.ĊĊFirst Published: '\n",
      "                                                     'Dec 07, 2016 18:57',\n",
      "                                  'tokens': [' demon',\n",
      "                                             'et',\n",
      "                                             'ization',\n",
      "                                             ' issue',\n",
      "                                             '.',\n",
      "                                             'Ċ',\n",
      "                                             'Ċ',\n",
      "                                             'First',\n",
      "                                             ' Published',\n",
      "                                             ':',\n",
      "                                             ' Dec',\n",
      "                                             ' 07',\n",
      "                                             ',',\n",
      "                                             ' 2016',\n",
      "                                             ' 18',\n",
      "                                             ':',\n",
      "                                             '57'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             46.11051177978516,\n",
      "                                             12.55528831481934,\n",
      "                                             2.171714067459106,\n",
      "                                             6.039763450622559,\n",
      "                                             5.608806133270264,\n",
      "                                             5.501268863677979,\n",
      "                                             4.540326595306396,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_token': ' posted',\n",
      "                                  'max_value': 'high',\n",
      "                                  'max_value_token_index': 8,\n",
      "                                  'sentence_string': 'ladesh, asiaĊĊFirst '\n",
      "                                                     'posted<|endoftext|>When '\n",
      "                                                     'Sergeant Clay Hunt left '\n",
      "                                                     'the U',\n",
      "                                  'tokens': ['l',\n",
      "                                             'adesh',\n",
      "                                             ',',\n",
      "                                             ' as',\n",
      "                                             'ia',\n",
      "                                             'Ċ',\n",
      "                                             'Ċ',\n",
      "                                             'First',\n",
      "                                             ' posted',\n",
      "                                             '<|endoftext|>',\n",
      "                                             'When',\n",
      "                                             ' Sergeant',\n",
      "                                             ' Clay',\n",
      "                                             ' Hunt',\n",
      "                                             ' left',\n",
      "                                             ' the',\n",
      "                                             ' U'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             49.49526596069336,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_token': ' posted',\n",
      "                                  'max_value': 'high',\n",
      "                                  'max_value_token_index': 8,\n",
      "                                  'sentence_string': ' markets, '\n",
      "                                                     'australiaĊĊFirst '\n",
      "                                                     'posted<|endoftext|>FARMINGTON '\n",
      "                                                     'âĢĶ A former counselor',\n",
      "                                  'tokens': [' markets',\n",
      "                                             ',',\n",
      "                                             ' aust',\n",
      "                                             'ral',\n",
      "                                             'ia',\n",
      "                                             'Ċ',\n",
      "                                             'Ċ',\n",
      "                                             'First',\n",
      "                                             ' posted',\n",
      "                                             '<|endoftext|>',\n",
      "                                             'F',\n",
      "                                             'ARM',\n",
      "                                             'INGTON',\n",
      "                                             ' âĢĶ',\n",
      "                                             ' A',\n",
      "                                             ' former',\n",
      "                                             ' counselor'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             45.93206405639648,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_token': ' week',\n",
      "                                  'max_value': 'high',\n",
      "                                  'max_value_token_index': 7,\n",
      "                                  'sentence_string': ' Morning Links '\n",
      "                                                     '---------------------- '\n",
      "                                                     'this week last week most '\n",
      "                                                     'discussedĊĊMost '\n",
      "                                                     'recommended from 60 '\n",
      "                                                     'comments',\n",
      "                                  'tokens': [' Morning',\n",
      "                                             ' Links',\n",
      "                                             ' --------------------',\n",
      "                                             '--',\n",
      "                                             ' this',\n",
      "                                             ' week',\n",
      "                                             ' last',\n",
      "                                             ' week',\n",
      "                                             ' most',\n",
      "                                             ' discussed',\n",
      "                                             'Ċ',\n",
      "                                             'Ċ',\n",
      "                                             'Most',\n",
      "                                             ' recommended',\n",
      "                                             ' from',\n",
      "                                             ' 60',\n",
      "                                             ' comments'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             1.456243395805359,\n",
      "                                             0,\n",
      "                                             4.115555763244629,\n",
      "                                             5.034063339233398,\n",
      "                                             8.674803733825684,\n",
      "                                             5.392209529876709,\n",
      "                                             4.928345680236816,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_token': ' posts',\n",
      "                                  'max_value': 'high',\n",
      "                                  'max_value_token_index': 8,\n",
      "                                  'sentence_string': ' part in that.ĊĊLast 5 '\n",
      "                                                     'posts by Jason '\n",
      "                                                     'Ditz<|endoftext|>The '\n",
      "                                                     'Democrats and',\n",
      "                                  'tokens': [' part',\n",
      "                                             ' in',\n",
      "                                             ' that',\n",
      "                                             '.',\n",
      "                                             'Ċ',\n",
      "                                             'Ċ',\n",
      "                                             'Last',\n",
      "                                             ' 5',\n",
      "                                             ' posts',\n",
      "                                             ' by',\n",
      "                                             ' Jason',\n",
      "                                             ' D',\n",
      "                                             'itz',\n",
      "                                             '<|endoftext|>',\n",
      "                                             'The',\n",
      "                                             ' Democrats',\n",
      "                                             ' and'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0.6818268299102783,\n",
      "                                             15.06083965301514,\n",
      "                                             21.10032081604004,\n",
      "                                             9.019525527954102,\n",
      "                                             0,\n",
      "                                             1.647749423980713,\n",
      "                                             6.434071063995361,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_value': 'low',\n",
      "                                  'sentence_string': ' quick lead only to have '\n",
      "                                                     'to bow out before the '\n",
      "                                                     'end of the game because '\n",
      "                                                     'they went',\n",
      "                                  'tokens': [' quick',\n",
      "                                             ' lead',\n",
      "                                             ' only',\n",
      "                                             ' to',\n",
      "                                             ' have',\n",
      "                                             ' to',\n",
      "                                             ' bow',\n",
      "                                             ' out',\n",
      "                                             ' before',\n",
      "                                             ' the',\n",
      "                                             ' end',\n",
      "                                             ' of',\n",
      "                                             ' the',\n",
      "                                             ' game',\n",
      "                                             ' because',\n",
      "                                             ' they',\n",
      "                                             ' went'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_value': 'low',\n",
      "                                  'sentence_string': 'ĊAldrin was a drug user '\n",
      "                                                     'before, Castillo '\n",
      "                                                     'admitted, but he had',\n",
      "                                  'tokens': ['Ċ',\n",
      "                                             'A',\n",
      "                                             'ld',\n",
      "                                             'rin',\n",
      "                                             ' was',\n",
      "                                             ' a',\n",
      "                                             ' drug',\n",
      "                                             ' user',\n",
      "                                             ' before',\n",
      "                                             ',',\n",
      "                                             ' Cast',\n",
      "                                             'illo',\n",
      "                                             ' admitted',\n",
      "                                             ',',\n",
      "                                             ' but',\n",
      "                                             ' he',\n",
      "                                             ' had'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_value': 'low',\n",
      "                                  'sentence_string': ' height at some '\n",
      "                                                     'international '\n",
      "                                                     'summit.ĊĊOr is it '\n",
      "                                                     'because of these deeply '\n",
      "                                                     'controversial statements',\n",
      "                                  'tokens': [' height',\n",
      "                                             ' at',\n",
      "                                             ' some',\n",
      "                                             ' international',\n",
      "                                             ' summit',\n",
      "                                             '.',\n",
      "                                             'Ċ',\n",
      "                                             'Ċ',\n",
      "                                             'Or',\n",
      "                                             ' is',\n",
      "                                             ' it',\n",
      "                                             ' because',\n",
      "                                             ' of',\n",
      "                                             ' these',\n",
      "                                             ' deeply',\n",
      "                                             ' controversial',\n",
      "                                             ' statements'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_value': 'low',\n",
      "                                  'sentence_string': ' years after he was '\n",
      "                                                     'jailed for espionage, '\n",
      "                                                     'before his release in '\n",
      "                                                     '1969.ĊĊHe',\n",
      "                                  'tokens': [' years',\n",
      "                                             ' after',\n",
      "                                             ' he',\n",
      "                                             ' was',\n",
      "                                             ' jailed',\n",
      "                                             ' for',\n",
      "                                             ' espionage',\n",
      "                                             ',',\n",
      "                                             ' before',\n",
      "                                             ' his',\n",
      "                                             ' release',\n",
      "                                             ' in',\n",
      "                                             ' 1969',\n",
      "                                             '.',\n",
      "                                             'Ċ',\n",
      "                                             'Ċ',\n",
      "                                             'He'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_value': 'low',\n",
      "                                  'sentence_string': ' faces with respect to '\n",
      "                                                     'women.ĊĊA CNN poll '\n",
      "                                                     'released Monday showed '\n",
      "                                                     'Obama moving into',\n",
      "                                  'tokens': [' faces',\n",
      "                                             ' with',\n",
      "                                             ' respect',\n",
      "                                             ' to',\n",
      "                                             ' women',\n",
      "                                             '.',\n",
      "                                             'Ċ',\n",
      "                                             'Ċ',\n",
      "                                             'A',\n",
      "                                             ' CNN',\n",
      "                                             ' poll',\n",
      "                                             ' released',\n",
      "                                             ' Monday',\n",
      "                                             ' showed',\n",
      "                                             ' Obama',\n",
      "                                             ' moving',\n",
      "                                             ' into'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]}]},\n",
      "             {'description': 'references to locations or events related to the '\n",
      "                             'state of Florida',\n",
      "              'feature_index': 737,\n",
      "              'gpt_predictions': [(1, 0.0),\n",
      "                                  (1, 0.0),\n",
      "                                  (1, 0.0),\n",
      "                                  (1, 0.0),\n",
      "                                  (1, 0.0),\n",
      "                                  (0, 0.0),\n",
      "                                  (0, 0.0),\n",
      "                                  (0, 0.0),\n",
      "                                  (0, 0.0),\n",
      "                                  (0, 0.0)],\n",
      "              'highest_activation': 26.77979850769043,\n",
      "              'show_sentences': [],\n",
      "              'test_sentences': [{'max_token': ' Shawn',\n",
      "                                  'max_value': 'high',\n",
      "                                  'max_value_token_index': 8,\n",
      "                                  'sentence_string': ' offending legends such '\n",
      "                                                     'as Ric Flair and Shawn '\n",
      "                                                     'Michaels by his recent '\n",
      "                                                     'behavior. Instead of',\n",
      "                                  'tokens': [' offending',\n",
      "                                             ' legends',\n",
      "                                             ' such',\n",
      "                                             ' as',\n",
      "                                             ' Ric',\n",
      "                                             ' Fl',\n",
      "                                             'air',\n",
      "                                             ' and',\n",
      "                                             ' Shawn',\n",
      "                                             ' Michaels',\n",
      "                                             ' by',\n",
      "                                             ' his',\n",
      "                                             ' recent',\n",
      "                                             ' behavior',\n",
      "                                             '.',\n",
      "                                             ' Instead',\n",
      "                                             ' of'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             16.1812801361084,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_token': ' Shawn',\n",
      "                                  'max_value': 'high',\n",
      "                                  'max_value_token_index': 8,\n",
      "                                  'sentence_string': 'The spokesman for the '\n",
      "                                                     'Director of National '\n",
      "                                                     'Intelligence Shawn '\n",
      "                                                     'Turner said intelligence '\n",
      "                                                     'officials are '\n",
      "                                                     'âĢľcurrently',\n",
      "                                  'tokens': ['The',\n",
      "                                             ' spokesman',\n",
      "                                             ' for',\n",
      "                                             ' the',\n",
      "                                             ' Director',\n",
      "                                             ' of',\n",
      "                                             ' National',\n",
      "                                             ' Intelligence',\n",
      "                                             ' Shawn',\n",
      "                                             ' Turner',\n",
      "                                             ' said',\n",
      "                                             ' intelligence',\n",
      "                                             ' officials',\n",
      "                                             ' are',\n",
      "                                             ' âĢ',\n",
      "                                             'ľ',\n",
      "                                             'currently'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             16.70410919189453,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_token': ' mort',\n",
      "                                  'max_value': 'high',\n",
      "                                  'max_value_token_index': 8,\n",
      "                                  'sentence_string': ' dreaded the awkward '\n",
      "                                                     'yearbook shot and the '\n",
      "                                                     'mortifying middle school '\n",
      "                                                     'âĢľglamour',\n",
      "                                  'tokens': [' dreaded',\n",
      "                                             ' the',\n",
      "                                             ' awkward',\n",
      "                                             ' year',\n",
      "                                             'book',\n",
      "                                             ' shot',\n",
      "                                             ' and',\n",
      "                                             ' the',\n",
      "                                             ' mort',\n",
      "                                             'ifying',\n",
      "                                             ' middle',\n",
      "                                             ' school',\n",
      "                                             ' âĢ',\n",
      "                                             'ľ',\n",
      "                                             'g',\n",
      "                                             'lam',\n",
      "                                             'our'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             19.1842041015625,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_token': ' Mort',\n",
      "                                  'max_value': 'high',\n",
      "                                  'max_value_token_index': 8,\n",
      "                                  'sentence_string': ' of EgyptâĢĻs major '\n",
      "                                                     'clubs. Mortada Mansour, '\n",
      "                                                     'the committeeâĢĻ',\n",
      "                                  'tokens': [' of',\n",
      "                                             ' Egypt',\n",
      "                                             'âĢ',\n",
      "                                             'Ļ',\n",
      "                                             's',\n",
      "                                             ' major',\n",
      "                                             ' clubs',\n",
      "                                             '.',\n",
      "                                             ' Mort',\n",
      "                                             'ada',\n",
      "                                             ' Mans',\n",
      "                                             'our',\n",
      "                                             ',',\n",
      "                                             ' the',\n",
      "                                             ' committee',\n",
      "                                             'âĢ',\n",
      "                                             'Ļ'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             21.67017936706543,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_token': ' Flor',\n",
      "                                  'max_value': 'high',\n",
      "                                  'max_value_token_index': 8,\n",
      "                                  'sentence_string': ' executives Stefan '\n",
      "                                                     'Fellenberg, 38, and '\n",
      "                                                     'Florian Krause, 32, who '\n",
      "                                                     'own',\n",
      "                                  'tokens': [' executives',\n",
      "                                             ' Stefan',\n",
      "                                             ' Fell',\n",
      "                                             'enberg',\n",
      "                                             ',',\n",
      "                                             ' 38',\n",
      "                                             ',',\n",
      "                                             ' and',\n",
      "                                             ' Flor',\n",
      "                                             'ian',\n",
      "                                             ' Kra',\n",
      "                                             'use',\n",
      "                                             ',',\n",
      "                                             ' 32',\n",
      "                                             ',',\n",
      "                                             ' who',\n",
      "                                             ' own'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             25.24061584472656,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_value': 'low',\n",
      "                                  'sentence_string': ' quick lead only to have '\n",
      "                                                     'to bow out before the '\n",
      "                                                     'end of the game because '\n",
      "                                                     'they went',\n",
      "                                  'tokens': [' quick',\n",
      "                                             ' lead',\n",
      "                                             ' only',\n",
      "                                             ' to',\n",
      "                                             ' have',\n",
      "                                             ' to',\n",
      "                                             ' bow',\n",
      "                                             ' out',\n",
      "                                             ' before',\n",
      "                                             ' the',\n",
      "                                             ' end',\n",
      "                                             ' of',\n",
      "                                             ' the',\n",
      "                                             ' game',\n",
      "                                             ' because',\n",
      "                                             ' they',\n",
      "                                             ' went'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_value': 'low',\n",
      "                                  'sentence_string': 'ĊAldrin was a drug user '\n",
      "                                                     'before, Castillo '\n",
      "                                                     'admitted, but he had',\n",
      "                                  'tokens': ['Ċ',\n",
      "                                             'A',\n",
      "                                             'ld',\n",
      "                                             'rin',\n",
      "                                             ' was',\n",
      "                                             ' a',\n",
      "                                             ' drug',\n",
      "                                             ' user',\n",
      "                                             ' before',\n",
      "                                             ',',\n",
      "                                             ' Cast',\n",
      "                                             'illo',\n",
      "                                             ' admitted',\n",
      "                                             ',',\n",
      "                                             ' but',\n",
      "                                             ' he',\n",
      "                                             ' had'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_value': 'low',\n",
      "                                  'sentence_string': ' height at some '\n",
      "                                                     'international '\n",
      "                                                     'summit.ĊĊOr is it '\n",
      "                                                     'because of these deeply '\n",
      "                                                     'controversial statements',\n",
      "                                  'tokens': [' height',\n",
      "                                             ' at',\n",
      "                                             ' some',\n",
      "                                             ' international',\n",
      "                                             ' summit',\n",
      "                                             '.',\n",
      "                                             'Ċ',\n",
      "                                             'Ċ',\n",
      "                                             'Or',\n",
      "                                             ' is',\n",
      "                                             ' it',\n",
      "                                             ' because',\n",
      "                                             ' of',\n",
      "                                             ' these',\n",
      "                                             ' deeply',\n",
      "                                             ' controversial',\n",
      "                                             ' statements'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_value': 'low',\n",
      "                                  'sentence_string': ' years after he was '\n",
      "                                                     'jailed for espionage, '\n",
      "                                                     'before his release in '\n",
      "                                                     '1969.ĊĊHe',\n",
      "                                  'tokens': [' years',\n",
      "                                             ' after',\n",
      "                                             ' he',\n",
      "                                             ' was',\n",
      "                                             ' jailed',\n",
      "                                             ' for',\n",
      "                                             ' espionage',\n",
      "                                             ',',\n",
      "                                             ' before',\n",
      "                                             ' his',\n",
      "                                             ' release',\n",
      "                                             ' in',\n",
      "                                             ' 1969',\n",
      "                                             '.',\n",
      "                                             'Ċ',\n",
      "                                             'Ċ',\n",
      "                                             'He'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_value': 'low',\n",
      "                                  'sentence_string': ' faces with respect to '\n",
      "                                                     'women.ĊĊA CNN poll '\n",
      "                                                     'released Monday showed '\n",
      "                                                     'Obama moving into',\n",
      "                                  'tokens': [' faces',\n",
      "                                             ' with',\n",
      "                                             ' respect',\n",
      "                                             ' to',\n",
      "                                             ' women',\n",
      "                                             '.',\n",
      "                                             'Ċ',\n",
      "                                             'Ċ',\n",
      "                                             'A',\n",
      "                                             ' CNN',\n",
      "                                             ' poll',\n",
      "                                             ' released',\n",
      "                                             ' Monday',\n",
      "                                             ' showed',\n",
      "                                             ' Obama',\n",
      "                                             ' moving',\n",
      "                                             ' into'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]}]}],\n",
      " 'timestamp': 1715381671.126019}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ">>>>>>> remote\n"
     ]
    }
   ],
   "source": [
    "with open(f'feats.json', 'r') as file:\n",
    "    feature_data = json.load(file)\n",
    "\n",
    "results = run_experiments(\n",
    "    num_features=2, \n",
    "    feature_data=feature_data,\n",
    "    test_pos=5, # Experiment with\n",
    "    test_neg=5, # Experiment with\n",
    "    show_pos=0, # Experiment with\n",
    "    show_neg=0, # Experiment with\n",
    "    neg_type='others', # Experiment with\n",
    "    binary_class=True, # Experiment with\n",
    "    show_max_token=False, # Experiment with\n",
    "    num_completions=1, # Experiment with\n",
    "    debug=False, \n",
    "    randomize_pos=True, \n",
    "    seed=42,\n",
    "    save_location='test'\n",
    ")\n",
    "\n",
    "# the run_experiments function automatically saves results to results/exp_{timestamp}.json\n",
    "pprint.pprint(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a past result file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hyperparameters': {'binary_class': True,\n",
      "                     'debug': False,\n",
      "                     'neg_type': 'others',\n",
      "                     'num_completions': 1,\n",
      "                     'randomize_pos': True,\n",
      "                     'seed': 42,\n",
      "                     'show_max_token': False,\n",
      "                     'show_neg': 0,\n",
      "                     'show_pos': 0,\n",
      "                     'test_neg': 3,\n",
      "                     'test_pos': 3},\n",
      " 'num_features': 1,\n",
      " 'results': [{'description': 'phrases related to direct confrontation or '\n",
      "                             'comparison',\n",
      "              'feature_index': 3111,\n",
      "              'gpt_predictions': [[1, 0.0],\n",
      "                                  [1, 0.0],\n",
      "                                  [1, 1.0],\n",
      "                                  [0, 0.0],\n",
      "                                  [0, 0.0],\n",
      "                                  [0, 0.0]],\n",
      "              'highest_activation': 44.37568664550781,\n",
      "              'show_sentences': [],\n",
      "              'test_sentences': [{'max_token': '-',\n",
      "                                  'max_value': 'high',\n",
      "                                  'max_value_token_index': 8,\n",
      "                                  'sentence_string': ' go with Bisping in a '\n",
      "                                                     'point-fighting '\n",
      "                                                     'situation. But I can see '\n",
      "                                                     'G',\n",
      "                                  'tokens': [' go',\n",
      "                                             ' with',\n",
      "                                             ' B',\n",
      "                                             'isp',\n",
      "                                             'ing',\n",
      "                                             ' in',\n",
      "                                             ' a',\n",
      "                                             ' point',\n",
      "                                             '-',\n",
      "                                             'fighting',\n",
      "                                             ' situation',\n",
      "                                             '.',\n",
      "                                             ' But',\n",
      "                                             ' I',\n",
      "                                             ' can',\n",
      "                                             ' see',\n",
      "                                             ' G'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             33.91120910644531,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_token': '-',\n",
      "                                  'max_value': 'high',\n",
      "                                  'max_value_token_index': 8,\n",
      "                                  'sentence_string': ' doesn<|endoftext|> huge '\n",
      "                                                     'market share in '\n",
      "                                                     \"'point-of-care rapid \"\n",
      "                                                     'tests used in hospitals',\n",
      "                                  'tokens': [' doesn',\n",
      "                                             '<|endoftext|>',\n",
      "                                             ' huge',\n",
      "                                             ' market',\n",
      "                                             ' share',\n",
      "                                             ' in',\n",
      "                                             \" '\",\n",
      "                                             'point',\n",
      "                                             '-',\n",
      "                                             'of',\n",
      "                                             '-',\n",
      "                                             'care',\n",
      "                                             ' rapid',\n",
      "                                             ' tests',\n",
      "                                             ' used',\n",
      "                                             ' in',\n",
      "                                             ' hospitals'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             2.900025844573975,\n",
      "                                             39.18094635009766,\n",
      "                                             0,\n",
      "                                             7.738319396972656,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_token': '-',\n",
      "                                  'max_value': 'high',\n",
      "                                  'max_value_token_index': 8,\n",
      "                                  'sentence_string': ', as<|endoftext|> the '\n",
      "                                                     'Sun Devils go '\n",
      "                                                     'head-to-head with the '\n",
      "                                                     'Blue Devils of',\n",
      "                                  'tokens': [',',\n",
      "                                             ' as',\n",
      "                                             '<|endoftext|>',\n",
      "                                             ' the',\n",
      "                                             ' Sun',\n",
      "                                             ' Devils',\n",
      "                                             ' go',\n",
      "                                             ' head',\n",
      "                                             '-',\n",
      "                                             'to',\n",
      "                                             '-',\n",
      "                                             'head',\n",
      "                                             ' with',\n",
      "                                             ' the',\n",
      "                                             ' Blue',\n",
      "                                             ' Devils',\n",
      "                                             ' of'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             33.09887313842773,\n",
      "                                             0,\n",
      "                                             1.359877228736877,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_value': 'low',\n",
      "                                  'sentence_string': 'Ċ1 cup waterĊĊ1 cup '\n",
      "                                                     'milkĊĊ1 teaspoon saltĊĊ5',\n",
      "                                  'tokens': ['Ċ',\n",
      "                                             '1',\n",
      "                                             ' cup',\n",
      "                                             ' water',\n",
      "                                             'Ċ',\n",
      "                                             'Ċ',\n",
      "                                             '1',\n",
      "                                             ' cup',\n",
      "                                             ' milk',\n",
      "                                             'Ċ',\n",
      "                                             'Ċ',\n",
      "                                             '1',\n",
      "                                             ' teaspoon',\n",
      "                                             ' salt',\n",
      "                                             'Ċ',\n",
      "                                             'Ċ',\n",
      "                                             '5'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_value': 'low',\n",
      "                                  'sentence_string': ': a six-month '\n",
      "                                                     'jet-setting junket of '\n",
      "                                                     'television and dance '\n",
      "                                                     'studios, A',\n",
      "                                  'tokens': [':',\n",
      "                                             ' a',\n",
      "                                             ' six',\n",
      "                                             '-',\n",
      "                                             'month',\n",
      "                                             ' jet',\n",
      "                                             '-',\n",
      "                                             'setting',\n",
      "                                             ' junk',\n",
      "                                             'et',\n",
      "                                             ' of',\n",
      "                                             ' television',\n",
      "                                             ' and',\n",
      "                                             ' dance',\n",
      "                                             ' studios',\n",
      "                                             ',',\n",
      "                                             ' A'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_value': 'low',\n",
      "                                  'sentence_string': 'owment.ĊĊBut U.S. '\n",
      "                                                     'interest in Chinese has '\n",
      "                                                     'fallen precipitously',\n",
      "                                  'tokens': ['owment',\n",
      "                                             '.',\n",
      "                                             'Ċ',\n",
      "                                             'Ċ',\n",
      "                                             'But',\n",
      "                                             ' U',\n",
      "                                             '.',\n",
      "                                             'S',\n",
      "                                             '.',\n",
      "                                             ' interest',\n",
      "                                             ' in',\n",
      "                                             ' Chinese',\n",
      "                                             ' has',\n",
      "                                             ' fallen',\n",
      "                                             ' precip',\n",
      "                                             'it',\n",
      "                                             'ously'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]}]}],\n",
      " 'timestamp': 1715322008.339694}\n"
     ]
    }
   ],
   "source": [
    "json_data = load_json_results('results/exp_1715322008.339694.json')\n",
    "pprint.pprint(json_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do analysis on loaded json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Older things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "predict_activations() got an unexpected keyword argument 'test_number'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# custom = [custom_accuracy(data, eps = 0.1) for data in all_data]\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m---> 19\u001b[0m     run()\n",
      "Cell \u001b[0;32mIn[4], line 6\u001b[0m, in \u001b[0;36mrun\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m():\n\u001b[0;32m----> 6\u001b[0m     data \u001b[38;5;241m=\u001b[39m get_predictions(\u001b[38;5;241m991\u001b[39m) \u001b[38;5;66;03m#806\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# for i in range(len(all_data)):\u001b[39;00m\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;66;03m# data = all_data[i]\u001b[39;00m\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;66;03m# print(feature_nums[i])\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m()\n",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m, in \u001b[0;36mget_predictions\u001b[0;34m(feature_num)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_predictions\u001b[39m(feature_num):\n\u001b[0;32m----> 2\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m predict_activations(feature_num, test_number\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, show_examples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m)\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m predictions\n",
      "\u001b[0;31mTypeError\u001b[0m: predict_activations() got an unexpected keyword argument 'test_number'"
     ]
    }
   ],
   "source": [
    "def get_predictions(feature_num):\n",
    "    predictions = predict_activations(feature_num, test_number=10, show_examples=8)\n",
    "    return predictions\n",
    "\n",
    "def run():\n",
    "    data = get_predictions(991) #806\n",
    "    # for i in range(len(all_data)):\n",
    "        # data = all_data[i]\n",
    "        # print(feature_nums[i])\n",
    "    print()\n",
    "    pprint.pprint(data)\n",
    "    custom = custom_accuracy(data)\n",
    "\n",
    "    print(custom)\n",
    "\n",
    "    # custom = [custom_accuracy(data, eps = 0.1) for data in all_data]\n",
    "\n",
    "for _ in range(1):\n",
    "    run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "### Losses\n",
    "def mse(data, normalize = False):\n",
    "    values = ([((elem[0]-elem[1])/(elem[0] if normalize else 1))**2 for elem in data])\n",
    "    return sum(values)/len(values)\n",
    "\n",
    "def nll_variant(data, eps = 1e-1):\n",
    "    values = ([np.log((min(elem) + eps)/(max(elem) + eps)) for elem in data])\n",
    "    return -sum(values)/len(values)\n",
    "\n",
    "def l1(data, normalize = True, eps = 0.1):\n",
    "    values = ([((eps + abs(elem[0]-elem[1]))/((max(elem) if normalize else 1) + eps))  for elem in data])\n",
    "    return sum(values)/len(values)\n",
    "\n",
    "### Plots\n",
    "def plot_mses_cdf(mses):\n",
    "    # Plotting the Mean Squared Errors (MSE) for each dataset\n",
    "    mses_sorted = np.sort(mses)\n",
    "    cdf = np.arange(1, len(mses_sorted)+1) / len(mses_sorted)\n",
    "    plt.plot(mses_sorted, cdf)\n",
    "    plt.title('Cumulative Distribution Function of MSEs')\n",
    "    plt.xlabel('MSE')\n",
    "    plt.ylabel('CDF')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def plot_probability_distribution(data, bins='auto', density=True, title = \"Default Title\"):\n",
    "    \"\"\"\n",
    "    Plots the probability distribution of the given data using a histogram.\n",
    "\n",
    "    Parameters:\n",
    "    - data (list or numpy array): The floating point numbers whose distribution you want to plot.\n",
    "    - bins (int, sequence or str, optional): The method for calculating histogram bins. Default is 'auto'.\n",
    "    - density (bool, optional): If True, the histogram is normalized to form a probability density,\n",
    "                                i.e., the area under the histogram will sum to 1. Default is True.\n",
    "    \"\"\"\n",
    "    # Calculate the histogram\n",
    "    counts, bin_edges = np.histogram(data, bins=bins, density=density)\n",
    "\n",
    "    # Calculate bin centers\n",
    "    bin_centers = 0.5 * (bin_edges[:-1] + bin_edges[1:])\n",
    "\n",
    "    # Plotting the histogram\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.bar(bin_centers, counts*np.diff(bin_edges), align='center', width=np.diff(bin_edges), edgecolor='black', alpha=0.7)\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Probability Density')\n",
    "    plt.title('Probability Distribution of Data')\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def analyze_data(all_data):\n",
    "    mses = [mse(data, normalize = False) for data in all_data]\n",
    "    nlls = [nll_variant(data) for data in all_data]\n",
    "    l1s = [l1(data, normalize = True) for data in all_data]\n",
    "\n",
    "    print('l1s', sorted(l1s))\n",
    "    plot_probability_distribution(mses, title = \"Distribution of MSEs\")\n",
    "    plot_probability_distribution(nlls, title = \"Distribution of NLL variant\")\n",
    "    plot_probability_distribution(l1s, title = \"Distribution of l1s variant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'concurrent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[74], line 9\u001b[0m\n\u001b[1;32m      5\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m predict_activations(feature_num, test_number\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, show_examples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m)\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m predictions\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m concurrent\u001b[38;5;241m.\u001b[39mfutures\u001b[38;5;241m.\u001b[39mThreadPoolExecutor() \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[1;32m     10\u001b[0m     all_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(executor\u001b[38;5;241m.\u001b[39mmap(get_predictions, feature_nums))\n\u001b[1;32m     12\u001b[0m mses \u001b[38;5;241m=\u001b[39m [mse(data, normalize \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m all_data]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'concurrent' is not defined"
     ]
    }
   ],
   "source": [
    "feature_nums = [806]#random.sample(range(0, 1000), 10)\n",
    "\n",
    "def get_predictions(feature_num):\n",
    "    predictions = predict_activations(feature_num, test_number=10, show_examples=8)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    all_data = list(executor.map(get_predictions, feature_nums))\n",
    "\n",
    "mses = [mse(data, normalize = False) for data in all_data]\n",
    "nlls = [nll_variant(data) for data in all_data]\n",
    "l1s = [l1(data, normalize = True) for data in all_data]\n",
    "\n",
    "# print('l1s', sorted(l1s))\n",
    "plot_probability_distribution(mses, title = \"Distribution of MSEs\")\n",
    "plot_probability_distribution(nlls, title = \"Distribution of NLL variant\")\n",
    "plot_probability_distribution(l1s, title = \"Distribution of l1s variant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_accuracy(data):\n",
    "    eps = max([elem[0] for elem in data]) / 10\n",
    "    values = []\n",
    "    for elem in data:\n",
    "        true, pred = elem\n",
    "        ## Add eps to avoid zero case\n",
    "        true, pred = true + eps, pred + eps\n",
    "        # Scale values\n",
    "        true, pred = true ** 0.75, pred ** 0.75\n",
    "        # Calculate difference\n",
    "        difference = abs(true - pred)\n",
    "        # Take ratio\n",
    "        error = difference / max(true, pred)\n",
    "        \n",
    "        accuracy = 1 - error\n",
    "        values.append(accuracy)\n",
    "    return sum(values)/len(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'feature_nums' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m feature_nums\n",
      "\u001b[0;31mNameError\u001b[0m: name 'feature_nums' is not defined"
     ]
    }
   ],
   "source": [
    "feature_nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('You are evaluating an english description of an autoencoder feature. The '\n",
      " 'description should correspond to sentences which result in high activation. '\n",
      " 'The english description of the feature is: \" past tense verbs\"\\n'\n",
      " 'Here are 8 examples of sentences and their corresponding activations:\\n'\n",
      " ' Example: \" economy\\'s cooled off enough, but it wasn\\'t always so. Back in '\n",
      " 'the mid\", Activation: 19.96\\n'\n",
      " 'Example: \" NL<|endoftext|>,\" Watts said.ĊĊRubio\\'s disclosure sheds new '\n",
      " 'light on his\", Activation: 0.00\\n'\n",
      " 'Example: \" in their NL<|endoftext|>,\" Watts said.ĊĊRubio\\'s disclosure sheds '\n",
      " 'new light\", Activation: 0.00\\n'\n",
      " 'Example: \"ĊĊRubio\\'s disclosure sheds new light on his comments in October, '\n",
      " 'when he\", Activation: 0.00\\n'\n",
      " 'Example: \" be sure to add a great feel and glitz to any game. These '\n",
      " 'wonderful futuristic\", Activation: 0.00\\n'\n",
      " 'Example: \" their NL<|endoftext|>,\" Watts said.ĊĊRubio\\'s disclosure sheds '\n",
      " 'new light on\", Activation: 0.00\\n'\n",
      " 'Example: \" of California, proved that prawns eat the mollusc hosts. He '\n",
      " 'shared\", Activation: 0.00\\n'\n",
      " 'Example: \" songs,\" Willow said. \"These songs were created through characters '\n",
      " 'I have developed within my\", Activation: 16.54\\n'\n",
      " 'Use the provided samples and the provided description to predict the '\n",
      " 'activation on a new sentence.\\n'\n",
      " 'You MUST respond with ONLY a number and NO OTHER content.')\n",
      "[{'activation': 21.34770965576172,\n",
      "  'max_index': 8,\n",
      "  'sentence_string': ' the surface area of the land. This was the allocation '\n",
      "                     'that was specified in the plan'},\n",
      " {'activation': 21.08387565612793,\n",
      "  'max_index': 8,\n",
      "  'sentence_string': \" to their online bordcasting but they didn't treated \"\n",
      "                     'well who play in the NASL'},\n",
      " {'activation': 15.00903415679932,\n",
      "  'max_index': 8,\n",
      "  'sentence_string': ' industry for add-ons to applications that were not '\n",
      "                     'meant to be extendible.<|endoftext|>'},\n",
      " {'activation': 0,\n",
      "  'max_index': 0,\n",
      "  'sentence_string': ' Central American folklore. It is a shape-changing '\n",
      "                     'spirit that typically takes the form of'},\n",
      " {'activation': 24.55836486816406,\n",
      "  'max_index': 8,\n",
      "  'sentence_string': ' Great Value, and other brands. It was John Henry Heinz, '\n",
      "                     'in fact,'},\n",
      " {'activation': 0,\n",
      "  'max_index': 0,\n",
      "  'sentence_string': ' comments in October, when he warned his fellow '\n",
      "                     'Republicans against trying to make political hay out'},\n",
      " {'activation': 0,\n",
      "  'max_index': 0,\n",
      "  'sentence_string': \"Rubio's disclosure sheds new light on his comments in \"\n",
      "                     'October, when he warned his'},\n",
      " {'activation': 0,\n",
      "  'max_index': 0,\n",
      "  'sentence_string': ' sheds new light on his comments in October, when he '\n",
      "                     'warned his fellow Republicans against trying'},\n",
      " {'activation': 0,\n",
      "  'max_index': 0,\n",
      "  'sentence_string': \"ĊRubio's disclosure sheds new light on his comments in \"\n",
      "                     'October, when he warned'},\n",
      " {'activation': 12.68032932281494,\n",
      "  'max_index': 8,\n",
      "  'sentence_string': ' disrupt the discourse with other people, who were '\n",
      "                     'either supportive or who were neutral and just'}]\n",
      "[(21.34770965576172, 0.0),\n",
      " (21.08387565612793, 0.0),\n",
      " (15.00903415679932, 0.0),\n",
      " (0, 0.0),\n",
      " (24.55836486816406, 0.0),\n",
      " (0, 15.23),\n",
      " (0, 0.0),\n",
      " (0, 0.0),\n",
      " (0, 0.0),\n",
      " (12.68032932281494, 0.0)]\n",
      "0.4124388582827179\n"
     ]
    }
   ],
   "source": [
    "def run():\n",
    "    data = get_predictions(806)\n",
    "    # for i in range(len(all_data)):\n",
    "        # data = all_data[i]\n",
    "        # print(feature_nums[i])\n",
    "    pprint.pprint(data)\n",
    "    custom = custom_accuracy(data)\n",
    "    print(custom)\n",
    "\n",
    "    # custom = [custom_accuracy(data, eps = 0.1) for data in all_data]\n",
    "\n",
    "for _ in range(1):\n",
    "    run()\n",
    "\n",
    "# plot_probability_distribution(custom, title = \"Distribution of custom accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[80], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m analyze_data(all_data)\n",
      "File \u001b[0;32m~/Desktop/GitHub/feature_benchmark/prediction_analysis.py:57\u001b[0m, in \u001b[0;36manalyze_data\u001b[0;34m(all_data)\u001b[0m\n",
      "File \u001b[0;32m~/Desktop/GitHub/feature_benchmark/prediction_analysis.py:7\u001b[0m, in \u001b[0;36mmse\u001b[0;34m(data, normalize)\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "analyze_data(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
