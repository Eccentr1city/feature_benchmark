{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getting_examples import *\n",
    "import pprint\n",
    "from predict_activations import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run an experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hyperparameters': {'binary_class': True,\n",
      "                     'debug': False,\n",
      "                     'neg_type': 'others',\n",
      "                     'num_completions': 1,\n",
      "                     'randomize_pos': False,\n",
      "                     'seed': 42,\n",
      "                     'show_max_token': True,\n",
      "                     'show_neg': 0,\n",
      "                     'show_pos': 0,\n",
      "                     'test_neg': 3,\n",
      "                     'test_pos': 3},\n",
      " 'num_features': 20,\n",
      " 'results': [{'description': 'timestamps or dates in a specific format',\n",
      "              'feature_index': 521,\n",
      "              'gpt_predictions': [(1, 0.0),\n",
      "                                  (1, 0.0),\n",
      "                                  (1, 0.0),\n",
      "                                  (0, 0.0),\n",
      "                                  (0, 0.0),\n",
      "                                  (0, 0.0)],\n",
      "              'highest_activation': 51.19043350219727,\n",
      "              'show_sentences': [],\n",
      "              'test_sentences': [{'max_token': ' posted',\n",
      "                                  'max_value': 'high',\n",
      "                                  'max_value_token_index': 8,\n",
      "                                  'sentence_string': ' health, '\n",
      "                                                     'australiaĊĊFirst '\n",
      "                                                     \"posted<|endoftext|>CARLTON'S \"\n",
      "                                                     'Dylan Buckley',\n",
      "                                  'tokens': [' health',\n",
      "                                             ',',\n",
      "                                             ' aust',\n",
      "                                             'ral',\n",
      "                                             'ia',\n",
      "                                             'Ċ',\n",
      "                                             'Ċ',\n",
      "                                             'First',\n",
      "                                             ' posted',\n",
      "                                             '<|endoftext|>',\n",
      "                                             'CAR',\n",
      "                                             'L',\n",
      "                                             'TON',\n",
      "                                             \"'\",\n",
      "                                             'S',\n",
      "                                             ' Dylan',\n",
      "                                             ' Buckley'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             51.19043350219727,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_token': ' posted',\n",
      "                                  'max_value': 'high',\n",
      "                                  'max_value_token_index': 8,\n",
      "                                  'sentence_string': ' sport, hondurasĊĊFirst '\n",
      "                                                     'posted<|endoftext|>Just '\n",
      "                                                     'after a midnight '\n",
      "                                                     'deadline that could',\n",
      "                                  'tokens': [' sport',\n",
      "                                             ',',\n",
      "                                             ' h',\n",
      "                                             'ond',\n",
      "                                             'uras',\n",
      "                                             'Ċ',\n",
      "                                             'Ċ',\n",
      "                                             'First',\n",
      "                                             ' posted',\n",
      "                                             '<|endoftext|>',\n",
      "                                             'Just',\n",
      "                                             ' after',\n",
      "                                             ' a',\n",
      "                                             ' midnight',\n",
      "                                             ' deadline',\n",
      "                                             ' that',\n",
      "                                             ' could'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             50.3966178894043,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_token': ' posted',\n",
      "                                  'max_value': 'high',\n",
      "                                  'max_value_token_index': 8,\n",
      "                                  'sentence_string': ', melbourne-3000ĊĊFirst '\n",
      "                                                     'posted<|endoftext|>You '\n",
      "                                                     'should know this about '\n",
      "                                                     'offensive line',\n",
      "                                  'tokens': [',',\n",
      "                                             ' mel',\n",
      "                                             'bourne',\n",
      "                                             '-',\n",
      "                                             '3000',\n",
      "                                             'Ċ',\n",
      "                                             'Ċ',\n",
      "                                             'First',\n",
      "                                             ' posted',\n",
      "                                             '<|endoftext|>',\n",
      "                                             'You',\n",
      "                                             ' should',\n",
      "                                             ' know',\n",
      "                                             ' this',\n",
      "                                             ' about',\n",
      "                                             ' offensive',\n",
      "                                             ' line'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             50.26014709472656,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_value': 'low',\n",
      "                                  'sentence_string': ' the disease remains '\n",
      "                                                     'largely unknown, '\n",
      "                                                     'genetic, viral, hormonal '\n",
      "                                                     'and environmental '\n",
      "                                                     'factors have been so',\n",
      "                                  'tokens': [' the',\n",
      "                                             ' disease',\n",
      "                                             ' remains',\n",
      "                                             ' largely',\n",
      "                                             ' unknown',\n",
      "                                             ',',\n",
      "                                             ' genetic',\n",
      "                                             ',',\n",
      "                                             ' viral',\n",
      "                                             ',',\n",
      "                                             ' hormonal',\n",
      "                                             ' and',\n",
      "                                             ' environmental',\n",
      "                                             ' factors',\n",
      "                                             ' have',\n",
      "                                             ' been',\n",
      "                                             ' so'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_value': 'low',\n",
      "                                  'sentence_string': ' of these are the ones '\n",
      "                                                     'associated with '\n",
      "                                                     'bacterial virulence ( 9 '\n",
      "                                                     'âĢĵ 11 ). The most',\n",
      "                                  'tokens': [' of',\n",
      "                                             ' these',\n",
      "                                             ' are',\n",
      "                                             ' the',\n",
      "                                             ' ones',\n",
      "                                             ' associated',\n",
      "                                             ' with',\n",
      "                                             ' bacterial',\n",
      "                                             ' vir',\n",
      "                                             'ulence',\n",
      "                                             ' (',\n",
      "                                             ' 9',\n",
      "                                             ' âĢĵ',\n",
      "                                             ' 11',\n",
      "                                             ' ).',\n",
      "                                             ' The',\n",
      "                                             ' most'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_value': 'low',\n",
      "                                  'sentence_string': ' height at some '\n",
      "                                                     'international '\n",
      "                                                     'summit.ĊĊOr is it '\n",
      "                                                     'because of these deeply '\n",
      "                                                     'controversial statements',\n",
      "                                  'tokens': [' height',\n",
      "                                             ' at',\n",
      "                                             ' some',\n",
      "                                             ' international',\n",
      "                                             ' summit',\n",
      "                                             '.',\n",
      "                                             'Ċ',\n",
      "                                             'Ċ',\n",
      "                                             'Or',\n",
      "                                             ' is',\n",
      "                                             ' it',\n",
      "                                             ' because',\n",
      "                                             ' of',\n",
      "                                             ' these',\n",
      "                                             ' deeply',\n",
      "                                             ' controversial',\n",
      "                                             ' statements'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]}]},\n",
      "             {'description': 'references to locations or events related to the '\n",
      "                             'state of Florida',\n",
      "              'feature_index': 737,\n",
      "              'gpt_predictions': [(1, 0.0),\n",
      "                                  (1, 1.0),\n",
      "                                  (1, 1.0),\n",
      "                                  (0, 0.0),\n",
      "                                  (0, 0.0),\n",
      "                                  (0, 0.0)],\n",
      "              'highest_activation': 26.77979850769043,\n",
      "              'show_sentences': [],\n",
      "              'test_sentences': [{'max_token': ' Flor',\n",
      "                                  'max_value': 'high',\n",
      "                                  'max_value_token_index': 8,\n",
      "                                  'sentence_string': ' like Rio, Sao Paulo, '\n",
      "                                                     'Salvador, Florianopolis '\n",
      "                                                     'and such. Try going to',\n",
      "                                  'tokens': [' like',\n",
      "                                             ' Rio',\n",
      "                                             ',',\n",
      "                                             ' Sao',\n",
      "                                             ' Paulo',\n",
      "                                             ',',\n",
      "                                             ' Salvador',\n",
      "                                             ',',\n",
      "                                             ' Flor',\n",
      "                                             'ian',\n",
      "                                             'opolis',\n",
      "                                             ' and',\n",
      "                                             ' such',\n",
      "                                             '.',\n",
      "                                             ' Try',\n",
      "                                             ' going',\n",
      "                                             ' to'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             26.77979850769043,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_token': ' Flor',\n",
      "                                  'max_value': 'high',\n",
      "                                  'max_value_token_index': 8,\n",
      "                                  'sentence_string': ' this because I went '\n",
      "                                                     'there recently with the '\n",
      "                                                     'Floridians, also '\n",
      "                                                     'known<|endoftext|> '\n",
      "                                                     'Deborah to',\n",
      "                                  'tokens': [' this',\n",
      "                                             ' because',\n",
      "                                             ' I',\n",
      "                                             ' went',\n",
      "                                             ' there',\n",
      "                                             ' recently',\n",
      "                                             ' with',\n",
      "                                             ' the',\n",
      "                                             ' Flor',\n",
      "                                             'id',\n",
      "                                             'ians',\n",
      "                                             ',',\n",
      "                                             ' also',\n",
      "                                             ' known',\n",
      "                                             '<|endoftext|>',\n",
      "                                             ' Deborah',\n",
      "                                             ' to'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             26.34127616882324,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_token': ' Flor',\n",
      "                                  'max_value': 'high',\n",
      "                                  'max_value_token_index': 8,\n",
      "                                  'sentence_string': ' so this trip was about '\n",
      "                                                     'making sure the '\n",
      "                                                     'Floridians saw as much '\n",
      "                                                     'as possible in',\n",
      "                                  'tokens': [' so',\n",
      "                                             ' this',\n",
      "                                             ' trip',\n",
      "                                             ' was',\n",
      "                                             ' about',\n",
      "                                             ' making',\n",
      "                                             ' sure',\n",
      "                                             ' the',\n",
      "                                             ' Flor',\n",
      "                                             'id',\n",
      "                                             'ians',\n",
      "                                             ' saw',\n",
      "                                             ' as',\n",
      "                                             ' much',\n",
      "                                             ' as',\n",
      "                                             ' possible',\n",
      "                                             ' in'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             25.96149253845215,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_value': 'low',\n",
      "                                  'sentence_string': ' the disease remains '\n",
      "                                                     'largely unknown, '\n",
      "                                                     'genetic, viral, hormonal '\n",
      "                                                     'and environmental '\n",
      "                                                     'factors have been so',\n",
      "                                  'tokens': [' the',\n",
      "                                             ' disease',\n",
      "                                             ' remains',\n",
      "                                             ' largely',\n",
      "                                             ' unknown',\n",
      "                                             ',',\n",
      "                                             ' genetic',\n",
      "                                             ',',\n",
      "                                             ' viral',\n",
      "                                             ',',\n",
      "                                             ' hormonal',\n",
      "                                             ' and',\n",
      "                                             ' environmental',\n",
      "                                             ' factors',\n",
      "                                             ' have',\n",
      "                                             ' been',\n",
      "                                             ' so'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_value': 'low',\n",
      "                                  'sentence_string': ' of these are the ones '\n",
      "                                                     'associated with '\n",
      "                                                     'bacterial virulence ( 9 '\n",
      "                                                     'âĢĵ 11 ). The most',\n",
      "                                  'tokens': [' of',\n",
      "                                             ' these',\n",
      "                                             ' are',\n",
      "                                             ' the',\n",
      "                                             ' ones',\n",
      "                                             ' associated',\n",
      "                                             ' with',\n",
      "                                             ' bacterial',\n",
      "                                             ' vir',\n",
      "                                             'ulence',\n",
      "                                             ' (',\n",
      "                                             ' 9',\n",
      "                                             ' âĢĵ',\n",
      "                                             ' 11',\n",
      "                                             ' ).',\n",
      "                                             ' The',\n",
      "                                             ' most'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_value': 'low',\n",
      "                                  'sentence_string': ' height at some '\n",
      "                                                     'international '\n",
      "                                                     'summit.ĊĊOr is it '\n",
      "                                                     'because of these deeply '\n",
      "                                                     'controversial statements',\n",
      "                                  'tokens': [' height',\n",
      "                                             ' at',\n",
      "                                             ' some',\n",
      "                                             ' international',\n",
      "                                             ' summit',\n",
      "                                             '.',\n",
      "                                             'Ċ',\n",
      "                                             'Ċ',\n",
      "                                             'Or',\n",
      "                                             ' is',\n",
      "                                             ' it',\n",
      "                                             ' because',\n",
      "                                             ' of',\n",
      "                                             ' these',\n",
      "                                             ' deeply',\n",
      "                                             ' controversial',\n",
      "                                             ' statements'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]}]},\n",
      "             {'description': 'mentions of the name \"Christian\" along with '\n",
      "                             'various contexts like sports, events, and other '\n",
      "                             'individuals',\n",
      "              'feature_index': 740,\n",
      "              'gpt_predictions': [(1, 1.0),\n",
      "                                  (1, 1.0),\n",
      "                                  (1, 1.0),\n",
      "                                  (0, 0.0),\n",
      "                                  (0, 0.0),\n",
      "                                  (0, 0.0)],\n",
      "              'highest_activation': 33.55897903442383,\n",
      "              'show_sentences': [],\n",
      "              'test_sentences': [{'max_token': 'Christian',\n",
      "                                  'max_value': 'high',\n",
      "                                  'max_value_token_index': 8,\n",
      "                                  'sentence_string': ' (Brad Evans 69), Andy '\n",
      "                                                     'Rose (Christian Tiffert '\n",
      "                                                     '69), Mauro Rosales',\n",
      "                                  'tokens': [' (',\n",
      "                                             'Brad',\n",
      "                                             ' Evans',\n",
      "                                             ' 69',\n",
      "                                             '),',\n",
      "                                             ' Andy',\n",
      "                                             ' Rose',\n",
      "                                             ' (',\n",
      "                                             'Christian',\n",
      "                                             ' Tiff',\n",
      "                                             'ert',\n",
      "                                             ' 69',\n",
      "                                             '),',\n",
      "                                             ' Mau',\n",
      "                                             'ro',\n",
      "                                             ' Ros',\n",
      "                                             'ales'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             33.55897903442383,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_token': 'Christian',\n",
      "                                  'max_value': 'high',\n",
      "                                  'max_value_token_index': 8,\n",
      "                                  'sentence_string': ' a scene in which '\n",
      "                                                     'Patrick Bateman '\n",
      "                                                     '(Christian Bale) gives a '\n",
      "                                                     'critique of the Hue',\n",
      "                                  'tokens': [' a',\n",
      "                                             ' scene',\n",
      "                                             ' in',\n",
      "                                             ' which',\n",
      "                                             ' Patrick',\n",
      "                                             ' Bat',\n",
      "                                             'eman',\n",
      "                                             ' (',\n",
      "                                             'Christian',\n",
      "                                             ' Bale',\n",
      "                                             ')',\n",
      "                                             ' gives',\n",
      "                                             ' a',\n",
      "                                             ' critique',\n",
      "                                             ' of',\n",
      "                                             ' the',\n",
      "                                             ' Hue'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             30.13565444946289,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_token': 'Christian',\n",
      "                                  'max_value': 'high',\n",
      "                                  'max_value_token_index': 8,\n",
      "                                  'sentence_string': ' Stanley Cup title in '\n",
      "                                                     'next.ĊĊChristian Ehrhoff '\n",
      "                                                     'scored at 2:41',\n",
      "                                  'tokens': [' Stanley',\n",
      "                                             ' Cup',\n",
      "                                             ' title',\n",
      "                                             ' in',\n",
      "                                             ' next',\n",
      "                                             '.',\n",
      "                                             'Ċ',\n",
      "                                             'Ċ',\n",
      "                                             'Christian',\n",
      "                                             ' E',\n",
      "                                             'hr',\n",
      "                                             'hoff',\n",
      "                                             ' scored',\n",
      "                                             ' at',\n",
      "                                             ' 2',\n",
      "                                             ':',\n",
      "                                             '41'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             29.58237266540527,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_value': 'low',\n",
      "                                  'sentence_string': ' the disease remains '\n",
      "                                                     'largely unknown, '\n",
      "                                                     'genetic, viral, hormonal '\n",
      "                                                     'and environmental '\n",
      "                                                     'factors have been so',\n",
      "                                  'tokens': [' the',\n",
      "                                             ' disease',\n",
      "                                             ' remains',\n",
      "                                             ' largely',\n",
      "                                             ' unknown',\n",
      "                                             ',',\n",
      "                                             ' genetic',\n",
      "                                             ',',\n",
      "                                             ' viral',\n",
      "                                             ',',\n",
      "                                             ' hormonal',\n",
      "                                             ' and',\n",
      "                                             ' environmental',\n",
      "                                             ' factors',\n",
      "                                             ' have',\n",
      "                                             ' been',\n",
      "                                             ' so'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_value': 'low',\n",
      "                                  'sentence_string': ' of these are the ones '\n",
      "                                                     'associated with '\n",
      "                                                     'bacterial virulence ( 9 '\n",
      "                                                     'âĢĵ 11 ). The most',\n",
      "                                  'tokens': [' of',\n",
      "                                             ' these',\n",
      "                                             ' are',\n",
      "                                             ' the',\n",
      "                                             ' ones',\n",
      "                                             ' associated',\n",
      "                                             ' with',\n",
      "                                             ' bacterial',\n",
      "                                             ' vir',\n",
      "                                             'ulence',\n",
      "                                             ' (',\n",
      "                                             ' 9',\n",
      "                                             ' âĢĵ',\n",
      "                                             ' 11',\n",
      "                                             ' ).',\n",
      "                                             ' The',\n",
      "                                             ' most'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_value': 'low',\n",
      "                                  'sentence_string': ' height at some '\n",
      "                                                     'international '\n",
      "                                                     'summit.ĊĊOr is it '\n",
      "                                                     'because of these deeply '\n",
      "                                                     'controversial statements',\n",
      "                                  'tokens': [' height',\n",
      "                                             ' at',\n",
      "                                             ' some',\n",
      "                                             ' international',\n",
      "                                             ' summit',\n",
      "                                             '.',\n",
      "                                             'Ċ',\n",
      "                                             'Ċ',\n",
      "                                             'Or',\n",
      "                                             ' is',\n",
      "                                             ' it',\n",
      "                                             ' because',\n",
      "                                             ' of',\n",
      "                                             ' these',\n",
      "                                             ' deeply',\n",
      "                                             ' controversial',\n",
      "                                             ' statements'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]}]},\n",
      "             {'description': 'metaphorical expressions related to success and '\n",
      "                             'achievement',\n",
      "              'feature_index': 660,\n",
      "              'gpt_predictions': [(1, 1.0),\n",
      "                                  (1, 0.0),\n",
      "                                  (1, 0.0),\n",
      "                                  (0, 0.0),\n",
      "                                  (0, 0.0),\n",
      "                                  (0, 0.0)],\n",
      "              'highest_activation': 11.15029907226562,\n",
      "              'show_sentences': [],\n",
      "              'test_sentences': [{'max_token': ' fish',\n",
      "                                  'max_value': 'high',\n",
      "                                  'max_value_token_index': 8,\n",
      "                                  'sentence_string': \", 'Yeah, you're a big \"\n",
      "                                                     'fish in a small pond. '\n",
      "                                                     'The only reason',\n",
      "                                  'tokens': [',',\n",
      "                                             \" '\",\n",
      "                                             'Yeah',\n",
      "                                             ',',\n",
      "                                             ' you',\n",
      "                                             \"'re\",\n",
      "                                             ' a',\n",
      "                                             ' big',\n",
      "                                             ' fish',\n",
      "                                             ' in',\n",
      "                                             ' a',\n",
      "                                             ' small',\n",
      "                                             ' pond',\n",
      "                                             '.',\n",
      "                                             ' The',\n",
      "                                             ' only',\n",
      "                                             ' reason'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             11.15029907226562,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             2.158911228179932,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_token': ' mob',\n",
      "                                  'max_value': 'high',\n",
      "                                  'max_value_token_index': 8,\n",
      "                                  'sentence_string': ' anatomy and crap. I '\n",
      "                                                     'even act the mob doctor '\n",
      "                                                     'to my friends when they '\n",
      "                                                     \"don't\",\n",
      "                                  'tokens': [' anatomy',\n",
      "                                             ' and',\n",
      "                                             ' crap',\n",
      "                                             '.',\n",
      "                                             ' I',\n",
      "                                             ' even',\n",
      "                                             ' act',\n",
      "                                             ' the',\n",
      "                                             ' mob',\n",
      "                                             ' doctor',\n",
      "                                             ' to',\n",
      "                                             ' my',\n",
      "                                             ' friends',\n",
      "                                             ' when',\n",
      "                                             ' they',\n",
      "                                             ' don',\n",
      "                                             \"'t\"],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             11.09251594543457,\n",
      "                                             6.619546413421631,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_token': ' play',\n",
      "                                  'max_value': 'high',\n",
      "                                  'max_value_token_index': 8,\n",
      "                                  'sentence_string': ' startups are '\n",
      "                                                     'team<|endoftext|> who '\n",
      "                                                     'makes the bigger play in '\n",
      "                                                     'practice.ĊĊ\"We\\'ll',\n",
      "                                  'tokens': [' startups',\n",
      "                                             ' are',\n",
      "                                             ' team',\n",
      "                                             '<|endoftext|>',\n",
      "                                             ' who',\n",
      "                                             ' makes',\n",
      "                                             ' the',\n",
      "                                             ' bigger',\n",
      "                                             ' play',\n",
      "                                             ' in',\n",
      "                                             ' practice',\n",
      "                                             '.',\n",
      "                                             'Ċ',\n",
      "                                             'Ċ',\n",
      "                                             '\"',\n",
      "                                             'We',\n",
      "                                             \"'ll\"],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             2.549750328063965,\n",
      "                                             10.69255638122559,\n",
      "                                             0.302903950214386,\n",
      "                                             1.651447653770447,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_value': 'low',\n",
      "                                  'sentence_string': ' the disease remains '\n",
      "                                                     'largely unknown, '\n",
      "                                                     'genetic, viral, hormonal '\n",
      "                                                     'and environmental '\n",
      "                                                     'factors have been so',\n",
      "                                  'tokens': [' the',\n",
      "                                             ' disease',\n",
      "                                             ' remains',\n",
      "                                             ' largely',\n",
      "                                             ' unknown',\n",
      "                                             ',',\n",
      "                                             ' genetic',\n",
      "                                             ',',\n",
      "                                             ' viral',\n",
      "                                             ',',\n",
      "                                             ' hormonal',\n",
      "                                             ' and',\n",
      "                                             ' environmental',\n",
      "                                             ' factors',\n",
      "                                             ' have',\n",
      "                                             ' been',\n",
      "                                             ' so'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_value': 'low',\n",
      "                                  'sentence_string': ' of these are the ones '\n",
      "                                                     'associated with '\n",
      "                                                     'bacterial virulence ( 9 '\n",
      "                                                     'âĢĵ 11 ). The most',\n",
      "                                  'tokens': [' of',\n",
      "                                             ' these',\n",
      "                                             ' are',\n",
      "                                             ' the',\n",
      "                                             ' ones',\n",
      "                                             ' associated',\n",
      "                                             ' with',\n",
      "                                             ' bacterial',\n",
      "                                             ' vir',\n",
      "                                             'ulence',\n",
      "                                             ' (',\n",
      "                                             ' 9',\n",
      "                                             ' âĢĵ',\n",
      "                                             ' 11',\n",
      "                                             ' ).',\n",
      "                                             ' The',\n",
      "                                             ' most'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_value': 'low',\n",
      "                                  'sentence_string': ' height at some '\n",
      "                                                     'international '\n",
      "                                                     'summit.ĊĊOr is it '\n",
      "                                                     'because of these deeply '\n",
      "                                                     'controversial statements',\n",
      "                                  'tokens': [' height',\n",
      "                                             ' at',\n",
      "                                             ' some',\n",
      "                                             ' international',\n",
      "                                             ' summit',\n",
      "                                             '.',\n",
      "                                             'Ċ',\n",
      "                                             'Ċ',\n",
      "                                             'Or',\n",
      "                                             ' is',\n",
      "                                             ' it',\n",
      "                                             ' because',\n",
      "                                             ' of',\n",
      "                                             ' these',\n",
      "                                             ' deeply',\n",
      "                                             ' controversial',\n",
      "                                             ' statements'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]}]},\n",
      "             {'description': 'references to locations in the United States',\n",
      "              'feature_index': 411,\n",
      "              'gpt_predictions': [(1, 1.0),\n",
      "                                  (1, 1.0),\n",
      "                                  (1, 1.0),\n",
      "                                  (0, 0.0),\n",
      "                                  (0, 0.0),\n",
      "                                  (0, 0.0)],\n",
      "              'highest_activation': 33.55773544311523,\n",
      "              'show_sentences': [],\n",
      "              'test_sentences': [{'max_token': 'S',\n",
      "                                  'max_value': 'high',\n",
      "                                  'max_value_token_index': 8,\n",
      "                                  'sentence_string': ' more than 40 locations '\n",
      "                                                     'in the U.S.ĊĊThe '\n",
      "                                                     'Rickmobile is also',\n",
      "                                  'tokens': [' more',\n",
      "                                             ' than',\n",
      "                                             ' 40',\n",
      "                                             ' locations',\n",
      "                                             ' in',\n",
      "                                             ' the',\n",
      "                                             ' U',\n",
      "                                             '.',\n",
      "                                             'S',\n",
      "                                             '.',\n",
      "                                             'Ċ',\n",
      "                                             'Ċ',\n",
      "                                             'The',\n",
      "                                             ' Rick',\n",
      "                                             'mobile',\n",
      "                                             ' is',\n",
      "                                             ' also'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             7.931962490081787,\n",
      "                                             0,\n",
      "                                             33.55773544311523,\n",
      "                                             21.31204223632812,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_token': 'S',\n",
      "                                  'max_value': 'high',\n",
      "                                  'max_value_token_index': 8,\n",
      "                                  'sentence_string': 'profitable light trucks '\n",
      "                                                     'in the U.S. while '\n",
      "                                                     'investing in new '\n",
      "                                                     'capacity in Mexico',\n",
      "                                  'tokens': ['prof',\n",
      "                                             'itable',\n",
      "                                             ' light',\n",
      "                                             ' trucks',\n",
      "                                             ' in',\n",
      "                                             ' the',\n",
      "                                             ' U',\n",
      "                                             '.',\n",
      "                                             'S',\n",
      "                                             '.',\n",
      "                                             ' while',\n",
      "                                             ' investing',\n",
      "                                             ' in',\n",
      "                                             ' new',\n",
      "                                             ' capacity',\n",
      "                                             ' in',\n",
      "                                             ' Mexico'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             7.076316356658936,\n",
      "                                             0,\n",
      "                                             33.3417854309082,\n",
      "                                             22.65333557128906,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_token': 'S',\n",
      "                                  'max_value': 'high',\n",
      "                                  'max_value_token_index': 8,\n",
      "                                  'sentence_string': ' stint as an artist in '\n",
      "                                                     'the U.S. Air Force, '\n",
      "                                                     'working as an illust',\n",
      "                                  'tokens': [' stint',\n",
      "                                             ' as',\n",
      "                                             ' an',\n",
      "                                             ' artist',\n",
      "                                             ' in',\n",
      "                                             ' the',\n",
      "                                             ' U',\n",
      "                                             '.',\n",
      "                                             'S',\n",
      "                                             '.',\n",
      "                                             ' Air',\n",
      "                                             ' Force',\n",
      "                                             ',',\n",
      "                                             ' working',\n",
      "                                             ' as',\n",
      "                                             ' an',\n",
      "                                             ' illust'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             1.072690963745117,\n",
      "                                             0,\n",
      "                                             32.38297653198242,\n",
      "                                             24.99531936645508,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_value': 'low',\n",
      "                                  'sentence_string': ' time before a fetus was '\n",
      "                                                     'infected from the '\n",
      "                                                     'virus,\" he said.ĊĊ\"The',\n",
      "                                  'tokens': [' time',\n",
      "                                             ' before',\n",
      "                                             ' a',\n",
      "                                             ' fetus',\n",
      "                                             ' was',\n",
      "                                             ' infected',\n",
      "                                             ' from',\n",
      "                                             ' the',\n",
      "                                             ' virus',\n",
      "                                             ',\"',\n",
      "                                             ' he',\n",
      "                                             ' said',\n",
      "                                             '.',\n",
      "                                             'Ċ',\n",
      "                                             'Ċ',\n",
      "                                             '\"',\n",
      "                                             'The'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_value': 'low',\n",
      "                                  'sentence_string': ' also provides an '\n",
      "                                                     'exaggerated account of '\n",
      "                                                     \"Prithviraj's war against \"\n",
      "                                                     'the Chandelas',\n",
      "                                  'tokens': [' also',\n",
      "                                             ' provides',\n",
      "                                             ' an',\n",
      "                                             ' exaggerated',\n",
      "                                             ' account',\n",
      "                                             ' of',\n",
      "                                             ' Pr',\n",
      "                                             'ith',\n",
      "                                             'vir',\n",
      "                                             'aj',\n",
      "                                             \"'s\",\n",
      "                                             ' war',\n",
      "                                             ' against',\n",
      "                                             ' the',\n",
      "                                             ' Chand',\n",
      "                                             'el',\n",
      "                                             'as'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_value': 'low',\n",
      "                                  'sentence_string': ' when soldiers and '\n",
      "                                                     'sailors were swimming in '\n",
      "                                                     'mustard agent when '\n",
      "                                                     'Germans bombed U.S. '\n",
      "                                                     'ships',\n",
      "                                  'tokens': [' when',\n",
      "                                             ' soldiers',\n",
      "                                             ' and',\n",
      "                                             ' sailors',\n",
      "                                             ' were',\n",
      "                                             ' swimming',\n",
      "                                             ' in',\n",
      "                                             ' mustard',\n",
      "                                             ' agent',\n",
      "                                             ' when',\n",
      "                                             ' Germans',\n",
      "                                             ' bombed',\n",
      "                                             ' U',\n",
      "                                             '.',\n",
      "                                             'S',\n",
      "                                             '.',\n",
      "                                             ' ships'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]}]},\n",
      "             {'description': 'numbers and financial terms',\n",
      "              'feature_index': 678,\n",
      "              'gpt_predictions': [(1, 1.0),\n",
      "                                  (1, 1.0),\n",
      "                                  (1, 0.0),\n",
      "                                  (0, 0.0),\n",
      "                                  (0, 0.0),\n",
      "                                  (0, 0.0)],\n",
      "              'highest_activation': 7.758599281311035,\n",
      "              'show_sentences': [],\n",
      "              'test_sentences': [{'max_token': ' year',\n",
      "                                  'max_value': 'high',\n",
      "                                  'max_value_token_index': 10,\n",
      "                                  'sentence_string': ' the backbone of a $7 '\n",
      "                                                     'trillion-dollar a year '\n",
      "                                                     'marketplace by 2050. '\n",
      "                                                     '\"Once',\n",
      "                                  'tokens': [' the',\n",
      "                                             ' backbone',\n",
      "                                             ' of',\n",
      "                                             ' a',\n",
      "                                             ' $',\n",
      "                                             '7',\n",
      "                                             ' trillion',\n",
      "                                             '-',\n",
      "                                             'dollar',\n",
      "                                             ' a',\n",
      "                                             ' year',\n",
      "                                             ' marketplace',\n",
      "                                             ' by',\n",
      "                                             ' 2050',\n",
      "                                             '.',\n",
      "                                             ' \"',\n",
      "                                             'Once'],\n",
      "                                  'values': [0.6214272975921631,\n",
      "                                             1.489738345146179,\n",
      "                                             3.367692232131958,\n",
      "                                             2.974807500839233,\n",
      "                                             4.009978771209717,\n",
      "                                             4.145596981048584,\n",
      "                                             4.90007495880127,\n",
      "                                             1.876375913619995,\n",
      "                                             6.055830478668213,\n",
      "                                             3.498064517974854,\n",
      "                                             7.758599281311035,\n",
      "                                             2.493829965591431,\n",
      "                                             0.8311287760734558,\n",
      "                                             0.7213778495788574,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_token': ' year',\n",
      "                                  'max_value': 'high',\n",
      "                                  'max_value_token_index': 8,\n",
      "                                  'sentence_string': ' of a $7 trillion-dollar '\n",
      "                                                     'a year marketplace by '\n",
      "                                                     '2050. \"Once money is',\n",
      "                                  'tokens': [' of',\n",
      "                                             ' a',\n",
      "                                             ' $',\n",
      "                                             '7',\n",
      "                                             ' trillion',\n",
      "                                             '-',\n",
      "                                             'dollar',\n",
      "                                             ' a',\n",
      "                                             ' year',\n",
      "                                             ' marketplace',\n",
      "                                             ' by',\n",
      "                                             ' 2050',\n",
      "                                             '.',\n",
      "                                             ' \"',\n",
      "                                             'Once',\n",
      "                                             ' money',\n",
      "                                             ' is'],\n",
      "                                  'values': [3.367692232131958,\n",
      "                                             2.974807500839233,\n",
      "                                             4.009978771209717,\n",
      "                                             4.145596981048584,\n",
      "                                             4.90007495880127,\n",
      "                                             1.876375913619995,\n",
      "                                             6.055830478668213,\n",
      "                                             3.498064517974854,\n",
      "                                             7.758599281311035,\n",
      "                                             2.493829965591431,\n",
      "                                             0.8311287760734558,\n",
      "                                             0.7213778495788574,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             1.138568878173828]},\n",
      "                                 {'max_token': 'jon',\n",
      "                                  'max_value': 'high',\n",
      "                                  'max_value_token_index': 8,\n",
      "                                  'sentence_string': 'ZN0ju âĢĶ Jon Schwartz '\n",
      "                                                     '(@jonlschwartz1) April '\n",
      "                                                     '26',\n",
      "                                  'tokens': ['Z',\n",
      "                                             'N',\n",
      "                                             '0',\n",
      "                                             'ju',\n",
      "                                             ' âĢĶ',\n",
      "                                             ' Jon',\n",
      "                                             ' Schwartz',\n",
      "                                             ' (@',\n",
      "                                             'jon',\n",
      "                                             'ls',\n",
      "                                             'ch',\n",
      "                                             'w',\n",
      "                                             'artz',\n",
      "                                             '1',\n",
      "                                             ')',\n",
      "                                             ' April',\n",
      "                                             ' 26'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             2.302272319793701,\n",
      "                                             4.523098945617676,\n",
      "                                             7.362570762634277,\n",
      "                                             5.113739967346191,\n",
      "                                             3.24357795715332,\n",
      "                                             3.244030237197876,\n",
      "                                             3.033562183380127,\n",
      "                                             0.1977303475141525,\n",
      "                                             0.4494135975837708,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_value': 'low',\n",
      "                                  'sentence_string': ' the disease remains '\n",
      "                                                     'largely unknown, '\n",
      "                                                     'genetic, viral, hormonal '\n",
      "                                                     'and environmental '\n",
      "                                                     'factors have been so',\n",
      "                                  'tokens': [' the',\n",
      "                                             ' disease',\n",
      "                                             ' remains',\n",
      "                                             ' largely',\n",
      "                                             ' unknown',\n",
      "                                             ',',\n",
      "                                             ' genetic',\n",
      "                                             ',',\n",
      "                                             ' viral',\n",
      "                                             ',',\n",
      "                                             ' hormonal',\n",
      "                                             ' and',\n",
      "                                             ' environmental',\n",
      "                                             ' factors',\n",
      "                                             ' have',\n",
      "                                             ' been',\n",
      "                                             ' so'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_value': 'low',\n",
      "                                  'sentence_string': ' of these are the ones '\n",
      "                                                     'associated with '\n",
      "                                                     'bacterial virulence ( 9 '\n",
      "                                                     'âĢĵ 11 ). The most',\n",
      "                                  'tokens': [' of',\n",
      "                                             ' these',\n",
      "                                             ' are',\n",
      "                                             ' the',\n",
      "                                             ' ones',\n",
      "                                             ' associated',\n",
      "                                             ' with',\n",
      "                                             ' bacterial',\n",
      "                                             ' vir',\n",
      "                                             'ulence',\n",
      "                                             ' (',\n",
      "                                             ' 9',\n",
      "                                             ' âĢĵ',\n",
      "                                             ' 11',\n",
      "                                             ' ).',\n",
      "                                             ' The',\n",
      "                                             ' most'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_value': 'low',\n",
      "                                  'sentence_string': ' height at some '\n",
      "                                                     'international '\n",
      "                                                     'summit.ĊĊOr is it '\n",
      "                                                     'because of these deeply '\n",
      "                                                     'controversial statements',\n",
      "                                  'tokens': [' height',\n",
      "                                             ' at',\n",
      "                                             ' some',\n",
      "                                             ' international',\n",
      "                                             ' summit',\n",
      "                                             '.',\n",
      "                                             'Ċ',\n",
      "                                             'Ċ',\n",
      "                                             'Or',\n",
      "                                             ' is',\n",
      "                                             ' it',\n",
      "                                             ' because',\n",
      "                                             ' of',\n",
      "                                             ' these',\n",
      "                                             ' deeply',\n",
      "                                             ' controversial',\n",
      "                                             ' statements'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]}]},\n",
      "             {'description': 'scientific or investigative terms and concepts',\n",
      "              'feature_index': 626,\n",
      "              'gpt_predictions': [(1, 0.0),\n",
      "                                  (1, 0.0),\n",
      "                                  (1, 1.0),\n",
      "                                  (0, 1.0),\n",
      "                                  (0, 1.0),\n",
      "                                  (0, 0.0)],\n",
      "              'highest_activation': 14.85885715484619,\n",
      "              'show_sentences': [],\n",
      "              'test_sentences': [{'max_token': ' through',\n",
      "                                  'max_value': 'high',\n",
      "                                  'max_value_token_index': 8,\n",
      "                                  'sentence_string': 'ester and Sunlight Labs '\n",
      "                                                     'Director, found through '\n",
      "                                                     'Churnalism that '\n",
      "                                                     'ReutersâĢĻ prematurely',\n",
      "                                  'tokens': ['ester',\n",
      "                                             ' and',\n",
      "                                             ' Sun',\n",
      "                                             'light',\n",
      "                                             ' Labs',\n",
      "                                             ' Director',\n",
      "                                             ',',\n",
      "                                             ' found',\n",
      "                                             ' through',\n",
      "                                             ' Ch',\n",
      "                                             'urnal',\n",
      "                                             'ism',\n",
      "                                             ' that',\n",
      "                                             ' Reuters',\n",
      "                                             'âĢ',\n",
      "                                             'Ļ',\n",
      "                                             ' prematurely'],\n",
      "                                  'values': [2.56632661819458,\n",
      "                                             0,\n",
      "                                             0.05439519882202148,\n",
      "                                             0,\n",
      "                                             2.402790784835815,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             14.85885715484619,\n",
      "                                             1.976359009742737,\n",
      "                                             0.8786564469337463,\n",
      "                                             3.969074726104736,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_token': ' closer',\n",
      "                                  'max_value': 'high',\n",
      "                                  'max_value_token_index': 8,\n",
      "                                  'sentence_string': ' few years '\n",
      "                                                     'ago.ĊĊHowever, closer '\n",
      "                                                     'examination revealed '\n",
      "                                                     'considerable differences '\n",
      "                                                     'in appearance, '\n",
      "                                                     'particularly',\n",
      "                                  'tokens': [' few',\n",
      "                                             ' years',\n",
      "                                             ' ago',\n",
      "                                             '.',\n",
      "                                             'Ċ',\n",
      "                                             'Ċ',\n",
      "                                             'However',\n",
      "                                             ',',\n",
      "                                             ' closer',\n",
      "                                             ' examination',\n",
      "                                             ' revealed',\n",
      "                                             ' considerable',\n",
      "                                             ' differences',\n",
      "                                             ' in',\n",
      "                                             ' appearance',\n",
      "                                             ',',\n",
      "                                             ' particularly'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             2.840531349182129,\n",
      "                                             2.25081992149353,\n",
      "                                             5.336575031280518,\n",
      "                                             14.68056774139404,\n",
      "                                             8.557292938232422,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_token': ' did',\n",
      "                                  'max_value': 'high',\n",
      "                                  'max_value_token_index': 8,\n",
      "                                  'sentence_string': ' race.ĊĊThe private '\n",
      "                                                     'investigator also did '\n",
      "                                                     'the maths and it '\n",
      "                                                     'didnâĢĻt',\n",
      "                                  'tokens': [' race',\n",
      "                                             '.',\n",
      "                                             'Ċ',\n",
      "                                             'Ċ',\n",
      "                                             'The',\n",
      "                                             ' private',\n",
      "                                             ' investigator',\n",
      "                                             ' also',\n",
      "                                             ' did',\n",
      "                                             ' the',\n",
      "                                             ' maths',\n",
      "                                             ' and',\n",
      "                                             ' it',\n",
      "                                             ' didn',\n",
      "                                             'âĢ',\n",
      "                                             'Ļ',\n",
      "                                             't'],\n",
      "                                  'values': [0,\n",
      "                                             0.1703451424837112,\n",
      "                                             0,\n",
      "                                             0.248449519276619,\n",
      "                                             1.45314610004425,\n",
      "                                             0,\n",
      "                                             4.916699886322021,\n",
      "                                             10.12405967712402,\n",
      "                                             14.22365570068359,\n",
      "                                             10.27671527862549,\n",
      "                                             0,\n",
      "                                             3.975500583648682,\n",
      "                                             0.02502468973398209,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_value': 'low',\n",
      "                                  'sentence_string': ' the disease remains '\n",
      "                                                     'largely unknown, '\n",
      "                                                     'genetic, viral, hormonal '\n",
      "                                                     'and environmental '\n",
      "                                                     'factors have been so',\n",
      "                                  'tokens': [' the',\n",
      "                                             ' disease',\n",
      "                                             ' remains',\n",
      "                                             ' largely',\n",
      "                                             ' unknown',\n",
      "                                             ',',\n",
      "                                             ' genetic',\n",
      "                                             ',',\n",
      "                                             ' viral',\n",
      "                                             ',',\n",
      "                                             ' hormonal',\n",
      "                                             ' and',\n",
      "                                             ' environmental',\n",
      "                                             ' factors',\n",
      "                                             ' have',\n",
      "                                             ' been',\n",
      "                                             ' so'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_value': 'low',\n",
      "                                  'sentence_string': ' of these are the ones '\n",
      "                                                     'associated with '\n",
      "                                                     'bacterial virulence ( 9 '\n",
      "                                                     'âĢĵ 11 ). The most',\n",
      "                                  'tokens': [' of',\n",
      "                                             ' these',\n",
      "                                             ' are',\n",
      "                                             ' the',\n",
      "                                             ' ones',\n",
      "                                             ' associated',\n",
      "                                             ' with',\n",
      "                                             ' bacterial',\n",
      "                                             ' vir',\n",
      "                                             'ulence',\n",
      "                                             ' (',\n",
      "                                             ' 9',\n",
      "                                             ' âĢĵ',\n",
      "                                             ' 11',\n",
      "                                             ' ).',\n",
      "                                             ' The',\n",
      "                                             ' most'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_value': 'low',\n",
      "                                  'sentence_string': ' height at some '\n",
      "                                                     'international '\n",
      "                                                     'summit.ĊĊOr is it '\n",
      "                                                     'because of these deeply '\n",
      "                                                     'controversial statements',\n",
      "                                  'tokens': [' height',\n",
      "                                             ' at',\n",
      "                                             ' some',\n",
      "                                             ' international',\n",
      "                                             ' summit',\n",
      "                                             '.',\n",
      "                                             'Ċ',\n",
      "                                             'Ċ',\n",
      "                                             'Or',\n",
      "                                             ' is',\n",
      "                                             ' it',\n",
      "                                             ' because',\n",
      "                                             ' of',\n",
      "                                             ' these',\n",
      "                                             ' deeply',\n",
      "                                             ' controversial',\n",
      "                                             ' statements'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]}]},\n",
      "             {'description': 'terms related to physical structures',\n",
      "              'feature_index': 513,\n",
      "              'gpt_predictions': [(1, 0.0),\n",
      "                                  (1, 0.0),\n",
      "                                  (1, 0.0),\n",
      "                                  (0, 0.0),\n",
      "                                  (0, 0.0),\n",
      "                                  (0, 0.0)],\n",
      "              'highest_activation': 42.0250129699707,\n",
      "              'show_sentences': [],\n",
      "              'test_sentences': [{'max_token': ' structure',\n",
      "                                  'max_value': 'high',\n",
      "                                  'max_value_token_index': 8,\n",
      "                                  'sentence_string': ' rule. One thing for '\n",
      "                                                     'sure is that '\n",
      "                                                     'structureâĢĻs an '\n",
      "                                                     'anxiety, about a',\n",
      "                                  'tokens': [' rule',\n",
      "                                             '.',\n",
      "                                             ' One',\n",
      "                                             ' thing',\n",
      "                                             ' for',\n",
      "                                             ' sure',\n",
      "                                             ' is',\n",
      "                                             ' that',\n",
      "                                             ' structure',\n",
      "                                             'âĢ',\n",
      "                                             'Ļ',\n",
      "                                             's',\n",
      "                                             ' an',\n",
      "                                             ' anxiety',\n",
      "                                             ',',\n",
      "                                             ' about',\n",
      "                                             ' a'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             42.0250129699707,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_token': ' structure',\n",
      "                                  'max_value': 'high',\n",
      "                                  'max_value_token_index': 8,\n",
      "                                  'sentence_string': '-ish fieldwork. This '\n",
      "                                                     'definition of structure '\n",
      "                                                     'âĢĶ indeed any '\n",
      "                                                     'definition of it âĢĶ '\n",
      "                                                     'begs',\n",
      "                                  'tokens': ['-',\n",
      "                                             'ish',\n",
      "                                             ' field',\n",
      "                                             'work',\n",
      "                                             '.',\n",
      "                                             ' This',\n",
      "                                             ' definition',\n",
      "                                             ' of',\n",
      "                                             ' structure',\n",
      "                                             ' âĢĶ',\n",
      "                                             ' indeed',\n",
      "                                             ' any',\n",
      "                                             ' definition',\n",
      "                                             ' of',\n",
      "                                             ' it',\n",
      "                                             ' âĢĶ',\n",
      "                                             ' begs'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             41.09658432006836,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_token': ' structure',\n",
      "                                  'max_value': 'high',\n",
      "                                  'max_value_token_index': 8,\n",
      "                                  'sentence_string': 'DoegeâĢĵPotter syndrome '\n",
      "                                                     'The structure of IGF-2, '\n",
      "                                                     'responsible for the',\n",
      "                                  'tokens': ['D',\n",
      "                                             'oe',\n",
      "                                             'ge',\n",
      "                                             'âĢĵ',\n",
      "                                             'Pot',\n",
      "                                             'ter',\n",
      "                                             ' syndrome',\n",
      "                                             ' The',\n",
      "                                             ' structure',\n",
      "                                             ' of',\n",
      "                                             ' IGF',\n",
      "                                             '-',\n",
      "                                             '2',\n",
      "                                             ',',\n",
      "                                             ' responsible',\n",
      "                                             ' for',\n",
      "                                             ' the'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             39.66020584106445,\n",
      "                                             2.385071754455566,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_value': 'low',\n",
      "                                  'sentence_string': ' the disease remains '\n",
      "                                                     'largely unknown, '\n",
      "                                                     'genetic, viral, hormonal '\n",
      "                                                     'and environmental '\n",
      "                                                     'factors have been so',\n",
      "                                  'tokens': [' the',\n",
      "                                             ' disease',\n",
      "                                             ' remains',\n",
      "                                             ' largely',\n",
      "                                             ' unknown',\n",
      "                                             ',',\n",
      "                                             ' genetic',\n",
      "                                             ',',\n",
      "                                             ' viral',\n",
      "                                             ',',\n",
      "                                             ' hormonal',\n",
      "                                             ' and',\n",
      "                                             ' environmental',\n",
      "                                             ' factors',\n",
      "                                             ' have',\n",
      "                                             ' been',\n",
      "                                             ' so'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_value': 'low',\n",
      "                                  'sentence_string': ' of these are the ones '\n",
      "                                                     'associated with '\n",
      "                                                     'bacterial virulence ( 9 '\n",
      "                                                     'âĢĵ 11 ). The most',\n",
      "                                  'tokens': [' of',\n",
      "                                             ' these',\n",
      "                                             ' are',\n",
      "                                             ' the',\n",
      "                                             ' ones',\n",
      "                                             ' associated',\n",
      "                                             ' with',\n",
      "                                             ' bacterial',\n",
      "                                             ' vir',\n",
      "                                             'ulence',\n",
      "                                             ' (',\n",
      "                                             ' 9',\n",
      "                                             ' âĢĵ',\n",
      "                                             ' 11',\n",
      "                                             ' ).',\n",
      "                                             ' The',\n",
      "                                             ' most'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_value': 'low',\n",
      "                                  'sentence_string': ' height at some '\n",
      "                                                     'international '\n",
      "                                                     'summit.ĊĊOr is it '\n",
      "                                                     'because of these deeply '\n",
      "                                                     'controversial statements',\n",
      "                                  'tokens': [' height',\n",
      "                                             ' at',\n",
      "                                             ' some',\n",
      "                                             ' international',\n",
      "                                             ' summit',\n",
      "                                             '.',\n",
      "                                             'Ċ',\n",
      "                                             'Ċ',\n",
      "                                             'Or',\n",
      "                                             ' is',\n",
      "                                             ' it',\n",
      "                                             ' because',\n",
      "                                             ' of',\n",
      "                                             ' these',\n",
      "                                             ' deeply',\n",
      "                                             ' controversial',\n",
      "                                             ' statements'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]}]},\n",
      "             {'description': 'disparities between genders or races in various '\n",
      "                             'aspects',\n",
      "              'feature_index': 859,\n",
      "              'gpt_predictions': [(1, 1.0),\n",
      "                                  (1, 1.0),\n",
      "                                  (1, 0.0),\n",
      "                                  (0, 0.0),\n",
      "                                  (0, 0.0),\n",
      "                                  (0, 0.0)],\n",
      "              'highest_activation': 32.53410720825195,\n",
      "              'show_sentences': [],\n",
      "              'test_sentences': [{'max_token': ' whites',\n",
      "                                  'max_value': 'high',\n",
      "                                  'max_value_token_index': 8,\n",
      "                                  'sentence_string': 'end to receive less '\n",
      "                                                     'aggressive medical care '\n",
      "                                                     'than whitesâĢĿ at its '\n",
      "                                                     'hospitals and clinics,',\n",
      "                                  'tokens': ['end',\n",
      "                                             ' to',\n",
      "                                             ' receive',\n",
      "                                             ' less',\n",
      "                                             ' aggressive',\n",
      "                                             ' medical',\n",
      "                                             ' care',\n",
      "                                             ' than',\n",
      "                                             ' whites',\n",
      "                                             'âĢ',\n",
      "                                             'Ŀ',\n",
      "                                             ' at',\n",
      "                                             ' its',\n",
      "                                             ' hospitals',\n",
      "                                             ' and',\n",
      "                                             ' clinics',\n",
      "                                             ','],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             32.53410720825195,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_token': ' men',\n",
      "                                  'max_value': 'high',\n",
      "                                  'max_value_token_index': 8,\n",
      "                                  'sentence_string': ' $1.16 less per hour '\n",
      "                                                     'than men in the same '\n",
      "                                                     'jobs, despite having '\n",
      "                                                     'more',\n",
      "                                  'tokens': [' $',\n",
      "                                             '1',\n",
      "                                             '.',\n",
      "                                             '16',\n",
      "                                             ' less',\n",
      "                                             ' per',\n",
      "                                             ' hour',\n",
      "                                             ' than',\n",
      "                                             ' men',\n",
      "                                             ' in',\n",
      "                                             ' the',\n",
      "                                             ' same',\n",
      "                                             ' jobs',\n",
      "                                             ',',\n",
      "                                             ' despite',\n",
      "                                             ' having',\n",
      "                                             ' more'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             31.87485885620117,\n",
      "                                             4.845519065856934,\n",
      "                                             3.445847034454346,\n",
      "                                             5.037674427032471,\n",
      "                                             4.492052555084229,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_token': ' participants',\n",
      "                                  'max_value': 'high',\n",
      "                                  'max_value_token_index': 8,\n",
      "                                  'sentence_string': ' participants performed '\n",
      "                                                     'worse than nonlonely '\n",
      "                                                     'participants on social '\n",
      "                                                     'sensitivity tasks framed '\n",
      "                                                     'as tests of',\n",
      "                                  'tokens': [' participants',\n",
      "                                             ' performed',\n",
      "                                             ' worse',\n",
      "                                             ' than',\n",
      "                                             ' non',\n",
      "                                             'l',\n",
      "                                             'one',\n",
      "                                             'ly',\n",
      "                                             ' participants',\n",
      "                                             ' on',\n",
      "                                             ' social',\n",
      "                                             ' sensitivity',\n",
      "                                             ' tasks',\n",
      "                                             ' framed',\n",
      "                                             ' as',\n",
      "                                             ' tests',\n",
      "                                             ' of'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             7.215490341186523,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             11.2095832824707,\n",
      "                                             31.55771446228027,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             2.587136268615723,\n",
      "                                             5.906308174133301,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_value': 'low',\n",
      "                                  'sentence_string': ' to other countries. '\n",
      "                                                     'Also, getting an H-1B '\n",
      "                                                     'visa is tougher as '\n",
      "                                                     'compared',\n",
      "                                  'tokens': [' to',\n",
      "                                             ' other',\n",
      "                                             ' countries',\n",
      "                                             '.',\n",
      "                                             ' Also',\n",
      "                                             ',',\n",
      "                                             ' getting',\n",
      "                                             ' an',\n",
      "                                             ' H',\n",
      "                                             '-',\n",
      "                                             '1',\n",
      "                                             'B',\n",
      "                                             ' visa',\n",
      "                                             ' is',\n",
      "                                             ' tougher',\n",
      "                                             ' as',\n",
      "                                             ' compared'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_value': 'low',\n",
      "                                  'sentence_string': 'The potential for '\n",
      "                                                     'industry is huge, a '\n",
      "                                                     'Mckinsey estimate claims '\n",
      "                                                     'that the energy',\n",
      "                                  'tokens': ['The',\n",
      "                                             ' potential',\n",
      "                                             ' for',\n",
      "                                             ' industry',\n",
      "                                             ' is',\n",
      "                                             ' huge',\n",
      "                                             ',',\n",
      "                                             ' a',\n",
      "                                             ' M',\n",
      "                                             'ck',\n",
      "                                             'in',\n",
      "                                             'sey',\n",
      "                                             ' estimate',\n",
      "                                             ' claims',\n",
      "                                             ' that',\n",
      "                                             ' the',\n",
      "                                             ' energy'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_value': 'low',\n",
      "                                  'sentence_string': 'ock find after analyzing '\n",
      "                                                     'almost 30 years of U.S. '\n",
      "                                                     'survey data. At the',\n",
      "                                  'tokens': ['ock',\n",
      "                                             ' find',\n",
      "                                             ' after',\n",
      "                                             ' analyzing',\n",
      "                                             ' almost',\n",
      "                                             ' 30',\n",
      "                                             ' years',\n",
      "                                             ' of',\n",
      "                                             ' U',\n",
      "                                             '.',\n",
      "                                             'S',\n",
      "                                             '.',\n",
      "                                             ' survey',\n",
      "                                             ' data',\n",
      "                                             '.',\n",
      "                                             ' At',\n",
      "                                             ' the'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]}]},\n",
      "             {'description': 'Proper nouns related to politics, names, and '\n",
      "                             'affiliations',\n",
      "              'feature_index': 136,\n",
      "              'gpt_predictions': [(1, 0.0),\n",
      "                                  (1, 1.0),\n",
      "                                  (1, 0.0),\n",
      "                                  (0, 0.0),\n",
      "                                  (0, 0.0),\n",
      "                                  (0, 0.0)],\n",
      "              'highest_activation': 39.01195907592773,\n",
      "              'show_sentences': [],\n",
      "              'test_sentences': [{'max_token': 'ase',\n",
      "                                  'max_value': 'high',\n",
      "                                  'max_value_token_index': 8,\n",
      "                                  'sentence_string': '\" and the \"A\" for '\n",
      "                                                     '\"asexual\" and/or \"allied',\n",
      "                                  'tokens': ['\"',\n",
      "                                             ' and',\n",
      "                                             ' the',\n",
      "                                             ' \"',\n",
      "                                             'A',\n",
      "                                             '\"',\n",
      "                                             ' for',\n",
      "                                             ' \"',\n",
      "                                             'ase',\n",
      "                                             'xual',\n",
      "                                             '\"',\n",
      "                                             ' and',\n",
      "                                             '/',\n",
      "                                             'or',\n",
      "                                             ' \"',\n",
      "                                             'all',\n",
      "                                             'ied'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             39.01195907592773,\n",
      "                                             3.470726251602173,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_token': 'ase',\n",
      "                                  'max_value': 'high',\n",
      "                                  'max_value_token_index': 8,\n",
      "                                  'sentence_string': 'yn FarrellĊĊState '\n",
      "                                                     'Senator Bob '\n",
      "                                                     'HasegawaĊĊFormer Mayor '\n",
      "                                                     'Mike McG',\n",
      "                                  'tokens': ['yn',\n",
      "                                             ' Farrell',\n",
      "                                             'Ċ',\n",
      "                                             'Ċ',\n",
      "                                             'State',\n",
      "                                             ' Senator',\n",
      "                                             ' Bob',\n",
      "                                             ' H',\n",
      "                                             'ase',\n",
      "                                             'g',\n",
      "                                             'awa',\n",
      "                                             'Ċ',\n",
      "                                             'Ċ',\n",
      "                                             'Former',\n",
      "                                             ' Mayor',\n",
      "                                             ' Mike',\n",
      "                                             ' McG'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             34.67829132080078,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_token': 'ase',\n",
      "                                  'max_value': 'high',\n",
      "                                  'max_value_token_index': 8,\n",
      "                                  'sentence_string': ' ice has sublimated to a '\n",
      "                                                     'gaseous form, resulting '\n",
      "                                                     'in areas where the',\n",
      "                                  'tokens': [' ice',\n",
      "                                             ' has',\n",
      "                                             ' sub',\n",
      "                                             'lim',\n",
      "                                             'ated',\n",
      "                                             ' to',\n",
      "                                             ' a',\n",
      "                                             ' g',\n",
      "                                             'ase',\n",
      "                                             'ous',\n",
      "                                             ' form',\n",
      "                                             ',',\n",
      "                                             ' resulting',\n",
      "                                             ' in',\n",
      "                                             ' areas',\n",
      "                                             ' where',\n",
      "                                             ' the'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             30.44026756286621,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_value': 'low',\n",
      "                                  'sentence_string': 'Ŀ LaHood told me. '\n",
      "                                                     'âĢľThatâĢĻs all. I don',\n",
      "                                  'tokens': ['Ŀ',\n",
      "                                             ' La',\n",
      "                                             'H',\n",
      "                                             'ood',\n",
      "                                             ' told',\n",
      "                                             ' me',\n",
      "                                             '.',\n",
      "                                             ' âĢ',\n",
      "                                             'ľ',\n",
      "                                             'That',\n",
      "                                             'âĢ',\n",
      "                                             'Ļ',\n",
      "                                             's',\n",
      "                                             ' all',\n",
      "                                             '.',\n",
      "                                             ' I',\n",
      "                                             ' don'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_value': 'low',\n",
      "                                  'sentence_string': ' hairâĢĶsitting around a '\n",
      "                                                     'rectangle of '\n",
      "                                                     'pushed-together tables '\n",
      "                                                     'in a nondescript',\n",
      "                                  'tokens': [' hair',\n",
      "                                             'âĢĶ',\n",
      "                                             's',\n",
      "                                             'itting',\n",
      "                                             ' around',\n",
      "                                             ' a',\n",
      "                                             ' rectangle',\n",
      "                                             ' of',\n",
      "                                             ' pushed',\n",
      "                                             '-',\n",
      "                                             'together',\n",
      "                                             ' tables',\n",
      "                                             ' in',\n",
      "                                             ' a',\n",
      "                                             ' nond',\n",
      "                                             'esc',\n",
      "                                             'ript'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_value': 'low',\n",
      "                                  'sentence_string': ' here.ĊĊFear of the Zika '\n",
      "                                                     'virus is spreading as '\n",
      "                                                     'images of afflicted '\n",
      "                                                     'infants fill',\n",
      "                                  'tokens': [' here',\n",
      "                                             '.',\n",
      "                                             'Ċ',\n",
      "                                             'Ċ',\n",
      "                                             'Fear',\n",
      "                                             ' of',\n",
      "                                             ' the',\n",
      "                                             ' Zika',\n",
      "                                             ' virus',\n",
      "                                             ' is',\n",
      "                                             ' spreading',\n",
      "                                             ' as',\n",
      "                                             ' images',\n",
      "                                             ' of',\n",
      "                                             ' afflicted',\n",
      "                                             ' infants',\n",
      "                                             ' fill'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]}]},\n",
      "             {'description': 'references to actions that users can perform or '\n",
      "                             'services they can access',\n",
      "              'feature_index': 811,\n",
      "              'gpt_predictions': [(1, 1.0),\n",
      "                                  (1, 1.0),\n",
      "                                  (1, 1.0),\n",
      "                                  (0, 0.0),\n",
      "                                  (0, 0.0),\n",
      "                                  (0, 0.0)],\n",
      "              'highest_activation': 26.56685829162598,\n",
      "              'show_sentences': [],\n",
      "              'test_sentences': [{'max_token': ' can',\n",
      "                                  'max_value': 'high',\n",
      "                                  'max_value_token_index': 8,\n",
      "                                  'sentence_string': ' of '\n",
      "                                                     'collaboration.ĊĊResearchers '\n",
      "                                                     'and students can attach '\n",
      "                                                     'questions, opinions, '\n",
      "                                                     'formulas, and',\n",
      "                                  'tokens': [' of',\n",
      "                                             ' collaboration',\n",
      "                                             '.',\n",
      "                                             'Ċ',\n",
      "                                             'Ċ',\n",
      "                                             'Researchers',\n",
      "                                             ' and',\n",
      "                                             ' students',\n",
      "                                             ' can',\n",
      "                                             ' attach',\n",
      "                                             ' questions',\n",
      "                                             ',',\n",
      "                                             ' opinions',\n",
      "                                             ',',\n",
      "                                             ' formulas',\n",
      "                                             ',',\n",
      "                                             ' and'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             26.56685829162598,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_token': ' can',\n",
      "                                  'max_value': 'high',\n",
      "                                  'max_value_token_index': 8,\n",
      "                                  'sentence_string': ' modern safety solutions '\n",
      "                                                     'into our products. Users '\n",
      "                                                     'can dial emergency '\n",
      "                                                     'services and access '\n",
      "                                                     'Medical ID card',\n",
      "                                  'tokens': [' modern',\n",
      "                                             ' safety',\n",
      "                                             ' solutions',\n",
      "                                             ' into',\n",
      "                                             ' our',\n",
      "                                             ' products',\n",
      "                                             '.',\n",
      "                                             ' Users',\n",
      "                                             ' can',\n",
      "                                             ' dial',\n",
      "                                             ' emergency',\n",
      "                                             ' services',\n",
      "                                             ' and',\n",
      "                                             ' access',\n",
      "                                             ' Medical',\n",
      "                                             ' ID',\n",
      "                                             ' card'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             26.30245780944824,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_token': ' can',\n",
      "                                  'max_value': 'high',\n",
      "                                  'max_value_token_index': 8,\n",
      "                                  'sentence_string': 'âĢ<|endoftext|> '\n",
      "                                                     'ticket-holders and '\n",
      "                                                     'corporate partners can '\n",
      "                                                     'also buy apparel '\n",
      "                                                     'featuring the new '\n",
      "                                                     'alternate logo',\n",
      "                                  'tokens': ['âĢ',\n",
      "                                             '<|endoftext|>',\n",
      "                                             ' ticket',\n",
      "                                             '-',\n",
      "                                             'holders',\n",
      "                                             ' and',\n",
      "                                             ' corporate',\n",
      "                                             ' partners',\n",
      "                                             ' can',\n",
      "                                             ' also',\n",
      "                                             ' buy',\n",
      "                                             ' apparel',\n",
      "                                             ' featuring',\n",
      "                                             ' the',\n",
      "                                             ' new',\n",
      "                                             ' alternate',\n",
      "                                             ' logo'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             25.79669189453125,\n",
      "                                             15.80652904510498,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_value': 'low',\n",
      "                                  'sentence_string': ' to other countries. '\n",
      "                                                     'Also, getting an H-1B '\n",
      "                                                     'visa is tougher as '\n",
      "                                                     'compared',\n",
      "                                  'tokens': [' to',\n",
      "                                             ' other',\n",
      "                                             ' countries',\n",
      "                                             '.',\n",
      "                                             ' Also',\n",
      "                                             ',',\n",
      "                                             ' getting',\n",
      "                                             ' an',\n",
      "                                             ' H',\n",
      "                                             '-',\n",
      "                                             '1',\n",
      "                                             'B',\n",
      "                                             ' visa',\n",
      "                                             ' is',\n",
      "                                             ' tougher',\n",
      "                                             ' as',\n",
      "                                             ' compared'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_value': 'low',\n",
      "                                  'sentence_string': 'The potential for '\n",
      "                                                     'industry is huge, a '\n",
      "                                                     'Mckinsey estimate claims '\n",
      "                                                     'that the energy',\n",
      "                                  'tokens': ['The',\n",
      "                                             ' potential',\n",
      "                                             ' for',\n",
      "                                             ' industry',\n",
      "                                             ' is',\n",
      "                                             ' huge',\n",
      "                                             ',',\n",
      "                                             ' a',\n",
      "                                             ' M',\n",
      "                                             'ck',\n",
      "                                             'in',\n",
      "                                             'sey',\n",
      "                                             ' estimate',\n",
      "                                             ' claims',\n",
      "                                             ' that',\n",
      "                                             ' the',\n",
      "                                             ' energy'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_value': 'low',\n",
      "                                  'sentence_string': 'ock find after analyzing '\n",
      "                                                     'almost 30 years of U.S. '\n",
      "                                                     'survey data. At the',\n",
      "                                  'tokens': ['ock',\n",
      "                                             ' find',\n",
      "                                             ' after',\n",
      "                                             ' analyzing',\n",
      "                                             ' almost',\n",
      "                                             ' 30',\n",
      "                                             ' years',\n",
      "                                             ' of',\n",
      "                                             ' U',\n",
      "                                             '.',\n",
      "                                             'S',\n",
      "                                             '.',\n",
      "                                             ' survey',\n",
      "                                             ' data',\n",
      "                                             '.',\n",
      "                                             ' At',\n",
      "                                             ' the'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]}]},\n",
      "             {'description': 'questions posed in a somewhat dramatic or urgent '\n",
      "                             'manner',\n",
      "              'feature_index': 76,\n",
      "              'gpt_predictions': [(1, 1.0),\n",
      "                                  (1, 1.0),\n",
      "                                  (1, 1.0),\n",
      "                                  (0, 0.0),\n",
      "                                  (0, 0.0),\n",
      "                                  (0, 0.0)],\n",
      "              'highest_activation': 51.857421875,\n",
      "              'show_sentences': [],\n",
      "              'test_sentences': [{'max_token': ',',\n",
      "                                  'max_value': 'high',\n",
      "                                  'max_value_token_index': 8,\n",
      "                                  'sentence_string': ' away from the '\n",
      "                                                     'public.ĊĊWhy, yes, of '\n",
      "                                                     'course, something this '\n",
      "                                                     'large',\n",
      "                                  'tokens': [' away',\n",
      "                                             ' from',\n",
      "                                             ' the',\n",
      "                                             ' public',\n",
      "                                             '.',\n",
      "                                             'Ċ',\n",
      "                                             'Ċ',\n",
      "                                             'Why',\n",
      "                                             ',',\n",
      "                                             ' yes',\n",
      "                                             ',',\n",
      "                                             ' of',\n",
      "                                             ' course',\n",
      "                                             ',',\n",
      "                                             ' something',\n",
      "                                             ' this',\n",
      "                                             ' large'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             51.857421875,\n",
      "                                             0,\n",
      "                                             3.874945640563965,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             5.287142753601074,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_token': ',',\n",
      "                                  'max_value': 'high',\n",
      "                                  'max_value_token_index': 8,\n",
      "                                  'sentence_string': ' using everything at '\n",
      "                                                     'your disposal. So why, '\n",
      "                                                     'then, would his Justice '\n",
      "                                                     'Department award a',\n",
      "                                  'tokens': [' using',\n",
      "                                             ' everything',\n",
      "                                             ' at',\n",
      "                                             ' your',\n",
      "                                             ' disposal',\n",
      "                                             '.',\n",
      "                                             ' So',\n",
      "                                             ' why',\n",
      "                                             ',',\n",
      "                                             ' then',\n",
      "                                             ',',\n",
      "                                             ' would',\n",
      "                                             ' his',\n",
      "                                             ' Justice',\n",
      "                                             ' Department',\n",
      "                                             ' award',\n",
      "                                             ' a'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             49.45631408691406,\n",
      "                                             3.030514240264893,\n",
      "                                             9.393561363220215,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_token': ',',\n",
      "                                  'max_value': 'high',\n",
      "                                  'max_value_token_index': 8,\n",
      "                                  'sentence_string': '\" of her behavior.ĊĊHow, '\n",
      "                                                     'after an abusive '\n",
      "                                                     'outburst, he would ask',\n",
      "                                  'tokens': ['\"',\n",
      "                                             ' of',\n",
      "                                             ' her',\n",
      "                                             ' behavior',\n",
      "                                             '.',\n",
      "                                             'Ċ',\n",
      "                                             'Ċ',\n",
      "                                             'How',\n",
      "                                             ',',\n",
      "                                             ' after',\n",
      "                                             ' an',\n",
      "                                             ' abusive',\n",
      "                                             ' outburst',\n",
      "                                             ',',\n",
      "                                             ' he',\n",
      "                                             ' would',\n",
      "                                             ' ask'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             47.95778274536133,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_value': 'low',\n",
      "                                  'sentence_string': 'Ŀ LaHood told me. '\n",
      "                                                     'âĢľThatâĢĻs all. I don',\n",
      "                                  'tokens': ['Ŀ',\n",
      "                                             ' La',\n",
      "                                             'H',\n",
      "                                             'ood',\n",
      "                                             ' told',\n",
      "                                             ' me',\n",
      "                                             '.',\n",
      "                                             ' âĢ',\n",
      "                                             'ľ',\n",
      "                                             'That',\n",
      "                                             'âĢ',\n",
      "                                             'Ļ',\n",
      "                                             's',\n",
      "                                             ' all',\n",
      "                                             '.',\n",
      "                                             ' I',\n",
      "                                             ' don'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_value': 'low',\n",
      "                                  'sentence_string': ' hairâĢĶsitting around a '\n",
      "                                                     'rectangle of '\n",
      "                                                     'pushed-together tables '\n",
      "                                                     'in a nondescript',\n",
      "                                  'tokens': [' hair',\n",
      "                                             'âĢĶ',\n",
      "                                             's',\n",
      "                                             'itting',\n",
      "                                             ' around',\n",
      "                                             ' a',\n",
      "                                             ' rectangle',\n",
      "                                             ' of',\n",
      "                                             ' pushed',\n",
      "                                             '-',\n",
      "                                             'together',\n",
      "                                             ' tables',\n",
      "                                             ' in',\n",
      "                                             ' a',\n",
      "                                             ' nond',\n",
      "                                             'esc',\n",
      "                                             'ript'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_value': 'low',\n",
      "                                  'sentence_string': ' here.ĊĊFear of the Zika '\n",
      "                                                     'virus is spreading as '\n",
      "                                                     'images of afflicted '\n",
      "                                                     'infants fill',\n",
      "                                  'tokens': [' here',\n",
      "                                             '.',\n",
      "                                             'Ċ',\n",
      "                                             'Ċ',\n",
      "                                             'Fear',\n",
      "                                             ' of',\n",
      "                                             ' the',\n",
      "                                             ' Zika',\n",
      "                                             ' virus',\n",
      "                                             ' is',\n",
      "                                             ' spreading',\n",
      "                                             ' as',\n",
      "                                             ' images',\n",
      "                                             ' of',\n",
      "                                             ' afflicted',\n",
      "                                             ' infants',\n",
      "                                             ' fill'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]}]},\n",
      "             {'description': 'references to various conspiracy theories',\n",
      "              'feature_index': 636,\n",
      "              'gpt_predictions': [(1, 1.0),\n",
      "                                  (1, 1.0),\n",
      "                                  (1, 1.0),\n",
      "                                  (0, 0.0),\n",
      "                                  (0, 0.0),\n",
      "                                  (0, 0.0)],\n",
      "              'highest_activation': 69.28605651855469,\n",
      "              'show_sentences': [],\n",
      "              'test_sentences': [{'max_token': ' conspiracy',\n",
      "                                  'max_value': 'high',\n",
      "                                  'max_value_token_index': 8,\n",
      "                                  'sentence_string': ' Trump had continued to '\n",
      "                                                     'publicly and privately '\n",
      "                                                     'entertain conspiracy '\n",
      "                                                     'theories, including the '\n",
      "                                                     'long-debunk',\n",
      "                                  'tokens': [' Trump',\n",
      "                                             ' had',\n",
      "                                             ' continued',\n",
      "                                             ' to',\n",
      "                                             ' publicly',\n",
      "                                             ' and',\n",
      "                                             ' privately',\n",
      "                                             ' entertain',\n",
      "                                             ' conspiracy',\n",
      "                                             ' theories',\n",
      "                                             ',',\n",
      "                                             ' including',\n",
      "                                             ' the',\n",
      "                                             ' long',\n",
      "                                             '-',\n",
      "                                             'deb',\n",
      "                                             'unk'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             69.28605651855469,\n",
      "                                             14.93484401702881,\n",
      "                                             0,\n",
      "                                             0.5840123891830444,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_token': ' conspiracy',\n",
      "                                  'max_value': 'high',\n",
      "                                  'max_value_token_index': 7,\n",
      "                                  'sentence_string': ' had continued to '\n",
      "                                                     'publicly and privately '\n",
      "                                                     'entertain conspiracy '\n",
      "                                                     'theories, including the '\n",
      "                                                     'long-debunked',\n",
      "                                  'tokens': [' had',\n",
      "                                             ' continued',\n",
      "                                             ' to',\n",
      "                                             ' publicly',\n",
      "                                             ' and',\n",
      "                                             ' privately',\n",
      "                                             ' entertain',\n",
      "                                             ' conspiracy',\n",
      "                                             ' theories',\n",
      "                                             ',',\n",
      "                                             ' including',\n",
      "                                             ' the',\n",
      "                                             ' long',\n",
      "                                             '-',\n",
      "                                             'deb',\n",
      "                                             'unk',\n",
      "                                             'ed'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             69.28605651855469,\n",
      "                                             14.93484401702881,\n",
      "                                             0,\n",
      "                                             0.5840123891830444,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_token': ' conspiracy',\n",
      "                                  'max_value': 'high',\n",
      "                                  'max_value_token_index': 7,\n",
      "                                  'sentence_string': ' been circulating \"truly '\n",
      "                                                     'absurd\" conspiracy '\n",
      "                                                     'theories.ĊĊ\"It will not '\n",
      "                                                     'distract',\n",
      "                                  'tokens': [' been',\n",
      "                                             ' circulating',\n",
      "                                             ' \"',\n",
      "                                             't',\n",
      "                                             'ruly',\n",
      "                                             ' absurd',\n",
      "                                             '\"',\n",
      "                                             ' conspiracy',\n",
      "                                             ' theories',\n",
      "                                             '.',\n",
      "                                             'Ċ',\n",
      "                                             'Ċ',\n",
      "                                             '\"',\n",
      "                                             'It',\n",
      "                                             ' will',\n",
      "                                             ' not',\n",
      "                                             ' distract'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             66.89694213867188,\n",
      "                                             12.76658630371094,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_value': 'low',\n",
      "                                  'sentence_string': ' the disease remains '\n",
      "                                                     'largely unknown, '\n",
      "                                                     'genetic, viral, hormonal '\n",
      "                                                     'and environmental '\n",
      "                                                     'factors have been so',\n",
      "                                  'tokens': [' the',\n",
      "                                             ' disease',\n",
      "                                             ' remains',\n",
      "                                             ' largely',\n",
      "                                             ' unknown',\n",
      "                                             ',',\n",
      "                                             ' genetic',\n",
      "                                             ',',\n",
      "                                             ' viral',\n",
      "                                             ',',\n",
      "                                             ' hormonal',\n",
      "                                             ' and',\n",
      "                                             ' environmental',\n",
      "                                             ' factors',\n",
      "                                             ' have',\n",
      "                                             ' been',\n",
      "                                             ' so'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_value': 'low',\n",
      "                                  'sentence_string': ' of these are the ones '\n",
      "                                                     'associated with '\n",
      "                                                     'bacterial virulence ( 9 '\n",
      "                                                     'âĢĵ 11 ). The most',\n",
      "                                  'tokens': [' of',\n",
      "                                             ' these',\n",
      "                                             ' are',\n",
      "                                             ' the',\n",
      "                                             ' ones',\n",
      "                                             ' associated',\n",
      "                                             ' with',\n",
      "                                             ' bacterial',\n",
      "                                             ' vir',\n",
      "                                             'ulence',\n",
      "                                             ' (',\n",
      "                                             ' 9',\n",
      "                                             ' âĢĵ',\n",
      "                                             ' 11',\n",
      "                                             ' ).',\n",
      "                                             ' The',\n",
      "                                             ' most'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_value': 'low',\n",
      "                                  'sentence_string': ' height at some '\n",
      "                                                     'international '\n",
      "                                                     'summit.ĊĊOr is it '\n",
      "                                                     'because of these deeply '\n",
      "                                                     'controversial statements',\n",
      "                                  'tokens': [' height',\n",
      "                                             ' at',\n",
      "                                             ' some',\n",
      "                                             ' international',\n",
      "                                             ' summit',\n",
      "                                             '.',\n",
      "                                             'Ċ',\n",
      "                                             'Ċ',\n",
      "                                             'Or',\n",
      "                                             ' is',\n",
      "                                             ' it',\n",
      "                                             ' because',\n",
      "                                             ' of',\n",
      "                                             ' these',\n",
      "                                             ' deeply',\n",
      "                                             ' controversial',\n",
      "                                             ' statements'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]}]},\n",
      "             {'description': 'references to a large quantity or group of '\n",
      "                             'people',\n",
      "              'feature_index': 973,\n",
      "              'gpt_predictions': [(1, 1.0),\n",
      "                                  (1, 1.0),\n",
      "                                  (1, 1.0),\n",
      "                                  (0, 0.0),\n",
      "                                  (0, 0.0),\n",
      "                                  (0, 0.0)],\n",
      "              'highest_activation': 27.30073165893555,\n",
      "              'show_sentences': [],\n",
      "              'test_sentences': [{'max_token': ' of',\n",
      "                                  'max_value': 'high',\n",
      "                                  'max_value_token_index': 8,\n",
      "                                  'sentence_string': ' really exciting. I know '\n",
      "                                                     'that a lot of the fans '\n",
      "                                                     'were probably seeing us '\n",
      "                                                     'for the',\n",
      "                                  'tokens': [' really',\n",
      "                                             ' exciting',\n",
      "                                             '.',\n",
      "                                             ' I',\n",
      "                                             ' know',\n",
      "                                             ' that',\n",
      "                                             ' a',\n",
      "                                             ' lot',\n",
      "                                             ' of',\n",
      "                                             ' the',\n",
      "                                             ' fans',\n",
      "                                             ' were',\n",
      "                                             ' probably',\n",
      "                                             ' seeing',\n",
      "                                             ' us',\n",
      "                                             ' for',\n",
      "                                             ' the'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             3.329301118850708,\n",
      "                                             16.25069236755371,\n",
      "                                             27.30073165893555,\n",
      "                                             12.81426048278809,\n",
      "                                             0.3569294810295105,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_token': ' of',\n",
      "                                  'max_value': 'high',\n",
      "                                  'max_value_token_index': 8,\n",
      "                                  'sentence_string': ' explained. âĢľI think a '\n",
      "                                                     'lot of Americans would '\n",
      "                                                     'appreciate that.âĢĿĊ',\n",
      "                                  'tokens': [' explained',\n",
      "                                             '.',\n",
      "                                             ' âĢ',\n",
      "                                             'ľ',\n",
      "                                             'I',\n",
      "                                             ' think',\n",
      "                                             ' a',\n",
      "                                             ' lot',\n",
      "                                             ' of',\n",
      "                                             ' Americans',\n",
      "                                             ' would',\n",
      "                                             ' appreciate',\n",
      "                                             ' that',\n",
      "                                             '.',\n",
      "                                             'âĢ',\n",
      "                                             'Ŀ',\n",
      "                                             'Ċ'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0.9451820254325867,\n",
      "                                             12.24942970275879,\n",
      "                                             25.35811805725098,\n",
      "                                             2.913897752761841,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_token': ' of',\n",
      "                                  'max_value': 'high',\n",
      "                                  'max_value_token_index': 8,\n",
      "                                  'sentence_string': ' penalised for doing '\n",
      "                                                     'what I believe lots of '\n",
      "                                                     'newspapers and lots of '\n",
      "                                                     'TV channels should have',\n",
      "                                  'tokens': [' penal',\n",
      "                                             'ised',\n",
      "                                             ' for',\n",
      "                                             ' doing',\n",
      "                                             ' what',\n",
      "                                             ' I',\n",
      "                                             ' believe',\n",
      "                                             ' lots',\n",
      "                                             ' of',\n",
      "                                             ' newspapers',\n",
      "                                             ' and',\n",
      "                                             ' lots',\n",
      "                                             ' of',\n",
      "                                             ' TV',\n",
      "                                             ' channels',\n",
      "                                             ' should',\n",
      "                                             ' have'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             14.89928150177002,\n",
      "                                             24.93867683410645,\n",
      "                                             0.005065396428108215,\n",
      "                                             0,\n",
      "                                             7.578578472137451,\n",
      "                                             11.85309791564941,\n",
      "                                             0,\n",
      "                                             0.4800727963447571,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_value': 'low',\n",
      "                                  'sentence_string': ' to other countries. '\n",
      "                                                     'Also, getting an H-1B '\n",
      "                                                     'visa is tougher as '\n",
      "                                                     'compared',\n",
      "                                  'tokens': [' to',\n",
      "                                             ' other',\n",
      "                                             ' countries',\n",
      "                                             '.',\n",
      "                                             ' Also',\n",
      "                                             ',',\n",
      "                                             ' getting',\n",
      "                                             ' an',\n",
      "                                             ' H',\n",
      "                                             '-',\n",
      "                                             '1',\n",
      "                                             'B',\n",
      "                                             ' visa',\n",
      "                                             ' is',\n",
      "                                             ' tougher',\n",
      "                                             ' as',\n",
      "                                             ' compared'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_value': 'low',\n",
      "                                  'sentence_string': 'The potential for '\n",
      "                                                     'industry is huge, a '\n",
      "                                                     'Mckinsey estimate claims '\n",
      "                                                     'that the energy',\n",
      "                                  'tokens': ['The',\n",
      "                                             ' potential',\n",
      "                                             ' for',\n",
      "                                             ' industry',\n",
      "                                             ' is',\n",
      "                                             ' huge',\n",
      "                                             ',',\n",
      "                                             ' a',\n",
      "                                             ' M',\n",
      "                                             'ck',\n",
      "                                             'in',\n",
      "                                             'sey',\n",
      "                                             ' estimate',\n",
      "                                             ' claims',\n",
      "                                             ' that',\n",
      "                                             ' the',\n",
      "                                             ' energy'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_value': 'low',\n",
      "                                  'sentence_string': 'ock find after analyzing '\n",
      "                                                     'almost 30 years of U.S. '\n",
      "                                                     'survey data. At the',\n",
      "                                  'tokens': ['ock',\n",
      "                                             ' find',\n",
      "                                             ' after',\n",
      "                                             ' analyzing',\n",
      "                                             ' almost',\n",
      "                                             ' 30',\n",
      "                                             ' years',\n",
      "                                             ' of',\n",
      "                                             ' U',\n",
      "                                             '.',\n",
      "                                             'S',\n",
      "                                             '.',\n",
      "                                             ' survey',\n",
      "                                             ' data',\n",
      "                                             '.',\n",
      "                                             ' At',\n",
      "                                             ' the'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]}]},\n",
      "             {'description': 'periods at the end of sentences',\n",
      "              'feature_index': 938,\n",
      "              'gpt_predictions': [(1, 1.0),\n",
      "                                  (1, 0.0),\n",
      "                                  (1, 0.0),\n",
      "                                  (0, 0.0),\n",
      "                                  (0, 0.0),\n",
      "                                  (0, 0.0)],\n",
      "              'highest_activation': 63.76441955566406,\n",
      "              'show_sentences': [],\n",
      "              'test_sentences': [{'max_token': '.',\n",
      "                                  'max_value': 'high',\n",
      "                                  'max_value_token_index': 8,\n",
      "                                  'sentence_string': ' the<|endoftext|> '\n",
      "                                                     \"story's conclusion.ĊĊ. \"\n",
      "                                                     'This performance '\n",
      "                                                     'investigates an old '\n",
      "                                                     'secret that has',\n",
      "                                  'tokens': [' the',\n",
      "                                             '<|endoftext|>',\n",
      "                                             ' story',\n",
      "                                             \"'s\",\n",
      "                                             ' conclusion',\n",
      "                                             '.',\n",
      "                                             'Ċ',\n",
      "                                             'Ċ',\n",
      "                                             '.',\n",
      "                                             ' This',\n",
      "                                             ' performance',\n",
      "                                             ' investigates',\n",
      "                                             ' an',\n",
      "                                             ' old',\n",
      "                                             ' secret',\n",
      "                                             ' that',\n",
      "                                             ' has'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             63.76441955566406,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_token': '.',\n",
      "                                  'max_value': 'high',\n",
      "                                  'max_value_token_index': 8,\n",
      "                                  'sentence_string': ', he has<|endoftext|> '\n",
      "                                                     'statement.ĊĊ.@farenet '\n",
      "                                                     '@WomeninFootball @',\n",
      "                                  'tokens': [',',\n",
      "                                             ' he',\n",
      "                                             ' has',\n",
      "                                             '<|endoftext|>',\n",
      "                                             ' statement',\n",
      "                                             '.',\n",
      "                                             'Ċ',\n",
      "                                             'Ċ',\n",
      "                                             '.',\n",
      "                                             '@',\n",
      "                                             'fare',\n",
      "                                             'net',\n",
      "                                             ' @',\n",
      "                                             'Women',\n",
      "                                             'in',\n",
      "                                             'Football',\n",
      "                                             ' @'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             60.09085845947266,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_token': '.',\n",
      "                                  'max_value': 'high',\n",
      "                                  'max_value_token_index': 8,\n",
      "                                  'sentence_string': 'man) January 17, '\n",
      "                                                     '2017ĊĊ.@realDonaldTrump '\n",
      "                                                     'Every morning an angry '\n",
      "                                                     'billionaire cries',\n",
      "                                  'tokens': ['man',\n",
      "                                             ')',\n",
      "                                             ' January',\n",
      "                                             ' 17',\n",
      "                                             ',',\n",
      "                                             ' 2017',\n",
      "                                             'Ċ',\n",
      "                                             'Ċ',\n",
      "                                             '.',\n",
      "                                             '@',\n",
      "                                             'realDonaldTrump',\n",
      "                                             ' Every',\n",
      "                                             ' morning',\n",
      "                                             ' an',\n",
      "                                             ' angry',\n",
      "                                             ' billionaire',\n",
      "                                             ' cries'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             55.85807037353516,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_value': 'low',\n",
      "                                  'sentence_string': ' to other countries. '\n",
      "                                                     'Also, getting an H-1B '\n",
      "                                                     'visa is tougher as '\n",
      "                                                     'compared',\n",
      "                                  'tokens': [' to',\n",
      "                                             ' other',\n",
      "                                             ' countries',\n",
      "                                             '.',\n",
      "                                             ' Also',\n",
      "                                             ',',\n",
      "                                             ' getting',\n",
      "                                             ' an',\n",
      "                                             ' H',\n",
      "                                             '-',\n",
      "                                             '1',\n",
      "                                             'B',\n",
      "                                             ' visa',\n",
      "                                             ' is',\n",
      "                                             ' tougher',\n",
      "                                             ' as',\n",
      "                                             ' compared'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_value': 'low',\n",
      "                                  'sentence_string': 'The potential for '\n",
      "                                                     'industry is huge, a '\n",
      "                                                     'Mckinsey estimate claims '\n",
      "                                                     'that the energy',\n",
      "                                  'tokens': ['The',\n",
      "                                             ' potential',\n",
      "                                             ' for',\n",
      "                                             ' industry',\n",
      "                                             ' is',\n",
      "                                             ' huge',\n",
      "                                             ',',\n",
      "                                             ' a',\n",
      "                                             ' M',\n",
      "                                             'ck',\n",
      "                                             'in',\n",
      "                                             'sey',\n",
      "                                             ' estimate',\n",
      "                                             ' claims',\n",
      "                                             ' that',\n",
      "                                             ' the',\n",
      "                                             ' energy'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_value': 'low',\n",
      "                                  'sentence_string': 'ock find after analyzing '\n",
      "                                                     'almost 30 years of U.S. '\n",
      "                                                     'survey data. At the',\n",
      "                                  'tokens': ['ock',\n",
      "                                             ' find',\n",
      "                                             ' after',\n",
      "                                             ' analyzing',\n",
      "                                             ' almost',\n",
      "                                             ' 30',\n",
      "                                             ' years',\n",
      "                                             ' of',\n",
      "                                             ' U',\n",
      "                                             '.',\n",
      "                                             'S',\n",
      "                                             '.',\n",
      "                                             ' survey',\n",
      "                                             ' data',\n",
      "                                             '.',\n",
      "                                             ' At',\n",
      "                                             ' the'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]}]},\n",
      "             {'description': 'language related to appreciation and support, '\n",
      "                             'especially in the context of online '\n",
      "                             'participation and activism',\n",
      "              'feature_index': 899,\n",
      "              'gpt_predictions': [(1, 0.0),\n",
      "                                  (1, 1.0),\n",
      "                                  (1, 0.0),\n",
      "                                  (0, 0.0),\n",
      "                                  (0, 0.0),\n",
      "                                  (0, 0.0)],\n",
      "              'highest_activation': 10.72458267211914,\n",
      "              'show_sentences': [],\n",
      "              'test_sentences': [{'max_token': ' We',\n",
      "                                  'max_value': 'high',\n",
      "                                  'max_value_token_index': 9,\n",
      "                                  'sentence_string': 'Ļve got incredible '\n",
      "                                                     'participation across the '\n",
      "                                                     'web. WeâĢĻve had hits '\n",
      "                                                     'from Ottawa',\n",
      "                                  'tokens': ['Ļ',\n",
      "                                             've',\n",
      "                                             ' got',\n",
      "                                             ' incredible',\n",
      "                                             ' participation',\n",
      "                                             ' across',\n",
      "                                             ' the',\n",
      "                                             ' web',\n",
      "                                             '.',\n",
      "                                             ' We',\n",
      "                                             'âĢ',\n",
      "                                             'Ļ',\n",
      "                                             've',\n",
      "                                             ' had',\n",
      "                                             ' hits',\n",
      "                                             ' from',\n",
      "                                             ' Ottawa'],\n",
      "                                  'values': [0.7261441946029663,\n",
      "                                             4.631937026977539,\n",
      "                                             4.435512542724609,\n",
      "                                             6.913612842559814,\n",
      "                                             6.513927459716797,\n",
      "                                             1.017479419708252,\n",
      "                                             0,\n",
      "                                             1.85525107383728,\n",
      "                                             10.18198394775391,\n",
      "                                             10.72458267211914,\n",
      "                                             0,\n",
      "                                             0.4605589210987091,\n",
      "                                             8.579977989196777,\n",
      "                                             8.800178527832031,\n",
      "                                             3.969276428222656,\n",
      "                                             3.021960735321045,\n",
      "                                             0.2717242836952209]},\n",
      "                                 {'max_token': ' We',\n",
      "                                  'max_value': 'high',\n",
      "                                  'max_value_token_index': 14,\n",
      "                                  'sentence_string': 'ĊâĢľWeâĢĻve got '\n",
      "                                                     'incredible participation '\n",
      "                                                     'across the web. WeâĢĻ',\n",
      "                                  'tokens': ['Ċ',\n",
      "                                             'âĢ',\n",
      "                                             'ľ',\n",
      "                                             'We',\n",
      "                                             'âĢ',\n",
      "                                             'Ļ',\n",
      "                                             've',\n",
      "                                             ' got',\n",
      "                                             ' incredible',\n",
      "                                             ' participation',\n",
      "                                             ' across',\n",
      "                                             ' the',\n",
      "                                             ' web',\n",
      "                                             '.',\n",
      "                                             ' We',\n",
      "                                             'âĢ',\n",
      "                                             'Ļ'],\n",
      "                                  'values': [1.286702156066895,\n",
      "                                             0,\n",
      "                                             4.421596527099609,\n",
      "                                             5.988869667053223,\n",
      "                                             0,\n",
      "                                             0.7261441946029663,\n",
      "                                             4.631937026977539,\n",
      "                                             4.435512542724609,\n",
      "                                             6.913612842559814,\n",
      "                                             6.513927459716797,\n",
      "                                             1.017479419708252,\n",
      "                                             0,\n",
      "                                             1.85525107383728,\n",
      "                                             10.18198394775391,\n",
      "                                             10.72458267211914,\n",
      "                                             0,\n",
      "                                             0.4605589210987091]},\n",
      "                                 {'max_token': ' We',\n",
      "                                  'max_value': 'high',\n",
      "                                  'max_value_token_index': 8,\n",
      "                                  'sentence_string': 've got incredible '\n",
      "                                                     'participation across the '\n",
      "                                                     'web. WeâĢĻve had hits '\n",
      "                                                     'from Ottawa<|endoftext|>',\n",
      "                                  'tokens': ['ve',\n",
      "                                             ' got',\n",
      "                                             ' incredible',\n",
      "                                             ' participation',\n",
      "                                             ' across',\n",
      "                                             ' the',\n",
      "                                             ' web',\n",
      "                                             '.',\n",
      "                                             ' We',\n",
      "                                             'âĢ',\n",
      "                                             'Ļ',\n",
      "                                             've',\n",
      "                                             ' had',\n",
      "                                             ' hits',\n",
      "                                             ' from',\n",
      "                                             ' Ottawa',\n",
      "                                             '<|endoftext|>'],\n",
      "                                  'values': [4.631937026977539,\n",
      "                                             4.435512542724609,\n",
      "                                             6.913612842559814,\n",
      "                                             6.513927459716797,\n",
      "                                             1.017479419708252,\n",
      "                                             0,\n",
      "                                             1.85525107383728,\n",
      "                                             10.18198394775391,\n",
      "                                             10.72458267211914,\n",
      "                                             0,\n",
      "                                             0.4605589210987091,\n",
      "                                             8.579977989196777,\n",
      "                                             8.800178527832031,\n",
      "                                             3.969276428222656,\n",
      "                                             3.021960735321045,\n",
      "                                             0.2717242836952209,\n",
      "                                             0]},\n",
      "                                 {'max_value': 'low',\n",
      "                                  'sentence_string': ' to other countries. '\n",
      "                                                     'Also, getting an H-1B '\n",
      "                                                     'visa is tougher as '\n",
      "                                                     'compared',\n",
      "                                  'tokens': [' to',\n",
      "                                             ' other',\n",
      "                                             ' countries',\n",
      "                                             '.',\n",
      "                                             ' Also',\n",
      "                                             ',',\n",
      "                                             ' getting',\n",
      "                                             ' an',\n",
      "                                             ' H',\n",
      "                                             '-',\n",
      "                                             '1',\n",
      "                                             'B',\n",
      "                                             ' visa',\n",
      "                                             ' is',\n",
      "                                             ' tougher',\n",
      "                                             ' as',\n",
      "                                             ' compared'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_value': 'low',\n",
      "                                  'sentence_string': 'The potential for '\n",
      "                                                     'industry is huge, a '\n",
      "                                                     'Mckinsey estimate claims '\n",
      "                                                     'that the energy',\n",
      "                                  'tokens': ['The',\n",
      "                                             ' potential',\n",
      "                                             ' for',\n",
      "                                             ' industry',\n",
      "                                             ' is',\n",
      "                                             ' huge',\n",
      "                                             ',',\n",
      "                                             ' a',\n",
      "                                             ' M',\n",
      "                                             'ck',\n",
      "                                             'in',\n",
      "                                             'sey',\n",
      "                                             ' estimate',\n",
      "                                             ' claims',\n",
      "                                             ' that',\n",
      "                                             ' the',\n",
      "                                             ' energy'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_value': 'low',\n",
      "                                  'sentence_string': 'ock find after analyzing '\n",
      "                                                     'almost 30 years of U.S. '\n",
      "                                                     'survey data. At the',\n",
      "                                  'tokens': ['ock',\n",
      "                                             ' find',\n",
      "                                             ' after',\n",
      "                                             ' analyzing',\n",
      "                                             ' almost',\n",
      "                                             ' 30',\n",
      "                                             ' years',\n",
      "                                             ' of',\n",
      "                                             ' U',\n",
      "                                             '.',\n",
      "                                             'S',\n",
      "                                             '.',\n",
      "                                             ' survey',\n",
      "                                             ' data',\n",
      "                                             '.',\n",
      "                                             ' At',\n",
      "                                             ' the'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]}]},\n",
      "             {'description': 'various URLs and related alphanumeric patterns',\n",
      "              'feature_index': 280,\n",
      "              'gpt_predictions': [(1, 1.0),\n",
      "                                  (1, 1.0),\n",
      "                                  (1, 1.0),\n",
      "                                  (0, 0.0),\n",
      "                                  (0, 0.0),\n",
      "                                  (0, 0.0)],\n",
      "              'highest_activation': 26.07519912719727,\n",
      "              'show_sentences': [],\n",
      "              'test_sentences': [{'max_token': 'a',\n",
      "                                  'max_value': 'high',\n",
      "                                  'max_value_token_index': 8,\n",
      "                                  'sentence_string': '.com/watch?v=ieaZLdqGdhwĊĊ',\n",
      "                                  'tokens': ['.',\n",
      "                                             'com',\n",
      "                                             '/',\n",
      "                                             'watch',\n",
      "                                             '?',\n",
      "                                             'v',\n",
      "                                             '=',\n",
      "                                             'ie',\n",
      "                                             'a',\n",
      "                                             'Z',\n",
      "                                             'L',\n",
      "                                             'dq',\n",
      "                                             'G',\n",
      "                                             'dh',\n",
      "                                             'w',\n",
      "                                             'Ċ',\n",
      "                                             'Ċ'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             14.3837890625,\n",
      "                                             26.07519912719727,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_token': 'o',\n",
      "                                  'max_value': 'high',\n",
      "                                  'max_value_token_index': 8,\n",
      "                                  'sentence_string': ' https://t.co/bUoHsX8HJf '\n",
      "                                                     'âĢĶ',\n",
      "                                  'tokens': [' https',\n",
      "                                             '://',\n",
      "                                             't',\n",
      "                                             '.',\n",
      "                                             'co',\n",
      "                                             '/',\n",
      "                                             'b',\n",
      "                                             'U',\n",
      "                                             'o',\n",
      "                                             'H',\n",
      "                                             's',\n",
      "                                             'X',\n",
      "                                             '8',\n",
      "                                             'H',\n",
      "                                             'J',\n",
      "                                             'f',\n",
      "                                             ' âĢĶ'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             6.125267505645752,\n",
      "                                             23.54202079772949,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_token': 'o',\n",
      "                                  'max_value': 'high',\n",
      "                                  'max_value_token_index': 8,\n",
      "                                  'sentence_string': 'VMxEzARBgNVBAoTCkdvb2dsZ',\n",
      "                                  'tokens': ['VM',\n",
      "                                             'x',\n",
      "                                             'E',\n",
      "                                             'z',\n",
      "                                             'ARB',\n",
      "                                             'g',\n",
      "                                             'NV',\n",
      "                                             'BA',\n",
      "                                             'o',\n",
      "                                             'TC',\n",
      "                                             'k',\n",
      "                                             'd',\n",
      "                                             'v',\n",
      "                                             'b',\n",
      "                                             '2',\n",
      "                                             'ds',\n",
      "                                             'Z'],\n",
      "                                  'values': [0,\n",
      "                                             2.344800233840942,\n",
      "                                             5.742846965789795,\n",
      "                                             0.0106344074010849,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             5.923023223876953,\n",
      "                                             22.91638946533203,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_value': 'low',\n",
      "                                  'sentence_string': ' time before a fetus was '\n",
      "                                                     'infected from the '\n",
      "                                                     'virus,\" he said.ĊĊ\"The',\n",
      "                                  'tokens': [' time',\n",
      "                                             ' before',\n",
      "                                             ' a',\n",
      "                                             ' fetus',\n",
      "                                             ' was',\n",
      "                                             ' infected',\n",
      "                                             ' from',\n",
      "                                             ' the',\n",
      "                                             ' virus',\n",
      "                                             ',\"',\n",
      "                                             ' he',\n",
      "                                             ' said',\n",
      "                                             '.',\n",
      "                                             'Ċ',\n",
      "                                             'Ċ',\n",
      "                                             '\"',\n",
      "                                             'The'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_value': 'low',\n",
      "                                  'sentence_string': ' also provides an '\n",
      "                                                     'exaggerated account of '\n",
      "                                                     \"Prithviraj's war against \"\n",
      "                                                     'the Chandelas',\n",
      "                                  'tokens': [' also',\n",
      "                                             ' provides',\n",
      "                                             ' an',\n",
      "                                             ' exaggerated',\n",
      "                                             ' account',\n",
      "                                             ' of',\n",
      "                                             ' Pr',\n",
      "                                             'ith',\n",
      "                                             'vir',\n",
      "                                             'aj',\n",
      "                                             \"'s\",\n",
      "                                             ' war',\n",
      "                                             ' against',\n",
      "                                             ' the',\n",
      "                                             ' Chand',\n",
      "                                             'el',\n",
      "                                             'as'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_value': 'low',\n",
      "                                  'sentence_string': ' when soldiers and '\n",
      "                                                     'sailors were swimming in '\n",
      "                                                     'mustard agent when '\n",
      "                                                     'Germans bombed U.S. '\n",
      "                                                     'ships',\n",
      "                                  'tokens': [' when',\n",
      "                                             ' soldiers',\n",
      "                                             ' and',\n",
      "                                             ' sailors',\n",
      "                                             ' were',\n",
      "                                             ' swimming',\n",
      "                                             ' in',\n",
      "                                             ' mustard',\n",
      "                                             ' agent',\n",
      "                                             ' when',\n",
      "                                             ' Germans',\n",
      "                                             ' bombed',\n",
      "                                             ' U',\n",
      "                                             '.',\n",
      "                                             'S',\n",
      "                                             '.',\n",
      "                                             ' ships'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]}]},\n",
      "             {'description': 'mentions of statistical information and survey '\n",
      "                             'results',\n",
      "              'feature_index': 883,\n",
      "              'gpt_predictions': [(1, 0.0),\n",
      "                                  (1, 0.0),\n",
      "                                  (1, 0.0),\n",
      "                                  (0, 0.0),\n",
      "                                  (0, 0.0),\n",
      "                                  (0, 1.0)],\n",
      "              'highest_activation': 11.25397968292236,\n",
      "              'show_sentences': [],\n",
      "              'test_sentences': [{'max_token': ' statistically',\n",
      "                                  'max_value': 'high',\n",
      "                                  'max_value_token_index': 8,\n",
      "                                  'sentence_string': ' take a stand against '\n",
      "                                                     'slavery and racism was '\n",
      "                                                     'statistically quite '\n",
      "                                                     'small. But the chance '\n",
      "                                                     'that someone',\n",
      "                                  'tokens': [' take',\n",
      "                                             ' a',\n",
      "                                             ' stand',\n",
      "                                             ' against',\n",
      "                                             ' slavery',\n",
      "                                             ' and',\n",
      "                                             ' racism',\n",
      "                                             ' was',\n",
      "                                             ' statistically',\n",
      "                                             ' quite',\n",
      "                                             ' small',\n",
      "                                             '.',\n",
      "                                             ' But',\n",
      "                                             ' the',\n",
      "                                             ' chance',\n",
      "                                             ' that',\n",
      "                                             ' someone'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             11.25397968292236,\n",
      "                                             6.56877613067627,\n",
      "                                             5.384710788726807,\n",
      "                                             2.500287294387817,\n",
      "                                             0.9211032390594482,\n",
      "                                             2.876265287399292,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_token': ',',\n",
      "                                  'max_value': 'high',\n",
      "                                  'max_value_token_index': 8,\n",
      "                                  'sentence_string': ' of the way President '\n",
      "                                                     'Trump is handling '\n",
      "                                                     'healthcare, according to '\n",
      "                                                     'a new survey.ĊĊ',\n",
      "                                  'tokens': [' of',\n",
      "                                             ' the',\n",
      "                                             ' way',\n",
      "                                             ' President',\n",
      "                                             ' Trump',\n",
      "                                             ' is',\n",
      "                                             ' handling',\n",
      "                                             ' healthcare',\n",
      "                                             ',',\n",
      "                                             ' according',\n",
      "                                             ' to',\n",
      "                                             ' a',\n",
      "                                             ' new',\n",
      "                                             ' survey',\n",
      "                                             '.',\n",
      "                                             'Ċ',\n",
      "                                             'Ċ'],\n",
      "                                  'values': [2.289097785949707,\n",
      "                                             2.010042190551758,\n",
      "                                             1.221293807029724,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             10.89350700378418,\n",
      "                                             5.490302562713623,\n",
      "                                             10.26891994476318,\n",
      "                                             8.275634765625,\n",
      "                                             9.336747169494629,\n",
      "                                             0.3790759444236755,\n",
      "                                             4.056347370147705,\n",
      "                                             1.551887273788452,\n",
      "                                             5.155369758605957]},\n",
      "                                 {'max_token': ',',\n",
      "                                  'max_value': 'high',\n",
      "                                  'max_value_token_index': 4,\n",
      "                                  'sentence_string': ' Trump is handling '\n",
      "                                                     'healthcare, according to '\n",
      "                                                     'a new survey.ĊĊA CBS '\n",
      "                                                     'News poll',\n",
      "                                  'tokens': [' Trump',\n",
      "                                             ' is',\n",
      "                                             ' handling',\n",
      "                                             ' healthcare',\n",
      "                                             ',',\n",
      "                                             ' according',\n",
      "                                             ' to',\n",
      "                                             ' a',\n",
      "                                             ' new',\n",
      "                                             ' survey',\n",
      "                                             '.',\n",
      "                                             'Ċ',\n",
      "                                             'Ċ',\n",
      "                                             'A',\n",
      "                                             ' CBS',\n",
      "                                             ' News',\n",
      "                                             ' poll'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             10.89350700378418,\n",
      "                                             5.490302562713623,\n",
      "                                             10.26891994476318,\n",
      "                                             8.275634765625,\n",
      "                                             9.336747169494629,\n",
      "                                             0.3790759444236755,\n",
      "                                             4.056347370147705,\n",
      "                                             1.551887273788452,\n",
      "                                             5.155369758605957,\n",
      "                                             6.673123359680176,\n",
      "                                             0,\n",
      "                                             2.428754806518555,\n",
      "                                             1.280020475387573]},\n",
      "                                 {'max_value': 'low',\n",
      "                                  'sentence_string': ' to other countries. '\n",
      "                                                     'Also, getting an H-1B '\n",
      "                                                     'visa is tougher as '\n",
      "                                                     'compared',\n",
      "                                  'tokens': [' to',\n",
      "                                             ' other',\n",
      "                                             ' countries',\n",
      "                                             '.',\n",
      "                                             ' Also',\n",
      "                                             ',',\n",
      "                                             ' getting',\n",
      "                                             ' an',\n",
      "                                             ' H',\n",
      "                                             '-',\n",
      "                                             '1',\n",
      "                                             'B',\n",
      "                                             ' visa',\n",
      "                                             ' is',\n",
      "                                             ' tougher',\n",
      "                                             ' as',\n",
      "                                             ' compared'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_value': 'low',\n",
      "                                  'sentence_string': 'The potential for '\n",
      "                                                     'industry is huge, a '\n",
      "                                                     'Mckinsey estimate claims '\n",
      "                                                     'that the energy',\n",
      "                                  'tokens': ['The',\n",
      "                                             ' potential',\n",
      "                                             ' for',\n",
      "                                             ' industry',\n",
      "                                             ' is',\n",
      "                                             ' huge',\n",
      "                                             ',',\n",
      "                                             ' a',\n",
      "                                             ' M',\n",
      "                                             'ck',\n",
      "                                             'in',\n",
      "                                             'sey',\n",
      "                                             ' estimate',\n",
      "                                             ' claims',\n",
      "                                             ' that',\n",
      "                                             ' the',\n",
      "                                             ' energy'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_value': 'low',\n",
      "                                  'sentence_string': 'ock find after analyzing '\n",
      "                                                     'almost 30 years of U.S. '\n",
      "                                                     'survey data. At the',\n",
      "                                  'tokens': ['ock',\n",
      "                                             ' find',\n",
      "                                             ' after',\n",
      "                                             ' analyzing',\n",
      "                                             ' almost',\n",
      "                                             ' 30',\n",
      "                                             ' years',\n",
      "                                             ' of',\n",
      "                                             ' U',\n",
      "                                             '.',\n",
      "                                             'S',\n",
      "                                             '.',\n",
      "                                             ' survey',\n",
      "                                             ' data',\n",
      "                                             '.',\n",
      "                                             ' At',\n",
      "                                             ' the'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]}]},\n",
      "             {'description': 'words related to voting or expressing a choice',\n",
      "              'feature_index': 761,\n",
      "              'gpt_predictions': [(1, 1.0),\n",
      "                                  (1, 1.0),\n",
      "                                  (1, 1.0),\n",
      "                                  (0, 0.0),\n",
      "                                  (0, 0.0),\n",
      "                                  (0, 0.0)],\n",
      "              'highest_activation': 63.60680770874023,\n",
      "              'show_sentences': [],\n",
      "              'test_sentences': [{'max_token': ' vote',\n",
      "                                  'max_value': 'high',\n",
      "                                  'max_value_token_index': 8,\n",
      "                                  'sentence_string': ' that route, Rigell '\n",
      "                                                     'seems inclined to vote '\n",
      "                                                     'no, but if he were to '\n",
      "                                                     'back',\n",
      "                                  'tokens': [' that',\n",
      "                                             ' route',\n",
      "                                             ',',\n",
      "                                             ' Rig',\n",
      "                                             'ell',\n",
      "                                             ' seems',\n",
      "                                             ' inclined',\n",
      "                                             ' to',\n",
      "                                             ' vote',\n",
      "                                             ' no',\n",
      "                                             ',',\n",
      "                                             ' but',\n",
      "                                             ' if',\n",
      "                                             ' he',\n",
      "                                             ' were',\n",
      "                                             ' to',\n",
      "                                             ' back'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             63.60680770874023,\n",
      "                                             4.982561588287354,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_token': ' voted',\n",
      "                                  'max_value': 'high',\n",
      "                                  'max_value_token_index': 8,\n",
      "                                  'sentence_string': ', according to '\n",
      "                                                     'Corker.ĊĊHe voted '\n",
      "                                                     'against the annual '\n",
      "                                                     'defense policy bill, a',\n",
      "                                  'tokens': [',',\n",
      "                                             ' according',\n",
      "                                             ' to',\n",
      "                                             ' Corker',\n",
      "                                             '.',\n",
      "                                             'Ċ',\n",
      "                                             'Ċ',\n",
      "                                             'He',\n",
      "                                             ' voted',\n",
      "                                             ' against',\n",
      "                                             ' the',\n",
      "                                             ' annual',\n",
      "                                             ' defense',\n",
      "                                             ' policy',\n",
      "                                             ' bill',\n",
      "                                             ',',\n",
      "                                             ' a'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             60.83102798461914,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_token': ' voted',\n",
      "                                  'max_value': 'high',\n",
      "                                  'max_value_token_index': 8,\n",
      "                                  'sentence_string': ' marijuana users in '\n",
      "                                                     'Colorado and Washington, '\n",
      "                                                     'which voted in November '\n",
      "                                                     'to legalize the '\n",
      "                                                     'recreational use of',\n",
      "                                  'tokens': [' marijuana',\n",
      "                                             ' users',\n",
      "                                             ' in',\n",
      "                                             ' Colorado',\n",
      "                                             ' and',\n",
      "                                             ' Washington',\n",
      "                                             ',',\n",
      "                                             ' which',\n",
      "                                             ' voted',\n",
      "                                             ' in',\n",
      "                                             ' November',\n",
      "                                             ' to',\n",
      "                                             ' legalize',\n",
      "                                             ' the',\n",
      "                                             ' recreational',\n",
      "                                             ' use',\n",
      "                                             ' of'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             60.82101821899414,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_value': 'low',\n",
      "                                  'sentence_string': ' the disease remains '\n",
      "                                                     'largely unknown, '\n",
      "                                                     'genetic, viral, hormonal '\n",
      "                                                     'and environmental '\n",
      "                                                     'factors have been so',\n",
      "                                  'tokens': [' the',\n",
      "                                             ' disease',\n",
      "                                             ' remains',\n",
      "                                             ' largely',\n",
      "                                             ' unknown',\n",
      "                                             ',',\n",
      "                                             ' genetic',\n",
      "                                             ',',\n",
      "                                             ' viral',\n",
      "                                             ',',\n",
      "                                             ' hormonal',\n",
      "                                             ' and',\n",
      "                                             ' environmental',\n",
      "                                             ' factors',\n",
      "                                             ' have',\n",
      "                                             ' been',\n",
      "                                             ' so'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_value': 'low',\n",
      "                                  'sentence_string': ' of these are the ones '\n",
      "                                                     'associated with '\n",
      "                                                     'bacterial virulence ( 9 '\n",
      "                                                     'âĢĵ 11 ). The most',\n",
      "                                  'tokens': [' of',\n",
      "                                             ' these',\n",
      "                                             ' are',\n",
      "                                             ' the',\n",
      "                                             ' ones',\n",
      "                                             ' associated',\n",
      "                                             ' with',\n",
      "                                             ' bacterial',\n",
      "                                             ' vir',\n",
      "                                             'ulence',\n",
      "                                             ' (',\n",
      "                                             ' 9',\n",
      "                                             ' âĢĵ',\n",
      "                                             ' 11',\n",
      "                                             ' ).',\n",
      "                                             ' The',\n",
      "                                             ' most'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_value': 'low',\n",
      "                                  'sentence_string': ' height at some '\n",
      "                                                     'international '\n",
      "                                                     'summit.ĊĊOr is it '\n",
      "                                                     'because of these deeply '\n",
      "                                                     'controversial statements',\n",
      "                                  'tokens': [' height',\n",
      "                                             ' at',\n",
      "                                             ' some',\n",
      "                                             ' international',\n",
      "                                             ' summit',\n",
      "                                             '.',\n",
      "                                             'Ċ',\n",
      "                                             'Ċ',\n",
      "                                             'Or',\n",
      "                                             ' is',\n",
      "                                             ' it',\n",
      "                                             ' because',\n",
      "                                             ' of',\n",
      "                                             ' these',\n",
      "                                             ' deeply',\n",
      "                                             ' controversial',\n",
      "                                             ' statements'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]}]},\n",
      "             {'description': 'mentions of the Syrian President, Bashar '\n",
      "                             'al-Assad',\n",
      "              'feature_index': 319,\n",
      "              'gpt_predictions': [(1, 1.0),\n",
      "                                  (1, 1.0),\n",
      "                                  (1, 1.0),\n",
      "                                  (0, 0.0),\n",
      "                                  (0, 0.0),\n",
      "                                  (0, 0.0)],\n",
      "              'highest_activation': 87.25408172607422,\n",
      "              'show_sentences': [],\n",
      "              'test_sentences': [{'max_token': ' Bashar',\n",
      "                                  'max_value': 'high',\n",
      "                                  'max_value_token_index': 8,\n",
      "                                  'sentence_string': ' stance on Syria '\n",
      "                                                     'including the future of '\n",
      "                                                     'President Bashar '\n",
      "                                                     'al-Assad to accommodate '\n",
      "                                                     'Russia, opposition',\n",
      "                                  'tokens': [' stance',\n",
      "                                             ' on',\n",
      "                                             ' Syria',\n",
      "                                             ' including',\n",
      "                                             ' the',\n",
      "                                             ' future',\n",
      "                                             ' of',\n",
      "                                             ' President',\n",
      "                                             ' Bashar',\n",
      "                                             ' al',\n",
      "                                             '-',\n",
      "                                             'Assad',\n",
      "                                             ' to',\n",
      "                                             ' accommodate',\n",
      "                                             ' Russia',\n",
      "                                             ',',\n",
      "                                             ' opposition'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             87.25408172607422,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_token': ' Bashar',\n",
      "                                  'max_value': 'high',\n",
      "                                  'max_value_token_index': 8,\n",
      "                                  'sentence_string': ' has come to the defense '\n",
      "                                                     'of Syrian President '\n",
      "                                                     'Bashar al-Assad, warning '\n",
      "                                                     'Western nations not',\n",
      "                                  'tokens': [' has',\n",
      "                                             ' come',\n",
      "                                             ' to',\n",
      "                                             ' the',\n",
      "                                             ' defense',\n",
      "                                             ' of',\n",
      "                                             ' Syrian',\n",
      "                                             ' President',\n",
      "                                             ' Bashar',\n",
      "                                             ' al',\n",
      "                                             '-',\n",
      "                                             'Assad',\n",
      "                                             ',',\n",
      "                                             ' warning',\n",
      "                                             ' Western',\n",
      "                                             ' nations',\n",
      "                                             ' not'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             86.22286224365234,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_token': ' Bashar',\n",
      "                                  'max_value': 'high',\n",
      "                                  'max_value_token_index': 8,\n",
      "                                  'sentence_string': ' in the custody of the '\n",
      "                                                     'regime of President '\n",
      "                                                     'Bashar al-Assad, and '\n",
      "                                                     'initially fleeing to',\n",
      "                                  'tokens': [' in',\n",
      "                                             ' the',\n",
      "                                             ' custody',\n",
      "                                             ' of',\n",
      "                                             ' the',\n",
      "                                             ' regime',\n",
      "                                             ' of',\n",
      "                                             ' President',\n",
      "                                             ' Bashar',\n",
      "                                             ' al',\n",
      "                                             '-',\n",
      "                                             'Assad',\n",
      "                                             ',',\n",
      "                                             ' and',\n",
      "                                             ' initially',\n",
      "                                             ' fleeing',\n",
      "                                             ' to'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             84.18256378173828,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_value': 'low',\n",
      "                                  'sentence_string': ' time before a fetus was '\n",
      "                                                     'infected from the '\n",
      "                                                     'virus,\" he said.ĊĊ\"The',\n",
      "                                  'tokens': [' time',\n",
      "                                             ' before',\n",
      "                                             ' a',\n",
      "                                             ' fetus',\n",
      "                                             ' was',\n",
      "                                             ' infected',\n",
      "                                             ' from',\n",
      "                                             ' the',\n",
      "                                             ' virus',\n",
      "                                             ',\"',\n",
      "                                             ' he',\n",
      "                                             ' said',\n",
      "                                             '.',\n",
      "                                             'Ċ',\n",
      "                                             'Ċ',\n",
      "                                             '\"',\n",
      "                                             'The'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_value': 'low',\n",
      "                                  'sentence_string': ' also provides an '\n",
      "                                                     'exaggerated account of '\n",
      "                                                     \"Prithviraj's war against \"\n",
      "                                                     'the Chandelas',\n",
      "                                  'tokens': [' also',\n",
      "                                             ' provides',\n",
      "                                             ' an',\n",
      "                                             ' exaggerated',\n",
      "                                             ' account',\n",
      "                                             ' of',\n",
      "                                             ' Pr',\n",
      "                                             'ith',\n",
      "                                             'vir',\n",
      "                                             'aj',\n",
      "                                             \"'s\",\n",
      "                                             ' war',\n",
      "                                             ' against',\n",
      "                                             ' the',\n",
      "                                             ' Chand',\n",
      "                                             'el',\n",
      "                                             'as'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_value': 'low',\n",
      "                                  'sentence_string': ' when soldiers and '\n",
      "                                                     'sailors were swimming in '\n",
      "                                                     'mustard agent when '\n",
      "                                                     'Germans bombed U.S. '\n",
      "                                                     'ships',\n",
      "                                  'tokens': [' when',\n",
      "                                             ' soldiers',\n",
      "                                             ' and',\n",
      "                                             ' sailors',\n",
      "                                             ' were',\n",
      "                                             ' swimming',\n",
      "                                             ' in',\n",
      "                                             ' mustard',\n",
      "                                             ' agent',\n",
      "                                             ' when',\n",
      "                                             ' Germans',\n",
      "                                             ' bombed',\n",
      "                                             ' U',\n",
      "                                             '.',\n",
      "                                             'S',\n",
      "                                             '.',\n",
      "                                             ' ships'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]}]}],\n",
      " 'timestamp': 1715375590.7580118}\n"
     ]
    }
   ],
   "source": [
    "with open(f'feats.json', 'r') as file:\n",
    "    feature_data = json.load(file)\n",
    "\n",
    "results = run_experiments(\n",
    "    num_features=20, \n",
    "    feature_data=feature_data,\n",
    "    test_pos=3, # Experiment with\n",
    "    test_neg=3, # Experiment with\n",
    "    show_pos=0, # Experiment with\n",
    "    show_neg=0, # Experiment with\n",
    "    neg_type='others', # Experiment with\n",
    "    binary_class=True, # Experiment with\n",
    "    show_max_token=True, # Experiment with\n",
    "    num_completions=1, # Experiment with\n",
    "    debug=False, \n",
    "    randomize_pos=False, \n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "# the run_experiments function automatically saves results to results/exp_{timestamp}.json\n",
    "pprint.pprint(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a past result file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hyperparameters': {'binary_class': True,\n",
      "                     'debug': False,\n",
      "                     'neg_type': 'others',\n",
      "                     'num_completions': 1,\n",
      "                     'randomize_pos': True,\n",
      "                     'seed': 42,\n",
      "                     'show_max_token': False,\n",
      "                     'show_neg': 0,\n",
      "                     'show_pos': 0,\n",
      "                     'test_neg': 3,\n",
      "                     'test_pos': 3},\n",
      " 'num_features': 1,\n",
      " 'results': [{'description': 'phrases related to direct confrontation or '\n",
      "                             'comparison',\n",
      "              'feature_index': 3111,\n",
      "              'gpt_predictions': [[1, 0.0],\n",
      "                                  [1, 0.0],\n",
      "                                  [1, 1.0],\n",
      "                                  [0, 0.0],\n",
      "                                  [0, 0.0],\n",
      "                                  [0, 0.0]],\n",
      "              'highest_activation': 44.37568664550781,\n",
      "              'show_sentences': [],\n",
      "              'test_sentences': [{'max_token': '-',\n",
      "                                  'max_value': 'high',\n",
      "                                  'max_value_token_index': 8,\n",
      "                                  'sentence_string': ' go with Bisping in a '\n",
      "                                                     'point-fighting '\n",
      "                                                     'situation. But I can see '\n",
      "                                                     'G',\n",
      "                                  'tokens': [' go',\n",
      "                                             ' with',\n",
      "                                             ' B',\n",
      "                                             'isp',\n",
      "                                             'ing',\n",
      "                                             ' in',\n",
      "                                             ' a',\n",
      "                                             ' point',\n",
      "                                             '-',\n",
      "                                             'fighting',\n",
      "                                             ' situation',\n",
      "                                             '.',\n",
      "                                             ' But',\n",
      "                                             ' I',\n",
      "                                             ' can',\n",
      "                                             ' see',\n",
      "                                             ' G'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             33.91120910644531,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_token': '-',\n",
      "                                  'max_value': 'high',\n",
      "                                  'max_value_token_index': 8,\n",
      "                                  'sentence_string': ' doesn<|endoftext|> huge '\n",
      "                                                     'market share in '\n",
      "                                                     \"'point-of-care rapid \"\n",
      "                                                     'tests used in hospitals',\n",
      "                                  'tokens': [' doesn',\n",
      "                                             '<|endoftext|>',\n",
      "                                             ' huge',\n",
      "                                             ' market',\n",
      "                                             ' share',\n",
      "                                             ' in',\n",
      "                                             \" '\",\n",
      "                                             'point',\n",
      "                                             '-',\n",
      "                                             'of',\n",
      "                                             '-',\n",
      "                                             'care',\n",
      "                                             ' rapid',\n",
      "                                             ' tests',\n",
      "                                             ' used',\n",
      "                                             ' in',\n",
      "                                             ' hospitals'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             2.900025844573975,\n",
      "                                             39.18094635009766,\n",
      "                                             0,\n",
      "                                             7.738319396972656,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_token': '-',\n",
      "                                  'max_value': 'high',\n",
      "                                  'max_value_token_index': 8,\n",
      "                                  'sentence_string': ', as<|endoftext|> the '\n",
      "                                                     'Sun Devils go '\n",
      "                                                     'head-to-head with the '\n",
      "                                                     'Blue Devils of',\n",
      "                                  'tokens': [',',\n",
      "                                             ' as',\n",
      "                                             '<|endoftext|>',\n",
      "                                             ' the',\n",
      "                                             ' Sun',\n",
      "                                             ' Devils',\n",
      "                                             ' go',\n",
      "                                             ' head',\n",
      "                                             '-',\n",
      "                                             'to',\n",
      "                                             '-',\n",
      "                                             'head',\n",
      "                                             ' with',\n",
      "                                             ' the',\n",
      "                                             ' Blue',\n",
      "                                             ' Devils',\n",
      "                                             ' of'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             33.09887313842773,\n",
      "                                             0,\n",
      "                                             1.359877228736877,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_value': 'low',\n",
      "                                  'sentence_string': 'Ċ1 cup waterĊĊ1 cup '\n",
      "                                                     'milkĊĊ1 teaspoon saltĊĊ5',\n",
      "                                  'tokens': ['Ċ',\n",
      "                                             '1',\n",
      "                                             ' cup',\n",
      "                                             ' water',\n",
      "                                             'Ċ',\n",
      "                                             'Ċ',\n",
      "                                             '1',\n",
      "                                             ' cup',\n",
      "                                             ' milk',\n",
      "                                             'Ċ',\n",
      "                                             'Ċ',\n",
      "                                             '1',\n",
      "                                             ' teaspoon',\n",
      "                                             ' salt',\n",
      "                                             'Ċ',\n",
      "                                             'Ċ',\n",
      "                                             '5'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_value': 'low',\n",
      "                                  'sentence_string': ': a six-month '\n",
      "                                                     'jet-setting junket of '\n",
      "                                                     'television and dance '\n",
      "                                                     'studios, A',\n",
      "                                  'tokens': [':',\n",
      "                                             ' a',\n",
      "                                             ' six',\n",
      "                                             '-',\n",
      "                                             'month',\n",
      "                                             ' jet',\n",
      "                                             '-',\n",
      "                                             'setting',\n",
      "                                             ' junk',\n",
      "                                             'et',\n",
      "                                             ' of',\n",
      "                                             ' television',\n",
      "                                             ' and',\n",
      "                                             ' dance',\n",
      "                                             ' studios',\n",
      "                                             ',',\n",
      "                                             ' A'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]},\n",
      "                                 {'max_value': 'low',\n",
      "                                  'sentence_string': 'owment.ĊĊBut U.S. '\n",
      "                                                     'interest in Chinese has '\n",
      "                                                     'fallen precipitously',\n",
      "                                  'tokens': ['owment',\n",
      "                                             '.',\n",
      "                                             'Ċ',\n",
      "                                             'Ċ',\n",
      "                                             'But',\n",
      "                                             ' U',\n",
      "                                             '.',\n",
      "                                             'S',\n",
      "                                             '.',\n",
      "                                             ' interest',\n",
      "                                             ' in',\n",
      "                                             ' Chinese',\n",
      "                                             ' has',\n",
      "                                             ' fallen',\n",
      "                                             ' precip',\n",
      "                                             'it',\n",
      "                                             'ously'],\n",
      "                                  'values': [0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0,\n",
      "                                             0]}]}],\n",
      " 'timestamp': 1715322008.339694}\n"
     ]
    }
   ],
   "source": [
    "json_data = load_json_results('results/exp_1715322008.339694.json')\n",
    "pprint.pprint(json_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get a simpler idea by printing the json tree (or just by opening a results file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{\n",
      "hyperparameters\n",
      "    {\n",
      "    test_pos\n",
      "    test_neg\n",
      "    show_pos\n",
      "    show_neg\n",
      "    binary_class\n",
      "    neg_type\n",
      "    show_max_token\n",
      "    num_completions\n",
      "    debug\n",
      "    randomize_pos\n",
      "    seed\n",
      "    }\n",
      "\n",
      "num_features\n",
      "results []\n",
      "        {\n",
      "        feature_index\n",
      "        gpt_predictions []\n",
      "                .\n",
      "                .\n",
      "                .\n",
      "            .\n",
      "            .\n",
      "            .\n",
      "        description\n",
      "        test_sentences []\n",
      "                {\n",
      "                max_value\n",
      "                max_value_token_index\n",
      "                sentence_string\n",
      "                max_token\n",
      "                tokens []\n",
      "                    .\n",
      "                    .\n",
      "                    .\n",
      "                values []\n",
      "                    .\n",
      "                    .\n",
      "                    .\n",
      "                }\n",
      "\n",
      "            .\n",
      "            .\n",
      "            .\n",
      "        show_sentences []\n",
      "            []\n",
      "        highest_activation\n",
      "        }\n",
      "\n",
      "    .\n",
      "    .\n",
      "    .\n",
      "timestamp\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print_json_tree(json_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do analysis on loaded json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_json_results(fetch_feature_data(1), 'feat1.json')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Older things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "predict_activations() got an unexpected keyword argument 'test_number'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# custom = [custom_accuracy(data, eps = 0.1) for data in all_data]\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m---> 19\u001b[0m     run()\n",
      "Cell \u001b[0;32mIn[4], line 6\u001b[0m, in \u001b[0;36mrun\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m():\n\u001b[0;32m----> 6\u001b[0m     data \u001b[38;5;241m=\u001b[39m get_predictions(\u001b[38;5;241m991\u001b[39m) \u001b[38;5;66;03m#806\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# for i in range(len(all_data)):\u001b[39;00m\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;66;03m# data = all_data[i]\u001b[39;00m\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;66;03m# print(feature_nums[i])\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m()\n",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m, in \u001b[0;36mget_predictions\u001b[0;34m(feature_num)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_predictions\u001b[39m(feature_num):\n\u001b[0;32m----> 2\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m predict_activations(feature_num, test_number\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, show_examples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m)\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m predictions\n",
      "\u001b[0;31mTypeError\u001b[0m: predict_activations() got an unexpected keyword argument 'test_number'"
     ]
    }
   ],
   "source": [
    "def get_predictions(feature_num):\n",
    "    predictions = predict_activations(feature_num, test_number=10, show_examples=8)\n",
    "    return predictions\n",
    "\n",
    "def run():\n",
    "    data = get_predictions(991) #806\n",
    "    # for i in range(len(all_data)):\n",
    "        # data = all_data[i]\n",
    "        # print(feature_nums[i])\n",
    "    print()\n",
    "    pprint.pprint(data)\n",
    "    custom = custom_accuracy(data)\n",
    "\n",
    "    print(custom)\n",
    "\n",
    "    # custom = [custom_accuracy(data, eps = 0.1) for data in all_data]\n",
    "\n",
    "for _ in range(1):\n",
    "    run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "### Losses\n",
    "def mse(data, normalize = False):\n",
    "    values = ([((elem[0]-elem[1])/(elem[0] if normalize else 1))**2 for elem in data])\n",
    "    return sum(values)/len(values)\n",
    "\n",
    "def nll_variant(data, eps = 1e-1):\n",
    "    values = ([np.log((min(elem) + eps)/(max(elem) + eps)) for elem in data])\n",
    "    return -sum(values)/len(values)\n",
    "\n",
    "def l1(data, normalize = True, eps = 0.1):\n",
    "    values = ([((eps + abs(elem[0]-elem[1]))/((max(elem) if normalize else 1) + eps))  for elem in data])\n",
    "    return sum(values)/len(values)\n",
    "\n",
    "### Plots\n",
    "def plot_mses_cdf(mses):\n",
    "    # Plotting the Mean Squared Errors (MSE) for each dataset\n",
    "    mses_sorted = np.sort(mses)\n",
    "    cdf = np.arange(1, len(mses_sorted)+1) / len(mses_sorted)\n",
    "    plt.plot(mses_sorted, cdf)\n",
    "    plt.title('Cumulative Distribution Function of MSEs')\n",
    "    plt.xlabel('MSE')\n",
    "    plt.ylabel('CDF')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def plot_probability_distribution(data, bins='auto', density=True, title = \"Default Title\"):\n",
    "    \"\"\"\n",
    "    Plots the probability distribution of the given data using a histogram.\n",
    "\n",
    "    Parameters:\n",
    "    - data (list or numpy array): The floating point numbers whose distribution you want to plot.\n",
    "    - bins (int, sequence or str, optional): The method for calculating histogram bins. Default is 'auto'.\n",
    "    - density (bool, optional): If True, the histogram is normalized to form a probability density,\n",
    "                                i.e., the area under the histogram will sum to 1. Default is True.\n",
    "    \"\"\"\n",
    "    # Calculate the histogram\n",
    "    counts, bin_edges = np.histogram(data, bins=bins, density=density)\n",
    "\n",
    "    # Calculate bin centers\n",
    "    bin_centers = 0.5 * (bin_edges[:-1] + bin_edges[1:])\n",
    "\n",
    "    # Plotting the histogram\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.bar(bin_centers, counts*np.diff(bin_edges), align='center', width=np.diff(bin_edges), edgecolor='black', alpha=0.7)\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Probability Density')\n",
    "    plt.title('Probability Distribution of Data')\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def analyze_data(all_data):\n",
    "    mses = [mse(data, normalize = False) for data in all_data]\n",
    "    nlls = [nll_variant(data) for data in all_data]\n",
    "    l1s = [l1(data, normalize = True) for data in all_data]\n",
    "\n",
    "    print('l1s', sorted(l1s))\n",
    "    plot_probability_distribution(mses, title = \"Distribution of MSEs\")\n",
    "    plot_probability_distribution(nlls, title = \"Distribution of NLL variant\")\n",
    "    plot_probability_distribution(l1s, title = \"Distribution of l1s variant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'concurrent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[74], line 9\u001b[0m\n\u001b[1;32m      5\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m predict_activations(feature_num, test_number\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, show_examples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m)\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m predictions\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m concurrent\u001b[38;5;241m.\u001b[39mfutures\u001b[38;5;241m.\u001b[39mThreadPoolExecutor() \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[1;32m     10\u001b[0m     all_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(executor\u001b[38;5;241m.\u001b[39mmap(get_predictions, feature_nums))\n\u001b[1;32m     12\u001b[0m mses \u001b[38;5;241m=\u001b[39m [mse(data, normalize \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m all_data]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'concurrent' is not defined"
     ]
    }
   ],
   "source": [
    "feature_nums = [806]#random.sample(range(0, 1000), 10)\n",
    "\n",
    "def get_predictions(feature_num):\n",
    "    predictions = predict_activations(feature_num, test_number=10, show_examples=8)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    all_data = list(executor.map(get_predictions, feature_nums))\n",
    "\n",
    "mses = [mse(data, normalize = False) for data in all_data]\n",
    "nlls = [nll_variant(data) for data in all_data]\n",
    "l1s = [l1(data, normalize = True) for data in all_data]\n",
    "\n",
    "# print('l1s', sorted(l1s))\n",
    "plot_probability_distribution(mses, title = \"Distribution of MSEs\")\n",
    "plot_probability_distribution(nlls, title = \"Distribution of NLL variant\")\n",
    "plot_probability_distribution(l1s, title = \"Distribution of l1s variant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_accuracy(data):\n",
    "    eps = max([elem[0] for elem in data]) / 10\n",
    "    values = []\n",
    "    for elem in data:\n",
    "        true, pred = elem\n",
    "        ## Add eps to avoid zero case\n",
    "        true, pred = true + eps, pred + eps\n",
    "        # Scale values\n",
    "        true, pred = true ** 0.75, pred ** 0.75\n",
    "        # Calculate difference\n",
    "        difference = abs(true - pred)\n",
    "        # Take ratio\n",
    "        error = difference / max(true, pred)\n",
    "        \n",
    "        accuracy = 1 - error\n",
    "        values.append(accuracy)\n",
    "    return sum(values)/len(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'feature_nums' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m feature_nums\n",
      "\u001b[0;31mNameError\u001b[0m: name 'feature_nums' is not defined"
     ]
    }
   ],
   "source": [
    "feature_nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('You are evaluating an english description of an autoencoder feature. The '\n",
      " 'description should correspond to sentences which result in high activation. '\n",
      " 'The english description of the feature is: \" past tense verbs\"\\n'\n",
      " 'Here are 8 examples of sentences and their corresponding activations:\\n'\n",
      " ' Example: \" economy\\'s cooled off enough, but it wasn\\'t always so. Back in '\n",
      " 'the mid\", Activation: 19.96\\n'\n",
      " 'Example: \" NL<|endoftext|>,\" Watts said.ĊĊRubio\\'s disclosure sheds new '\n",
      " 'light on his\", Activation: 0.00\\n'\n",
      " 'Example: \" in their NL<|endoftext|>,\" Watts said.ĊĊRubio\\'s disclosure sheds '\n",
      " 'new light\", Activation: 0.00\\n'\n",
      " 'Example: \"ĊĊRubio\\'s disclosure sheds new light on his comments in October, '\n",
      " 'when he\", Activation: 0.00\\n'\n",
      " 'Example: \" be sure to add a great feel and glitz to any game. These '\n",
      " 'wonderful futuristic\", Activation: 0.00\\n'\n",
      " 'Example: \" their NL<|endoftext|>,\" Watts said.ĊĊRubio\\'s disclosure sheds '\n",
      " 'new light on\", Activation: 0.00\\n'\n",
      " 'Example: \" of California, proved that prawns eat the mollusc hosts. He '\n",
      " 'shared\", Activation: 0.00\\n'\n",
      " 'Example: \" songs,\" Willow said. \"These songs were created through characters '\n",
      " 'I have developed within my\", Activation: 16.54\\n'\n",
      " 'Use the provided samples and the provided description to predict the '\n",
      " 'activation on a new sentence.\\n'\n",
      " 'You MUST respond with ONLY a number and NO OTHER content.')\n",
      "[{'activation': 21.34770965576172,\n",
      "  'max_index': 8,\n",
      "  'sentence_string': ' the surface area of the land. This was the allocation '\n",
      "                     'that was specified in the plan'},\n",
      " {'activation': 21.08387565612793,\n",
      "  'max_index': 8,\n",
      "  'sentence_string': \" to their online bordcasting but they didn't treated \"\n",
      "                     'well who play in the NASL'},\n",
      " {'activation': 15.00903415679932,\n",
      "  'max_index': 8,\n",
      "  'sentence_string': ' industry for add-ons to applications that were not '\n",
      "                     'meant to be extendible.<|endoftext|>'},\n",
      " {'activation': 0,\n",
      "  'max_index': 0,\n",
      "  'sentence_string': ' Central American folklore. It is a shape-changing '\n",
      "                     'spirit that typically takes the form of'},\n",
      " {'activation': 24.55836486816406,\n",
      "  'max_index': 8,\n",
      "  'sentence_string': ' Great Value, and other brands. It was John Henry Heinz, '\n",
      "                     'in fact,'},\n",
      " {'activation': 0,\n",
      "  'max_index': 0,\n",
      "  'sentence_string': ' comments in October, when he warned his fellow '\n",
      "                     'Republicans against trying to make political hay out'},\n",
      " {'activation': 0,\n",
      "  'max_index': 0,\n",
      "  'sentence_string': \"Rubio's disclosure sheds new light on his comments in \"\n",
      "                     'October, when he warned his'},\n",
      " {'activation': 0,\n",
      "  'max_index': 0,\n",
      "  'sentence_string': ' sheds new light on his comments in October, when he '\n",
      "                     'warned his fellow Republicans against trying'},\n",
      " {'activation': 0,\n",
      "  'max_index': 0,\n",
      "  'sentence_string': \"ĊRubio's disclosure sheds new light on his comments in \"\n",
      "                     'October, when he warned'},\n",
      " {'activation': 12.68032932281494,\n",
      "  'max_index': 8,\n",
      "  'sentence_string': ' disrupt the discourse with other people, who were '\n",
      "                     'either supportive or who were neutral and just'}]\n",
      "[(21.34770965576172, 0.0),\n",
      " (21.08387565612793, 0.0),\n",
      " (15.00903415679932, 0.0),\n",
      " (0, 0.0),\n",
      " (24.55836486816406, 0.0),\n",
      " (0, 15.23),\n",
      " (0, 0.0),\n",
      " (0, 0.0),\n",
      " (0, 0.0),\n",
      " (12.68032932281494, 0.0)]\n",
      "0.4124388582827179\n"
     ]
    }
   ],
   "source": [
    "def run():\n",
    "    data = get_predictions(806)\n",
    "    # for i in range(len(all_data)):\n",
    "        # data = all_data[i]\n",
    "        # print(feature_nums[i])\n",
    "    pprint.pprint(data)\n",
    "    custom = custom_accuracy(data)\n",
    "    print(custom)\n",
    "\n",
    "    # custom = [custom_accuracy(data, eps = 0.1) for data in all_data]\n",
    "\n",
    "for _ in range(1):\n",
    "    run()\n",
    "\n",
    "# plot_probability_distribution(custom, title = \"Distribution of custom accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[80], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m analyze_data(all_data)\n",
      "File \u001b[0;32m~/Desktop/GitHub/feature_benchmark/prediction_analysis.py:57\u001b[0m, in \u001b[0;36manalyze_data\u001b[0;34m(all_data)\u001b[0m\n",
      "File \u001b[0;32m~/Desktop/GitHub/feature_benchmark/prediction_analysis.py:7\u001b[0m, in \u001b[0;36mmse\u001b[0;34m(data, normalize)\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "analyze_data(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
