{
    "hyperparameters": {
        "test_pos": 5,
        "test_neg": 5,
        "show_pos": 0,
        "show_neg": 0,
        "binary_class": true,
        "neg_type": "others",
        "show_max_token": false,
        "num_completions": 1,
        "debug": false,
        "randomize_pos": true,
        "seed": 42
    },
    "num_features": 100,
    "results": [
        {
            "feature_index": 521,
            "gpt_predictions": [
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "timestamps or dates in a specific format",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " demonetization issue.\u010a\u010aFirst Published: Dec 07, 2016 18:57",
                    "max_token": " Published",
                    "tokens": [
                        " demon",
                        "et",
                        "ization",
                        " issue",
                        ".",
                        "\u010a",
                        "\u010a",
                        "First",
                        " Published",
                        ":",
                        " Dec",
                        " 07",
                        ",",
                        " 2016",
                        " 18",
                        ":",
                        "57"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        46.11051177978516,
                        12.55528831481934,
                        2.171714067459106,
                        6.039763450622559,
                        5.608806133270264,
                        5.501268863677979,
                        4.540326595306396,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "ladesh, asia\u010a\u010aFirst posted<|endoftext|>When Sergeant Clay Hunt left the U",
                    "max_token": " posted",
                    "tokens": [
                        "l",
                        "adesh",
                        ",",
                        " as",
                        "ia",
                        "\u010a",
                        "\u010a",
                        "First",
                        " posted",
                        "<|endoftext|>",
                        "When",
                        " Sergeant",
                        " Clay",
                        " Hunt",
                        " left",
                        " the",
                        " U"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        49.49526596069336,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " markets, australia\u010a\u010aFirst posted<|endoftext|>FARMINGTON \u00e2\u0122\u0136 A former counselor",
                    "max_token": " posted",
                    "tokens": [
                        " markets",
                        ",",
                        " aust",
                        "ral",
                        "ia",
                        "\u010a",
                        "\u010a",
                        "First",
                        " posted",
                        "<|endoftext|>",
                        "F",
                        "ARM",
                        "INGTON",
                        " \u00e2\u0122\u0136",
                        " A",
                        " former",
                        " counselor"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        45.93206405639648,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 7,
                    "sentence_string": " Morning Links ---------------------- this week last week most discussed\u010a\u010aMost recommended from 60 comments",
                    "max_token": " week",
                    "tokens": [
                        " Morning",
                        " Links",
                        " --------------------",
                        "--",
                        " this",
                        " week",
                        " last",
                        " week",
                        " most",
                        " discussed",
                        "\u010a",
                        "\u010a",
                        "Most",
                        " recommended",
                        " from",
                        " 60",
                        " comments"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        1.456243395805359,
                        0,
                        4.115555763244629,
                        5.034063339233398,
                        8.674803733825684,
                        5.392209529876709,
                        4.928345680236816,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " part in that.\u010a\u010aLast 5 posts by Jason Ditz<|endoftext|>The Democrats and",
                    "max_token": " posts",
                    "tokens": [
                        " part",
                        " in",
                        " that",
                        ".",
                        "\u010a",
                        "\u010a",
                        "Last",
                        " 5",
                        " posts",
                        " by",
                        " Jason",
                        " D",
                        "itz",
                        "<|endoftext|>",
                        "The",
                        " Democrats",
                        " and"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0.6818268299102783,
                        15.06083965301514,
                        21.10032081604004,
                        9.019525527954102,
                        0,
                        1.647749423980713,
                        6.434071063995361,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " quick lead only to have to bow out before the end of the game because they went",
                    "tokens": [
                        " quick",
                        " lead",
                        " only",
                        " to",
                        " have",
                        " to",
                        " bow",
                        " out",
                        " before",
                        " the",
                        " end",
                        " of",
                        " the",
                        " game",
                        " because",
                        " they",
                        " went"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "\u010aAldrin was a drug user before, Castillo admitted, but he had",
                    "tokens": [
                        "\u010a",
                        "A",
                        "ld",
                        "rin",
                        " was",
                        " a",
                        " drug",
                        " user",
                        " before",
                        ",",
                        " Cast",
                        "illo",
                        " admitted",
                        ",",
                        " but",
                        " he",
                        " had"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " height at some international summit.\u010a\u010aOr is it because of these deeply controversial statements",
                    "tokens": [
                        " height",
                        " at",
                        " some",
                        " international",
                        " summit",
                        ".",
                        "\u010a",
                        "\u010a",
                        "Or",
                        " is",
                        " it",
                        " because",
                        " of",
                        " these",
                        " deeply",
                        " controversial",
                        " statements"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " years after he was jailed for espionage, before his release in 1969.\u010a\u010aHe",
                    "tokens": [
                        " years",
                        " after",
                        " he",
                        " was",
                        " jailed",
                        " for",
                        " espionage",
                        ",",
                        " before",
                        " his",
                        " release",
                        " in",
                        " 1969",
                        ".",
                        "\u010a",
                        "\u010a",
                        "He"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " faces with respect to women.\u010a\u010aA CNN poll released Monday showed Obama moving into",
                    "tokens": [
                        " faces",
                        " with",
                        " respect",
                        " to",
                        " women",
                        ".",
                        "\u010a",
                        "\u010a",
                        "A",
                        " CNN",
                        " poll",
                        " released",
                        " Monday",
                        " showed",
                        " Obama",
                        " moving",
                        " into"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 51.19043350219727
        },
        {
            "feature_index": 737,
            "gpt_predictions": [
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "references to locations or events related to the state of Florida",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " offending legends such as Ric Flair and Shawn Michaels by his recent behavior. Instead of",
                    "max_token": " Shawn",
                    "tokens": [
                        " offending",
                        " legends",
                        " such",
                        " as",
                        " Ric",
                        " Fl",
                        "air",
                        " and",
                        " Shawn",
                        " Michaels",
                        " by",
                        " his",
                        " recent",
                        " behavior",
                        ".",
                        " Instead",
                        " of"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        16.1812801361084,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "The spokesman for the Director of National Intelligence Shawn Turner said intelligence officials are \u00e2\u0122\u013ecurrently",
                    "max_token": " Shawn",
                    "tokens": [
                        "The",
                        " spokesman",
                        " for",
                        " the",
                        " Director",
                        " of",
                        " National",
                        " Intelligence",
                        " Shawn",
                        " Turner",
                        " said",
                        " intelligence",
                        " officials",
                        " are",
                        " \u00e2\u0122",
                        "\u013e",
                        "currently"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        16.70410919189453,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " dreaded the awkward yearbook shot and the mortifying middle school \u00e2\u0122\u013eglamour",
                    "max_token": " mort",
                    "tokens": [
                        " dreaded",
                        " the",
                        " awkward",
                        " year",
                        "book",
                        " shot",
                        " and",
                        " the",
                        " mort",
                        "ifying",
                        " middle",
                        " school",
                        " \u00e2\u0122",
                        "\u013e",
                        "g",
                        "lam",
                        "our"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        19.1842041015625,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " of Egypt\u00e2\u0122\u013bs major clubs. Mortada Mansour, the committee\u00e2\u0122\u013b",
                    "max_token": " Mort",
                    "tokens": [
                        " of",
                        " Egypt",
                        "\u00e2\u0122",
                        "\u013b",
                        "s",
                        " major",
                        " clubs",
                        ".",
                        " Mort",
                        "ada",
                        " Mans",
                        "our",
                        ",",
                        " the",
                        " committee",
                        "\u00e2\u0122",
                        "\u013b"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        21.67017936706543,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " executives Stefan Fellenberg, 38, and Florian Krause, 32, who own",
                    "max_token": " Flor",
                    "tokens": [
                        " executives",
                        " Stefan",
                        " Fell",
                        "enberg",
                        ",",
                        " 38",
                        ",",
                        " and",
                        " Flor",
                        "ian",
                        " Kra",
                        "use",
                        ",",
                        " 32",
                        ",",
                        " who",
                        " own"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        25.24061584472656,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " quick lead only to have to bow out before the end of the game because they went",
                    "tokens": [
                        " quick",
                        " lead",
                        " only",
                        " to",
                        " have",
                        " to",
                        " bow",
                        " out",
                        " before",
                        " the",
                        " end",
                        " of",
                        " the",
                        " game",
                        " because",
                        " they",
                        " went"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "\u010aAldrin was a drug user before, Castillo admitted, but he had",
                    "tokens": [
                        "\u010a",
                        "A",
                        "ld",
                        "rin",
                        " was",
                        " a",
                        " drug",
                        " user",
                        " before",
                        ",",
                        " Cast",
                        "illo",
                        " admitted",
                        ",",
                        " but",
                        " he",
                        " had"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " height at some international summit.\u010a\u010aOr is it because of these deeply controversial statements",
                    "tokens": [
                        " height",
                        " at",
                        " some",
                        " international",
                        " summit",
                        ".",
                        "\u010a",
                        "\u010a",
                        "Or",
                        " is",
                        " it",
                        " because",
                        " of",
                        " these",
                        " deeply",
                        " controversial",
                        " statements"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " years after he was jailed for espionage, before his release in 1969.\u010a\u010aHe",
                    "tokens": [
                        " years",
                        " after",
                        " he",
                        " was",
                        " jailed",
                        " for",
                        " espionage",
                        ",",
                        " before",
                        " his",
                        " release",
                        " in",
                        " 1969",
                        ".",
                        "\u010a",
                        "\u010a",
                        "He"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " faces with respect to women.\u010a\u010aA CNN poll released Monday showed Obama moving into",
                    "tokens": [
                        " faces",
                        " with",
                        " respect",
                        " to",
                        " women",
                        ".",
                        "\u010a",
                        "\u010a",
                        "A",
                        " CNN",
                        " poll",
                        " released",
                        " Monday",
                        " showed",
                        " Obama",
                        " moving",
                        " into"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 26.77979850769043
        },
        {
            "feature_index": 740,
            "gpt_predictions": [
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "mentions of the name \"Christian\" along with various contexts like sports, events, and other individuals",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": ", oil samples from Lewis Hamilton's and Sebastian Vettel's cars were taken away for",
                    "max_token": " Sebastian",
                    "tokens": [
                        ",",
                        " oil",
                        " samples",
                        " from",
                        " Lewis",
                        " Hamilton",
                        "'s",
                        " and",
                        " Sebastian",
                        " V",
                        "ettel",
                        "'s",
                        " cars",
                        " were",
                        " taken",
                        " away",
                        " for"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        19.03501319885254,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "My Name is Nashae Morin and Christian Genco saved my life with www.",
                    "max_token": " Christian",
                    "tokens": [
                        "My",
                        " Name",
                        " is",
                        " Nash",
                        "ae",
                        " Mor",
                        "in",
                        " and",
                        " Christian",
                        " Gen",
                        "co",
                        " saved",
                        " my",
                        " life",
                        " with",
                        " www",
                        "."
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        20.52132606506348,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " figures.\u010a\u010aA police spokesman, Christian Ciocan, said the two women",
                    "max_token": " Christian",
                    "tokens": [
                        " figures",
                        ".",
                        "\u010a",
                        "\u010a",
                        "A",
                        " police",
                        " spokesman",
                        ",",
                        " Christian",
                        " Ci",
                        "oc",
                        "an",
                        ",",
                        " said",
                        " the",
                        " two",
                        " women"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        22.01714134216309,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " Yet his stance, the development of which Christian Ryan has done a superb job of chronic",
                    "max_token": " Christian",
                    "tokens": [
                        " Yet",
                        " his",
                        " stance",
                        ",",
                        " the",
                        " development",
                        " of",
                        " which",
                        " Christian",
                        " Ryan",
                        " has",
                        " done",
                        " a",
                        " superb",
                        " job",
                        " of",
                        " chronic"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        26.40810775756836,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " dipping left-foot strike went straight over Christian Wetklo. Like several of his",
                    "max_token": " Christian",
                    "tokens": [
                        " dipping",
                        " left",
                        "-",
                        "foot",
                        " strike",
                        " went",
                        " straight",
                        " over",
                        " Christian",
                        " Wet",
                        "k",
                        "lo",
                        ".",
                        " Like",
                        " several",
                        " of",
                        " his"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        29.46577072143555,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " quick lead only to have to bow out before the end of the game because they went",
                    "tokens": [
                        " quick",
                        " lead",
                        " only",
                        " to",
                        " have",
                        " to",
                        " bow",
                        " out",
                        " before",
                        " the",
                        " end",
                        " of",
                        " the",
                        " game",
                        " because",
                        " they",
                        " went"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "\u010aAldrin was a drug user before, Castillo admitted, but he had",
                    "tokens": [
                        "\u010a",
                        "A",
                        "ld",
                        "rin",
                        " was",
                        " a",
                        " drug",
                        " user",
                        " before",
                        ",",
                        " Cast",
                        "illo",
                        " admitted",
                        ",",
                        " but",
                        " he",
                        " had"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " height at some international summit.\u010a\u010aOr is it because of these deeply controversial statements",
                    "tokens": [
                        " height",
                        " at",
                        " some",
                        " international",
                        " summit",
                        ".",
                        "\u010a",
                        "\u010a",
                        "Or",
                        " is",
                        " it",
                        " because",
                        " of",
                        " these",
                        " deeply",
                        " controversial",
                        " statements"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " years after he was jailed for espionage, before his release in 1969.\u010a\u010aHe",
                    "tokens": [
                        " years",
                        " after",
                        " he",
                        " was",
                        " jailed",
                        " for",
                        " espionage",
                        ",",
                        " before",
                        " his",
                        " release",
                        " in",
                        " 1969",
                        ".",
                        "\u010a",
                        "\u010a",
                        "He"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " faces with respect to women.\u010a\u010aA CNN poll released Monday showed Obama moving into",
                    "tokens": [
                        " faces",
                        " with",
                        " respect",
                        " to",
                        " women",
                        ".",
                        "\u010a",
                        "\u010a",
                        "A",
                        " CNN",
                        " poll",
                        " released",
                        " Monday",
                        " showed",
                        " Obama",
                        " moving",
                        " into"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 33.55897903442383
        },
        {
            "feature_index": 660,
            "gpt_predictions": [
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "metaphorical expressions related to success and achievement",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " said that is we are (all) God\u00e2\u0122\u013bs children.\u00e2\u0122\u013f\u010a",
                    "max_token": " God",
                    "tokens": [
                        " said",
                        " that",
                        " is",
                        " we",
                        " are",
                        " (",
                        "all",
                        ")",
                        " God",
                        "\u00e2\u0122",
                        "\u013b",
                        "s",
                        " children",
                        ".",
                        "\u00e2\u0122",
                        "\u013f",
                        "\u010a"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        4.575788497924805,
                        0,
                        0,
                        0,
                        1.955355763435364,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " took maybe<|endoftext|>It's truly the Wild West here in California,\" one anonymous software executive",
                    "max_token": " West",
                    "tokens": [
                        " took",
                        " maybe",
                        "<|endoftext|>",
                        "It",
                        "'s",
                        " truly",
                        " the",
                        " Wild",
                        " West",
                        " here",
                        " in",
                        " California",
                        ",\"",
                        " one",
                        " anonymous",
                        " software",
                        " executive"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        1.670000076293945,
                        8.280374526977539,
                        5.196239948272705,
                        0,
                        1.702194213867188,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " will be the market that squeezes the balloon that will create a new shape.\"\u010a",
                    "max_token": " balloon",
                    "tokens": [
                        " will",
                        " be",
                        " the",
                        " market",
                        " that",
                        " squee",
                        "zes",
                        " the",
                        " balloon",
                        " that",
                        " will",
                        " create",
                        " a",
                        " new",
                        " shape",
                        ".\"",
                        "\u010a"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0.1844971030950546,
                        0,
                        0,
                        0.3289022743701935,
                        0,
                        3.732946872711182,
                        0,
                        0,
                        0,
                        0,
                        0,
                        1.723617434501648,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " ended up giving her a large dose of it.\u010a\u010aLast year, I set",
                    "max_token": " it",
                    "tokens": [
                        " ended",
                        " up",
                        " giving",
                        " her",
                        " a",
                        " large",
                        " dose",
                        " of",
                        " it",
                        ".",
                        "\u010a",
                        "\u010a",
                        "Last",
                        " year",
                        ",",
                        " I",
                        " set"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        6.34404468536377,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " the right way. They got right to it, they investigated it thoroughly. We're",
                    "max_token": " it",
                    "tokens": [
                        " the",
                        " right",
                        " way",
                        ".",
                        " They",
                        " got",
                        " right",
                        " to",
                        " it",
                        ",",
                        " they",
                        " investigated",
                        " it",
                        " thoroughly",
                        ".",
                        " We",
                        "'re"
                    ],
                    "values": [
                        0,
                        0,
                        2.243355989456177,
                        0,
                        0,
                        0,
                        0,
                        0,
                        8.4086332321167,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " quick lead only to have to bow out before the end of the game because they went",
                    "tokens": [
                        " quick",
                        " lead",
                        " only",
                        " to",
                        " have",
                        " to",
                        " bow",
                        " out",
                        " before",
                        " the",
                        " end",
                        " of",
                        " the",
                        " game",
                        " because",
                        " they",
                        " went"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "\u010aAldrin was a drug user before, Castillo admitted, but he had",
                    "tokens": [
                        "\u010a",
                        "A",
                        "ld",
                        "rin",
                        " was",
                        " a",
                        " drug",
                        " user",
                        " before",
                        ",",
                        " Cast",
                        "illo",
                        " admitted",
                        ",",
                        " but",
                        " he",
                        " had"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " height at some international summit.\u010a\u010aOr is it because of these deeply controversial statements",
                    "tokens": [
                        " height",
                        " at",
                        " some",
                        " international",
                        " summit",
                        ".",
                        "\u010a",
                        "\u010a",
                        "Or",
                        " is",
                        " it",
                        " because",
                        " of",
                        " these",
                        " deeply",
                        " controversial",
                        " statements"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " years after he was jailed for espionage, before his release in 1969.\u010a\u010aHe",
                    "tokens": [
                        " years",
                        " after",
                        " he",
                        " was",
                        " jailed",
                        " for",
                        " espionage",
                        ",",
                        " before",
                        " his",
                        " release",
                        " in",
                        " 1969",
                        ".",
                        "\u010a",
                        "\u010a",
                        "He"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " faces with respect to women.\u010a\u010aA CNN poll released Monday showed Obama moving into",
                    "tokens": [
                        " faces",
                        " with",
                        " respect",
                        " to",
                        " women",
                        ".",
                        "\u010a",
                        "\u010a",
                        "A",
                        " CNN",
                        " poll",
                        " released",
                        " Monday",
                        " showed",
                        " Obama",
                        " moving",
                        " into"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 11.15029907226562
        },
        {
            "feature_index": 411,
            "gpt_predictions": [
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "references to locations in the United States",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " DVD and Blu-ray Disc in North America on June 19. The K-<|endoftext|>",
                    "max_token": " America",
                    "tokens": [
                        " DVD",
                        " and",
                        " Blu",
                        "-",
                        "ray",
                        " Disc",
                        " in",
                        " North",
                        " America",
                        " on",
                        " June",
                        " 19",
                        ".",
                        " The",
                        " K",
                        "-",
                        "<|endoftext|>"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        7.199306964874268,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " adults and children). In the U.S., the multiracial population is more",
                    "max_token": "S",
                    "tokens": [
                        " adults",
                        " and",
                        " children",
                        ").",
                        " In",
                        " the",
                        " U",
                        ".",
                        "S",
                        ".,",
                        " the",
                        " mult",
                        "ir",
                        "acial",
                        " population",
                        " is",
                        " more"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0.03488287329673767,
                        0,
                        30.25625419616699,
                        1.632348895072937,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " leader in its industry in the U.S. and wanted to support our initiative to",
                    "max_token": "S",
                    "tokens": [
                        " leader",
                        " in",
                        " its",
                        " industry",
                        " in",
                        " the",
                        " U",
                        ".",
                        "S",
                        ".",
                        " and",
                        " wanted",
                        " to",
                        " support",
                        " our",
                        " initiative",
                        " to"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0.6888258457183838,
                        0,
                        30.51607322692871,
                        19.06884956359863,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " several hundred Zika cases in the U.S.In March, a traveler returning from",
                    "max_token": "S",
                    "tokens": [
                        " several",
                        " hundred",
                        " Zika",
                        " cases",
                        " in",
                        " the",
                        " U",
                        ".",
                        "S",
                        ".",
                        "In",
                        " March",
                        ",",
                        " a",
                        " traveler",
                        " returning",
                        " from"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        5.59996223449707,
                        0,
                        30.70767211914062,
                        21.1783332824707,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " arrests and purges in the U.S. and Saudi Arabia.\u010a\u010aIn",
                    "max_token": "S",
                    "tokens": [
                        " arrests",
                        " and",
                        " pur",
                        "ges",
                        " in",
                        " the",
                        " U",
                        ".",
                        "S",
                        ".",
                        " and",
                        " Saudi",
                        " Arabia",
                        ".",
                        "\u010a",
                        "\u010a",
                        "In"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        1.518266201019287,
                        0,
                        30.64189720153809,
                        21.00813102722168,
                        0.8432492017745972,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "\u00e2\u0122\u013bThe<|endoftext|> potential impact surveys and mitigation and restoration plans\u00e2\u0122\u013b\u00e2\u0122\u013b are",
                    "tokens": [
                        "\u00e2\u0122",
                        "\u013b",
                        "The",
                        "<|endoftext|>",
                        " potential",
                        " impact",
                        " surveys",
                        " and",
                        " mitigation",
                        " and",
                        " restoration",
                        " plans",
                        "\u00e2\u0122",
                        "\u013b",
                        "\u00e2\u0122",
                        "\u013b",
                        " are"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " that kid who felt unwanted, or the grown up who remembers how hard it was to",
                    "tokens": [
                        " that",
                        " kid",
                        " who",
                        " felt",
                        " unwanted",
                        ",",
                        " or",
                        " the",
                        " grown",
                        " up",
                        " who",
                        " remembers",
                        " how",
                        " hard",
                        " it",
                        " was",
                        " to"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " Trump and Hillary in the first debate, before either said a word, Trump made what",
                    "tokens": [
                        " Trump",
                        " and",
                        " Hillary",
                        " in",
                        " the",
                        " first",
                        " debate",
                        ",",
                        " before",
                        " either",
                        " said",
                        " a",
                        " word",
                        ",",
                        " Trump",
                        " made",
                        " what"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "3 billion) installing everything from traffic-management technologies to smart electricity grids.\u010a\u010a",
                    "tokens": [
                        "3",
                        " billion",
                        ")",
                        " installing",
                        " everything",
                        " from",
                        " traffic",
                        "-",
                        "management",
                        " technologies",
                        " to",
                        " smart",
                        " electricity",
                        " grids",
                        ".",
                        "\u010a",
                        "\u010a"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " city staff (Fire, Police, Crime Prevention, and more)\u010a\u010aOption to",
                    "tokens": [
                        " city",
                        " staff",
                        " (",
                        "Fire",
                        ",",
                        " Police",
                        ",",
                        " Crime",
                        " Prevention",
                        ",",
                        " and",
                        " more",
                        ")",
                        "\u010a",
                        "\u010a",
                        "Option",
                        " to"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 33.55773544311523
        },
        {
            "feature_index": 678,
            "gpt_predictions": [
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "numbers and financial terms",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 14,
                    "sentence_string": " San Francisco Division, IRS-CI\u00e2\u0122\u013bs Washington, D.C. Field",
                    "max_token": "C",
                    "tokens": [
                        " San",
                        " Francisco",
                        " Division",
                        ",",
                        " IRS",
                        "-",
                        "CI",
                        "\u00e2\u0122",
                        "\u013b",
                        "s",
                        " Washington",
                        ",",
                        " D",
                        ".",
                        "C",
                        ".",
                        " Field"
                    ],
                    "values": [
                        0,
                        0,
                        0.7192895412445068,
                        0,
                        0,
                        0,
                        0.993273138999939,
                        1.907752633094788,
                        3.319319725036621,
                        1.773530244827271,
                        1.266852140426636,
                        1.790255546569824,
                        4.653037071228027,
                        3.481429576873779,
                        5.333300590515137,
                        3.472521305084229,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " I was baffled. Many extremely intelligent people I knew and had much respect for were praising",
                    "max_token": " I",
                    "tokens": [
                        " I",
                        " was",
                        " baffled",
                        ".",
                        " Many",
                        " extremely",
                        " intelligent",
                        " people",
                        " I",
                        " knew",
                        " and",
                        " had",
                        " much",
                        " respect",
                        " for",
                        " were",
                        " praising"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0.002102859318256378,
                        5.44221305847168,
                        3.390740156173706,
                        0.8037203550338745,
                        2.963909864425659,
                        0.6603792309761047,
                        1.714730024337769,
                        1.652942776679993,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " contest in the US, its indispensable patron, had been concluded.\u010a\u010aPress reports",
                    "max_token": ",",
                    "tokens": [
                        " contest",
                        " in",
                        " the",
                        " US",
                        ",",
                        " its",
                        " indispensable",
                        " patron",
                        ",",
                        " had",
                        " been",
                        " concluded",
                        ".",
                        "\u010a",
                        "\u010a",
                        "Press",
                        " reports"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0.6339829564094543,
                        1.234820604324341,
                        3.758003711700439,
                        1.510690808296204,
                        3.151426792144775,
                        6.084495544433594,
                        1.25793981552124,
                        3.46759819984436,
                        1.413575172424316,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " Laura Schlessinger on radio outlets coast to coast and then leading a series of transparent",
                    "max_token": " to",
                    "tokens": [
                        " Laura",
                        " Sch",
                        "less",
                        "inger",
                        " on",
                        " radio",
                        " outlets",
                        " coast",
                        " to",
                        " coast",
                        " and",
                        " then",
                        " leading",
                        " a",
                        " series",
                        " of",
                        " transparent"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0.0639883279800415,
                        6.348100185394287,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "vqb \u00e2\u0122\u0136 Darren Rovell (@darrenrovell) March 20,<|endoftext|>",
                    "max_token": "dar",
                    "tokens": [
                        "v",
                        "q",
                        "b",
                        " \u00e2\u0122\u0136",
                        " Darren",
                        " Rove",
                        "ll",
                        " (@",
                        "dar",
                        "ren",
                        "ro",
                        "vell",
                        ")",
                        " March",
                        " 20",
                        ",",
                        "<|endoftext|>"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0.2221350818872452,
                        3.6205894947052,
                        4.586859703063965,
                        7.301548480987549,
                        3.626868724822998,
                        6.183331966400146,
                        4.624452114105225,
                        0,
                        0,
                        0,
                        0.1203726679086685,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " quick lead only to have to bow out before the end of the game because they went",
                    "tokens": [
                        " quick",
                        " lead",
                        " only",
                        " to",
                        " have",
                        " to",
                        " bow",
                        " out",
                        " before",
                        " the",
                        " end",
                        " of",
                        " the",
                        " game",
                        " because",
                        " they",
                        " went"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "\u010aAldrin was a drug user before, Castillo admitted, but he had",
                    "tokens": [
                        "\u010a",
                        "A",
                        "ld",
                        "rin",
                        " was",
                        " a",
                        " drug",
                        " user",
                        " before",
                        ",",
                        " Cast",
                        "illo",
                        " admitted",
                        ",",
                        " but",
                        " he",
                        " had"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " height at some international summit.\u010a\u010aOr is it because of these deeply controversial statements",
                    "tokens": [
                        " height",
                        " at",
                        " some",
                        " international",
                        " summit",
                        ".",
                        "\u010a",
                        "\u010a",
                        "Or",
                        " is",
                        " it",
                        " because",
                        " of",
                        " these",
                        " deeply",
                        " controversial",
                        " statements"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " years after he was jailed for espionage, before his release in 1969.\u010a\u010aHe",
                    "tokens": [
                        " years",
                        " after",
                        " he",
                        " was",
                        " jailed",
                        " for",
                        " espionage",
                        ",",
                        " before",
                        " his",
                        " release",
                        " in",
                        " 1969",
                        ".",
                        "\u010a",
                        "\u010a",
                        "He"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " faces with respect to women.\u010a\u010aA CNN poll released Monday showed Obama moving into",
                    "tokens": [
                        " faces",
                        " with",
                        " respect",
                        " to",
                        " women",
                        ".",
                        "\u010a",
                        "\u010a",
                        "A",
                        " CNN",
                        " poll",
                        " released",
                        " Monday",
                        " showed",
                        " Obama",
                        " moving",
                        " into"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 7.758599281311035
        },
        {
            "feature_index": 626,
            "gpt_predictions": [
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "scientific or investigative terms and concepts",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " faces of the dead and missing. It will allow their loved ones to vent and to",
                    "max_token": " will",
                    "tokens": [
                        " faces",
                        " of",
                        " the",
                        " dead",
                        " and",
                        " missing",
                        ".",
                        " It",
                        " will",
                        " allow",
                        " their",
                        " loved",
                        " ones",
                        " to",
                        " vent",
                        " and",
                        " to"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0.7027789950370789,
                        3.39075779914856,
                        8.0816650390625,
                        6.394636631011963,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " body of facts that have been repeatedly confirmed through observation and experiment\u00e2\u0122\u013b (as the",
                    "max_token": " through",
                    "tokens": [
                        " body",
                        " of",
                        " facts",
                        " that",
                        " have",
                        " been",
                        " repeatedly",
                        " confirmed",
                        " through",
                        " observation",
                        " and",
                        " experiment",
                        "\u00e2\u0122",
                        "\u013b",
                        " (",
                        "as",
                        " the"
                    ],
                    "values": [
                        0.8590021133422852,
                        0.0505746603012085,
                        0,
                        0,
                        0,
                        0,
                        0.8042903542518616,
                        0,
                        12.34135246276855,
                        2.496979713439941,
                        6.712646961212158,
                        4.425094127655029,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " she was struck with a bout of inspiration. She's seen focused and passionate, inside",
                    "max_token": ".",
                    "tokens": [
                        " she",
                        " was",
                        " struck",
                        " with",
                        " a",
                        " bout",
                        " of",
                        " inspiration",
                        ".",
                        " She",
                        "'s",
                        " seen",
                        " focused",
                        " and",
                        " passionate",
                        ",",
                        " inside"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        2.997834205627441,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 12,
                    "sentence_string": " scarecrow.\u010a\u010a\"We are now waiting the results of an autopsy.\u00e2\u0122",
                    "max_token": " of",
                    "tokens": [
                        " scare",
                        "crow",
                        ".",
                        "\u010a",
                        "\u010a",
                        "\"",
                        "We",
                        " are",
                        " now",
                        " waiting",
                        " the",
                        " results",
                        " of",
                        " an",
                        " autopsy",
                        ".",
                        "\u00e2\u0122"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0.2045534402132034,
                        4.958849906921387,
                        3.104662418365479,
                        7.869415283203125,
                        0.01084642857313156,
                        0,
                        0.3859583139419556,
                        13.29865169525146,
                        10.16722679138184,
                        4.433640956878662,
                        1.377121567726135,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": ". He arrived at the results by carrying out a simple blind sorting exercise.\u010a\u010a",
                    "max_token": " out",
                    "tokens": [
                        ".",
                        " He",
                        " arrived",
                        " at",
                        " the",
                        " results",
                        " by",
                        " carrying",
                        " out",
                        " a",
                        " simple",
                        " blind",
                        " sorting",
                        " exercise",
                        ".",
                        "\u010a",
                        "\u010a"
                    ],
                    "values": [
                        0,
                        0.6712586283683777,
                        0.341829240322113,
                        0,
                        0,
                        0,
                        10.4913215637207,
                        5.72594165802002,
                        13.04933929443359,
                        12.70283222198486,
                        10.34599494934082,
                        5.711758613586426,
                        4.181159973144531,
                        0,
                        4.11253547668457,
                        0,
                        2.224752426147461
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " quick lead only to have to bow out before the end of the game because they went",
                    "tokens": [
                        " quick",
                        " lead",
                        " only",
                        " to",
                        " have",
                        " to",
                        " bow",
                        " out",
                        " before",
                        " the",
                        " end",
                        " of",
                        " the",
                        " game",
                        " because",
                        " they",
                        " went"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "\u010aAldrin was a drug user before, Castillo admitted, but he had",
                    "tokens": [
                        "\u010a",
                        "A",
                        "ld",
                        "rin",
                        " was",
                        " a",
                        " drug",
                        " user",
                        " before",
                        ",",
                        " Cast",
                        "illo",
                        " admitted",
                        ",",
                        " but",
                        " he",
                        " had"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " height at some international summit.\u010a\u010aOr is it because of these deeply controversial statements",
                    "tokens": [
                        " height",
                        " at",
                        " some",
                        " international",
                        " summit",
                        ".",
                        "\u010a",
                        "\u010a",
                        "Or",
                        " is",
                        " it",
                        " because",
                        " of",
                        " these",
                        " deeply",
                        " controversial",
                        " statements"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " years after he was jailed for espionage, before his release in 1969.\u010a\u010aHe",
                    "tokens": [
                        " years",
                        " after",
                        " he",
                        " was",
                        " jailed",
                        " for",
                        " espionage",
                        ",",
                        " before",
                        " his",
                        " release",
                        " in",
                        " 1969",
                        ".",
                        "\u010a",
                        "\u010a",
                        "He"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " faces with respect to women.\u010a\u010aA CNN poll released Monday showed Obama moving into",
                    "tokens": [
                        " faces",
                        " with",
                        " respect",
                        " to",
                        " women",
                        ".",
                        "\u010a",
                        "\u010a",
                        "A",
                        " CNN",
                        " poll",
                        " released",
                        " Monday",
                        " showed",
                        " Obama",
                        " moving",
                        " into"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 14.85885715484619
        },
        {
            "feature_index": 513,
            "gpt_predictions": [
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "terms related to physical structures",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " economy, in particular) that retains no structural or social mechanism for rewarding individualism.",
                    "max_token": " structural",
                    "tokens": [
                        " economy",
                        ",",
                        " in",
                        " particular",
                        ")",
                        " that",
                        " retains",
                        " no",
                        " structural",
                        " or",
                        " social",
                        " mechanism",
                        " for",
                        " rewarding",
                        " individual",
                        "ism",
                        "."
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        22.71561050415039,
                        0,
                        0,
                        3.097696542739868,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "NS). The barrier is not a rigid structure, but a dynamic interface with a range",
                    "max_token": " structure",
                    "tokens": [
                        "NS",
                        ").",
                        " The",
                        " barrier",
                        " is",
                        " not",
                        " a",
                        " rigid",
                        " structure",
                        ",",
                        " but",
                        " a",
                        " dynamic",
                        " interface",
                        " with",
                        " a",
                        " range"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        6.020285129547119,
                        32.90822982788086,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " make AppLink the Android of automotive app architectures.\u010a\u010aIn an unprecedented move for",
                    "max_token": " architectures",
                    "tokens": [
                        " make",
                        " App",
                        "Link",
                        " the",
                        " Android",
                        " of",
                        " automotive",
                        " app",
                        " architectures",
                        ".",
                        "\u010a",
                        "\u010a",
                        "In",
                        " an",
                        " unprecedented",
                        " move",
                        " for"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        8.775824546813965,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " ladder injuries include: A ladder is a structure designed for climbing that consists of two long",
                    "max_token": " structure",
                    "tokens": [
                        " ladder",
                        " injuries",
                        " include",
                        ":",
                        " A",
                        " ladder",
                        " is",
                        " a",
                        " structure",
                        " designed",
                        " for",
                        " climbing",
                        " that",
                        " consists",
                        " of",
                        " two",
                        " long"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        36.36680221557617,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " (See figure A.)\u010a\u010aStructure\u010a\u010aHere come the metaphors. John",
                    "max_token": "ructure",
                    "tokens": [
                        " (",
                        "See",
                        " figure",
                        " A",
                        ".)",
                        "\u010a",
                        "\u010a",
                        "St",
                        "ructure",
                        "\u010a",
                        "\u010a",
                        "Here",
                        " come",
                        " the",
                        " metaphors",
                        ".",
                        " John"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        36.004150390625,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " quick lead only to have to bow out before the end of the game because they went",
                    "tokens": [
                        " quick",
                        " lead",
                        " only",
                        " to",
                        " have",
                        " to",
                        " bow",
                        " out",
                        " before",
                        " the",
                        " end",
                        " of",
                        " the",
                        " game",
                        " because",
                        " they",
                        " went"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "\u010aAldrin was a drug user before, Castillo admitted, but he had",
                    "tokens": [
                        "\u010a",
                        "A",
                        "ld",
                        "rin",
                        " was",
                        " a",
                        " drug",
                        " user",
                        " before",
                        ",",
                        " Cast",
                        "illo",
                        " admitted",
                        ",",
                        " but",
                        " he",
                        " had"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " height at some international summit.\u010a\u010aOr is it because of these deeply controversial statements",
                    "tokens": [
                        " height",
                        " at",
                        " some",
                        " international",
                        " summit",
                        ".",
                        "\u010a",
                        "\u010a",
                        "Or",
                        " is",
                        " it",
                        " because",
                        " of",
                        " these",
                        " deeply",
                        " controversial",
                        " statements"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " years after he was jailed for espionage, before his release in 1969.\u010a\u010aHe",
                    "tokens": [
                        " years",
                        " after",
                        " he",
                        " was",
                        " jailed",
                        " for",
                        " espionage",
                        ",",
                        " before",
                        " his",
                        " release",
                        " in",
                        " 1969",
                        ".",
                        "\u010a",
                        "\u010a",
                        "He"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " faces with respect to women.\u010a\u010aA CNN poll released Monday showed Obama moving into",
                    "tokens": [
                        " faces",
                        " with",
                        " respect",
                        " to",
                        " women",
                        ".",
                        "\u010a",
                        "\u010a",
                        "A",
                        " CNN",
                        " poll",
                        " released",
                        " Monday",
                        " showed",
                        " Obama",
                        " moving",
                        " into"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 42.0250129699707
        },
        {
            "feature_index": 859,
            "gpt_predictions": [
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    1.0
                ]
            ],
            "description": "disparities between genders or races in various aspects",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " were twice as high as those reported for women, and that people in the most deprived",
                    "max_token": " women",
                    "tokens": [
                        " were",
                        " twice",
                        " as",
                        " high",
                        " as",
                        " those",
                        " reported",
                        " for",
                        " women",
                        ",",
                        " and",
                        " that",
                        " people",
                        " in",
                        " the",
                        " most",
                        " deprived"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0.3207950592041016,
                        0,
                        0,
                        18.11948585510254,
                        4.182205677032471,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " 22 points more likely than self-identified liberals to say the government should allow this activity",
                    "max_token": " liberals",
                    "tokens": [
                        " 22",
                        " points",
                        " more",
                        " likely",
                        " than",
                        " self",
                        "-",
                        "identified",
                        " liberals",
                        " to",
                        " say",
                        " the",
                        " government",
                        " should",
                        " allow",
                        " this",
                        " activity"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        4.63433837890625,
                        0,
                        4.638870716094971,
                        26.02081489562988,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " are now a larger party then the Christian Democrats and the party of the left (Social",
                    "max_token": " Democrats",
                    "tokens": [
                        " are",
                        " now",
                        " a",
                        " larger",
                        " party",
                        " then",
                        " the",
                        " Christian",
                        " Democrats",
                        " and",
                        " the",
                        " party",
                        " of",
                        " the",
                        " left",
                        " (",
                        "Social"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        8.045856475830078,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " 7 times likely to be single parents than men. That could be attributed to the European",
                    "max_token": " men",
                    "tokens": [
                        " 7",
                        " times",
                        " likely",
                        " to",
                        " be",
                        " single",
                        " parents",
                        " than",
                        " men",
                        ".",
                        " That",
                        " could",
                        " be",
                        " attributed",
                        " to",
                        " the",
                        " European"
                    ],
                    "values": [
                        0,
                        0,
                        1.146076202392578,
                        0,
                        0,
                        0,
                        0,
                        0,
                        28.4832649230957,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " more threatened species than do non-urban areas, leading them to dub cities \u00e2\u0122\u013e",
                    "max_token": " areas",
                    "tokens": [
                        " more",
                        " threatened",
                        " species",
                        " than",
                        " do",
                        " non",
                        "-",
                        "urban",
                        " areas",
                        ",",
                        " leading",
                        " them",
                        " to",
                        " dub",
                        " cities",
                        " \u00e2\u0122",
                        "\u013e"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        3.117537975311279,
                        9.11357307434082,
                        3.36429500579834,
                        8.594101905822754,
                        27.62572860717773,
                        5.923726558685303,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " coat.\u010a\u010aDetective Inspector Steve Christian, from Liverpool CID, said:",
                    "tokens": [
                        " coat",
                        ".",
                        "\u010a",
                        "\u010a",
                        "Det",
                        "ective",
                        " Inspector",
                        " Steve",
                        " Christian",
                        ",",
                        " from",
                        " Liverpool",
                        " C",
                        "ID",
                        ",",
                        " said",
                        ":"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " drop out of the presidential race, according to NBC News ' Andrea Mitchell.\u010a\u010a",
                    "tokens": [
                        " drop",
                        " out",
                        " of",
                        " the",
                        " presidential",
                        " race",
                        ",",
                        " according",
                        " to",
                        " NBC",
                        " News",
                        " '",
                        " Andrea",
                        " Mitchell",
                        ".",
                        "\u010a",
                        "\u010a"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " Rumors that Amazon will soon begin accepting Bitcoin are persistent. If the retail giant does",
                    "tokens": [
                        " Rum",
                        "ors",
                        " that",
                        " Amazon",
                        " will",
                        " soon",
                        " begin",
                        " accepting",
                        " Bitcoin",
                        " are",
                        " persistent",
                        ".",
                        " If",
                        " the",
                        " retail",
                        " giant",
                        " does"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": ", and banking flows was a monthly net TIC (Treasury International Capital)<|endoftext|>",
                    "tokens": [
                        ",",
                        " and",
                        " banking",
                        " flows",
                        " was",
                        " a",
                        " monthly",
                        " net",
                        " T",
                        "IC",
                        " (",
                        "Tre",
                        "asury",
                        " International",
                        " Capital",
                        ")",
                        "<|endoftext|>"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " faces with respect to women.\u010a\u010aA CNN poll released Monday showed Obama moving into",
                    "tokens": [
                        " faces",
                        " with",
                        " respect",
                        " to",
                        " women",
                        ".",
                        "\u010a",
                        "\u010a",
                        "A",
                        " CNN",
                        " poll",
                        " released",
                        " Monday",
                        " showed",
                        " Obama",
                        " moving",
                        " into"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 32.53410720825195
        },
        {
            "feature_index": 136,
            "gpt_predictions": [
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    1.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "Proper nouns related to politics, names, and affiliations",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " 350 per cent in Austria amid 'unease' over increasing numbers of migrants following the",
                    "max_token": "ase",
                    "tokens": [
                        " 350",
                        " per",
                        " cent",
                        " in",
                        " Austria",
                        " amid",
                        " '",
                        "une",
                        "ase",
                        "'",
                        " over",
                        " increasing",
                        " numbers",
                        " of",
                        " migrants",
                        " following",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        25.93350791931152,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "\u010a\u010aMuslim leader Mohd. Yaseen Qureshi said that the Indian",
                    "max_token": "ase",
                    "tokens": [
                        "\u010a",
                        "\u010a",
                        "Muslim",
                        " leader",
                        " Moh",
                        "d",
                        ".",
                        " Y",
                        "ase",
                        "en",
                        " Q",
                        "ures",
                        "hi",
                        " said",
                        " that",
                        " the",
                        " Indian"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        30.31112098693848,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " je omezuje v \u00c4\u012fase \u00c4\u012fi v prostoru.\"\u010a",
                    "max_token": "ase",
                    "tokens": [
                        " je",
                        " o",
                        "me",
                        "zu",
                        "je",
                        " v",
                        " \u00c4",
                        "\u012f",
                        "ase",
                        " \u00c4",
                        "\u012f",
                        "i",
                        " v",
                        " prost",
                        "oru",
                        ".\"",
                        "\u010a"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        25.23637008666992,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": ".\u010a\u010aLoved the gift. Merry Christmas to you, Secret Santa.<|endoftext|>",
                    "max_token": " Merry",
                    "tokens": [
                        ".",
                        "\u010a",
                        "\u010a",
                        "L",
                        "oved",
                        " the",
                        " gift",
                        ".",
                        " Merry",
                        " Christmas",
                        " to",
                        " you",
                        ",",
                        " Secret",
                        " Santa",
                        ".",
                        "<|endoftext|>"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        6.274285316467285,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": ".\u00e2\u0122\u013f\u010a\u010aBCR member Chase Aplin said he believed that the jury",
                    "max_token": " Chase",
                    "tokens": [
                        ".",
                        "\u00e2\u0122",
                        "\u013f",
                        "\u010a",
                        "\u010a",
                        "BC",
                        "R",
                        " member",
                        " Chase",
                        " A",
                        "plin",
                        " said",
                        " he",
                        " believed",
                        " that",
                        " the",
                        " jury"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        15.57279491424561,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "ESPN, UniMas, UDN) before facing the 2014 FIFA World Cup<|endoftext|> can",
                    "tokens": [
                        "ESPN",
                        ",",
                        " Uni",
                        "Mas",
                        ",",
                        " U",
                        "DN",
                        ")",
                        " before",
                        " facing",
                        " the",
                        " 2014",
                        " FIFA",
                        " World",
                        " Cup",
                        "<|endoftext|>",
                        " can"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " vaccine industry, with the assistance of a sold-out government, is forcing vaccinations on",
                    "tokens": [
                        " vaccine",
                        " industry",
                        ",",
                        " with",
                        " the",
                        " assistance",
                        " of",
                        " a",
                        " sold",
                        "-",
                        "out",
                        " government",
                        ",",
                        " is",
                        " forcing",
                        " vaccinations",
                        " on"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": ", build and maintain effective student groups, advertise and rebrand conservative values, engage in",
                    "tokens": [
                        ",",
                        " build",
                        " and",
                        " maintain",
                        " effective",
                        " student",
                        " groups",
                        ",",
                        " advertise",
                        " and",
                        " re",
                        "brand",
                        " conservative",
                        " values",
                        ",",
                        " engage",
                        " in"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " political protest\u010a\u010aUpdated\u010a\u010aIt was one of the great art heists of",
                    "tokens": [
                        " political",
                        " protest",
                        "\u010a",
                        "\u010a",
                        "Updated",
                        "\u010a",
                        "\u010a",
                        "It",
                        " was",
                        " one",
                        " of",
                        " the",
                        " great",
                        " art",
                        " he",
                        "ists",
                        " of"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " that kid who felt unwanted, or the grown up who remembers how hard it was to",
                    "tokens": [
                        " that",
                        " kid",
                        " who",
                        " felt",
                        " unwanted",
                        ",",
                        " or",
                        " the",
                        " grown",
                        " up",
                        " who",
                        " remembers",
                        " how",
                        " hard",
                        " it",
                        " was",
                        " to"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 39.01195907592773
        },
        {
            "feature_index": 811,
            "gpt_predictions": [
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    1.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "references to actions that users can perform or services they can access",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "It would be good if employees were able to manage the expectations of their managers, but",
                    "max_token": " to",
                    "tokens": [
                        "It",
                        " would",
                        " be",
                        " good",
                        " if",
                        " employees",
                        " were",
                        " able",
                        " to",
                        " manage",
                        " the",
                        " expectations",
                        " of",
                        " their",
                        " managers",
                        ",",
                        " but"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        4.752609252929688,
                        6.109696388244629,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " fighting game players.\u010a\u010aBeginners can go through the tutorial mode to learn combat",
                    "max_token": " can",
                    "tokens": [
                        " fighting",
                        " game",
                        " players",
                        ".",
                        "\u010a",
                        "\u010a",
                        "Begin",
                        "ners",
                        " can",
                        " go",
                        " through",
                        " the",
                        " tutorial",
                        " mode",
                        " to",
                        " learn",
                        " combat"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        24.02888107299805,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " and appreciated the flexibility it offered. Users could join a group by sending an email to",
                    "max_token": " could",
                    "tokens": [
                        " and",
                        " appreciated",
                        " the",
                        " flexibility",
                        " it",
                        " offered",
                        ".",
                        " Users",
                        " could",
                        " join",
                        " a",
                        " group",
                        " by",
                        " sending",
                        " an",
                        " email",
                        " to"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        24.09976005554199,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " year.\u010a\u010aVideo\u010a\u010aUsers can choose from four different ways of seeing each",
                    "max_token": " can",
                    "tokens": [
                        " year",
                        ".",
                        "\u010a",
                        "\u010a",
                        "Video",
                        "\u010a",
                        "\u010a",
                        "Users",
                        " can",
                        " choose",
                        " from",
                        " four",
                        " different",
                        " ways",
                        " of",
                        " seeing",
                        " each"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        24.22654724121094,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "AND Florida Waterpark:\u010a\u010aKids can build and race boats down a winding channel",
                    "max_token": " can",
                    "tokens": [
                        "AND",
                        " Florida",
                        " Water",
                        "park",
                        ":",
                        "\u010a",
                        "\u010a",
                        "Kids",
                        " can",
                        " build",
                        " and",
                        " race",
                        " boats",
                        " down",
                        " a",
                        " winding",
                        " channel"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        24.12546730041504,
                        0,
                        0.3868209421634674,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " coat.\u010a\u010aDetective Inspector Steve Christian, from Liverpool CID, said:",
                    "tokens": [
                        " coat",
                        ".",
                        "\u010a",
                        "\u010a",
                        "Det",
                        "ective",
                        " Inspector",
                        " Steve",
                        " Christian",
                        ",",
                        " from",
                        " Liverpool",
                        " C",
                        "ID",
                        ",",
                        " said",
                        ":"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " drop out of the presidential race, according to NBC News ' Andrea Mitchell.\u010a\u010a",
                    "tokens": [
                        " drop",
                        " out",
                        " of",
                        " the",
                        " presidential",
                        " race",
                        ",",
                        " according",
                        " to",
                        " NBC",
                        " News",
                        " '",
                        " Andrea",
                        " Mitchell",
                        ".",
                        "\u010a",
                        "\u010a"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " Rumors that Amazon will soon begin accepting Bitcoin are persistent. If the retail giant does",
                    "tokens": [
                        " Rum",
                        "ors",
                        " that",
                        " Amazon",
                        " will",
                        " soon",
                        " begin",
                        " accepting",
                        " Bitcoin",
                        " are",
                        " persistent",
                        ".",
                        " If",
                        " the",
                        " retail",
                        " giant",
                        " does"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": ", and banking flows was a monthly net TIC (Treasury International Capital)<|endoftext|>",
                    "tokens": [
                        ",",
                        " and",
                        " banking",
                        " flows",
                        " was",
                        " a",
                        " monthly",
                        " net",
                        " T",
                        "IC",
                        " (",
                        "Tre",
                        "asury",
                        " International",
                        " Capital",
                        ")",
                        "<|endoftext|>"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " faces with respect to women.\u010a\u010aA CNN poll released Monday showed Obama moving into",
                    "tokens": [
                        " faces",
                        " with",
                        " respect",
                        " to",
                        " women",
                        ".",
                        "\u010a",
                        "\u010a",
                        "A",
                        " CNN",
                        " poll",
                        " released",
                        " Monday",
                        " showed",
                        " Obama",
                        " moving",
                        " into"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 26.56685829162598
        },
        {
            "feature_index": 76,
            "gpt_predictions": [
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "questions posed in a somewhat dramatic or urgent manner",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " the post-mortem, to assess what, if any, went wrong, lessons learned",
                    "max_token": ",",
                    "tokens": [
                        " the",
                        " post",
                        "-",
                        "mortem",
                        ",",
                        " to",
                        " assess",
                        " what",
                        ",",
                        " if",
                        " any",
                        ",",
                        " went",
                        " wrong",
                        ",",
                        " lessons",
                        " learned"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        34.85013580322266,
                        9.050865173339844,
                        0,
                        2.233665466308594,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " Sweetie Belle in it! ... what, does that not get you hyped?",
                    "max_token": ",",
                    "tokens": [
                        " Sweet",
                        "ie",
                        " Belle",
                        " in",
                        " it",
                        "!",
                        " ...",
                        " what",
                        ",",
                        " does",
                        " that",
                        " not",
                        " get",
                        " you",
                        " hyp",
                        "ed",
                        "?"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        30.23759651184082,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " as you board the plane. But never, at any moment, imagine that the rig",
                    "max_token": ",",
                    "tokens": [
                        " as",
                        " you",
                        " board",
                        " the",
                        " plane",
                        ".",
                        " But",
                        " never",
                        ",",
                        " at",
                        " any",
                        " moment",
                        ",",
                        " imagine",
                        " that",
                        " the",
                        " rig"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        14.63453578948975,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " was placed under criminal investigation. For what, I was never told. I was forced",
                    "max_token": ",",
                    "tokens": [
                        " was",
                        " placed",
                        " under",
                        " criminal",
                        " investigation",
                        ".",
                        " For",
                        " what",
                        ",",
                        " I",
                        " was",
                        " never",
                        " told",
                        ".",
                        " I",
                        " was",
                        " forced"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        39.92352676391602,
                        1.819446444511414,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " prepared to hear from you about just how, and in what ways you've been affected",
                    "max_token": ",",
                    "tokens": [
                        " prepared",
                        " to",
                        " hear",
                        " from",
                        " you",
                        " about",
                        " just",
                        " how",
                        ",",
                        " and",
                        " in",
                        " what",
                        " ways",
                        " you",
                        "'ve",
                        " been",
                        " affected"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        28.54708862304688,
                        10.82963085174561,
                        1.423532128334045,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "ESPN, UniMas, UDN) before facing the 2014 FIFA World Cup<|endoftext|> can",
                    "tokens": [
                        "ESPN",
                        ",",
                        " Uni",
                        "Mas",
                        ",",
                        " U",
                        "DN",
                        ")",
                        " before",
                        " facing",
                        " the",
                        " 2014",
                        " FIFA",
                        " World",
                        " Cup",
                        "<|endoftext|>",
                        " can"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " vaccine industry, with the assistance of a sold-out government, is forcing vaccinations on",
                    "tokens": [
                        " vaccine",
                        " industry",
                        ",",
                        " with",
                        " the",
                        " assistance",
                        " of",
                        " a",
                        " sold",
                        "-",
                        "out",
                        " government",
                        ",",
                        " is",
                        " forcing",
                        " vaccinations",
                        " on"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": ", build and maintain effective student groups, advertise and rebrand conservative values, engage in",
                    "tokens": [
                        ",",
                        " build",
                        " and",
                        " maintain",
                        " effective",
                        " student",
                        " groups",
                        ",",
                        " advertise",
                        " and",
                        " re",
                        "brand",
                        " conservative",
                        " values",
                        ",",
                        " engage",
                        " in"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " political protest\u010a\u010aUpdated\u010a\u010aIt was one of the great art heists of",
                    "tokens": [
                        " political",
                        " protest",
                        "\u010a",
                        "\u010a",
                        "Updated",
                        "\u010a",
                        "\u010a",
                        "It",
                        " was",
                        " one",
                        " of",
                        " the",
                        " great",
                        " art",
                        " he",
                        "ists",
                        " of"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " that kid who felt unwanted, or the grown up who remembers how hard it was to",
                    "tokens": [
                        " that",
                        " kid",
                        " who",
                        " felt",
                        " unwanted",
                        ",",
                        " or",
                        " the",
                        " grown",
                        " up",
                        " who",
                        " remembers",
                        " how",
                        " hard",
                        " it",
                        " was",
                        " to"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 51.857421875
        },
        {
            "feature_index": 636,
            "gpt_predictions": [
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    1.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "references to various conspiracy theories",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "used for rabble-rousing and conspiracy propaganda,\u00e2\u0122\u013f German Vice Chancellor Sig",
                    "max_token": " conspiracy",
                    "tokens": [
                        "used",
                        " for",
                        " rab",
                        "ble",
                        "-",
                        "rous",
                        "ing",
                        " and",
                        " conspiracy",
                        " propaganda",
                        ",",
                        "\u00e2\u0122",
                        "\u013f",
                        " German",
                        " Vice",
                        " Chancellor",
                        " Sig"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        57.56881713867188,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "<|endoftext|>The 9/11 advance-knowledge conspiracy theories center on arguments that certain institutions or",
                    "max_token": " conspiracy",
                    "tokens": [
                        "<|endoftext|>",
                        "The",
                        " 9",
                        "/",
                        "11",
                        " advance",
                        "-",
                        "knowledge",
                        " conspiracy",
                        " theories",
                        " center",
                        " on",
                        " arguments",
                        " that",
                        " certain",
                        " institutions",
                        " or"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        58.21106338500977,
                        10.78355503082275,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " time.\u010a\u010aSo is there a conspiracy? Perhaps. They would never investigate themselves",
                    "max_token": " conspiracy",
                    "tokens": [
                        " time",
                        ".",
                        "\u010a",
                        "\u010a",
                        "So",
                        " is",
                        " there",
                        " a",
                        " conspiracy",
                        "?",
                        " Perhaps",
                        ".",
                        " They",
                        " would",
                        " never",
                        " investigate",
                        " themselves"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        60.65183258056641,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "\u013f with membership numbers, and peddle conspiracy theories about Clinton and Huma Abedin being in",
                    "max_token": " conspiracy",
                    "tokens": [
                        "\u013f",
                        " with",
                        " membership",
                        " numbers",
                        ",",
                        " and",
                        " pedd",
                        "le",
                        " conspiracy",
                        " theories",
                        " about",
                        " Clinton",
                        " and",
                        " Huma",
                        " Abedin",
                        " being",
                        " in"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        64.35992431640625,
                        10.33576583862305,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "ires Vladimir Putin and Saddam Hussein, a conspiracy theorist who accuses George W. Bush of",
                    "max_token": " conspiracy",
                    "tokens": [
                        "ires",
                        " Vladimir",
                        " Putin",
                        " and",
                        " Saddam",
                        " Hussein",
                        ",",
                        " a",
                        " conspiracy",
                        " theorist",
                        " who",
                        " accuses",
                        " George",
                        " W",
                        ".",
                        " Bush",
                        " of"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        66.55741882324219,
                        4.237740039825439,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " quick lead only to have to bow out before the end of the game because they went",
                    "tokens": [
                        " quick",
                        " lead",
                        " only",
                        " to",
                        " have",
                        " to",
                        " bow",
                        " out",
                        " before",
                        " the",
                        " end",
                        " of",
                        " the",
                        " game",
                        " because",
                        " they",
                        " went"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "\u010aAldrin was a drug user before, Castillo admitted, but he had",
                    "tokens": [
                        "\u010a",
                        "A",
                        "ld",
                        "rin",
                        " was",
                        " a",
                        " drug",
                        " user",
                        " before",
                        ",",
                        " Cast",
                        "illo",
                        " admitted",
                        ",",
                        " but",
                        " he",
                        " had"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " height at some international summit.\u010a\u010aOr is it because of these deeply controversial statements",
                    "tokens": [
                        " height",
                        " at",
                        " some",
                        " international",
                        " summit",
                        ".",
                        "\u010a",
                        "\u010a",
                        "Or",
                        " is",
                        " it",
                        " because",
                        " of",
                        " these",
                        " deeply",
                        " controversial",
                        " statements"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " years after he was jailed for espionage, before his release in 1969.\u010a\u010aHe",
                    "tokens": [
                        " years",
                        " after",
                        " he",
                        " was",
                        " jailed",
                        " for",
                        " espionage",
                        ",",
                        " before",
                        " his",
                        " release",
                        " in",
                        " 1969",
                        ".",
                        "\u010a",
                        "\u010a",
                        "He"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " faces with respect to women.\u010a\u010aA CNN poll released Monday showed Obama moving into",
                    "tokens": [
                        " faces",
                        " with",
                        " respect",
                        " to",
                        " women",
                        ".",
                        "\u010a",
                        "\u010a",
                        "A",
                        " CNN",
                        " poll",
                        " released",
                        " Monday",
                        " showed",
                        " Obama",
                        " moving",
                        " into"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 69.28605651855469
        },
        {
            "feature_index": 973,
            "gpt_predictions": [
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "references to a large quantity or group of people",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " organizing. And that is why a lot of people who can afford to spend a lot",
                    "max_token": " of",
                    "tokens": [
                        " organizing",
                        ".",
                        " And",
                        " that",
                        " is",
                        " why",
                        " a",
                        " lot",
                        " of",
                        " people",
                        " who",
                        " can",
                        " afford",
                        " to",
                        " spend",
                        " a",
                        " lot"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        13.26516914367676,
                        22.09026527404785,
                        3.3700110912323,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " privilege. The commenter said that a lot of the words seemed like SAT words. In",
                    "max_token": " of",
                    "tokens": [
                        " privilege",
                        ".",
                        " The",
                        " commenter",
                        " said",
                        " that",
                        " a",
                        " lot",
                        " of",
                        " the",
                        " words",
                        " seemed",
                        " like",
                        " SAT",
                        " words",
                        ".",
                        " In"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        11.16046714782715,
                        19.28182029724121,
                        5.316037178039551,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " somewhat re-post. But a lot of things I didn't cover actually took quite",
                    "max_token": " of",
                    "tokens": [
                        " somewhat",
                        " re",
                        "-",
                        "post",
                        ".",
                        " But",
                        " a",
                        " lot",
                        " of",
                        " things",
                        " I",
                        " didn",
                        "'t",
                        " cover",
                        " actually",
                        " took",
                        " quite"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        6.11370325088501,
                        18.33374786376953,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": ", because to be honest, a lot of the time it won\u00e2\u0122\u013bt be",
                    "max_token": " of",
                    "tokens": [
                        ",",
                        " because",
                        " to",
                        " be",
                        " honest",
                        ",",
                        " a",
                        " lot",
                        " of",
                        " the",
                        " time",
                        " it",
                        " won",
                        "\u00e2\u0122",
                        "\u013b",
                        "t",
                        " be"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        3.220046997070312,
                        0.8211557269096375,
                        0,
                        10.03084182739258,
                        24.05379295349121,
                        8.149053573608398,
                        2.220973968505859,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "-old-boys club. A lot of this has to do with control, power",
                    "max_token": " of",
                    "tokens": [
                        "-",
                        "old",
                        "-",
                        "boys",
                        " club",
                        ".",
                        " A",
                        " lot",
                        " of",
                        " this",
                        " has",
                        " to",
                        " do",
                        " with",
                        " control",
                        ",",
                        " power"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        7.641307830810547,
                        20.1483154296875,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " coat.\u010a\u010aDetective Inspector Steve Christian, from Liverpool CID, said:",
                    "tokens": [
                        " coat",
                        ".",
                        "\u010a",
                        "\u010a",
                        "Det",
                        "ective",
                        " Inspector",
                        " Steve",
                        " Christian",
                        ",",
                        " from",
                        " Liverpool",
                        " C",
                        "ID",
                        ",",
                        " said",
                        ":"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " drop out of the presidential race, according to NBC News ' Andrea Mitchell.\u010a\u010a",
                    "tokens": [
                        " drop",
                        " out",
                        " of",
                        " the",
                        " presidential",
                        " race",
                        ",",
                        " according",
                        " to",
                        " NBC",
                        " News",
                        " '",
                        " Andrea",
                        " Mitchell",
                        ".",
                        "\u010a",
                        "\u010a"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " Rumors that Amazon will soon begin accepting Bitcoin are persistent. If the retail giant does",
                    "tokens": [
                        " Rum",
                        "ors",
                        " that",
                        " Amazon",
                        " will",
                        " soon",
                        " begin",
                        " accepting",
                        " Bitcoin",
                        " are",
                        " persistent",
                        ".",
                        " If",
                        " the",
                        " retail",
                        " giant",
                        " does"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": ", and banking flows was a monthly net TIC (Treasury International Capital)<|endoftext|>",
                    "tokens": [
                        ",",
                        " and",
                        " banking",
                        " flows",
                        " was",
                        " a",
                        " monthly",
                        " net",
                        " T",
                        "IC",
                        " (",
                        "Tre",
                        "asury",
                        " International",
                        " Capital",
                        ")",
                        "<|endoftext|>"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " faces with respect to women.\u010a\u010aA CNN poll released Monday showed Obama moving into",
                    "tokens": [
                        " faces",
                        " with",
                        " respect",
                        " to",
                        " women",
                        ".",
                        "\u010a",
                        "\u010a",
                        "A",
                        " CNN",
                        " poll",
                        " released",
                        " Monday",
                        " showed",
                        " Obama",
                        " moving",
                        " into"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 27.30073165893555
        },
        {
            "feature_index": 938,
            "gpt_predictions": [
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    0,
                    1.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "periods at the end of sentences",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "let) January 17, 2017\u010a\u010a.@realDonaldTrump It'll be ok. Just",
                    "max_token": ".",
                    "tokens": [
                        "let",
                        ")",
                        " January",
                        " 17",
                        ",",
                        " 2017",
                        "\u010a",
                        "\u010a",
                        ".",
                        "@",
                        "realDonaldTrump",
                        " It",
                        "'ll",
                        " be",
                        " ok",
                        ".",
                        " Just"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        36.14587020874023,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": ". between Spear and Third.\u010a\u010a.<|endoftext|>The development cycle of the upcoming T",
                    "max_token": ".",
                    "tokens": [
                        ".",
                        " between",
                        " Spear",
                        " and",
                        " Third",
                        ".",
                        "\u010a",
                        "\u010a",
                        ".",
                        "<|endoftext|>",
                        "The",
                        " development",
                        " cycle",
                        " of",
                        " the",
                        " upcoming",
                        " T"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        45.71392059326172,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "'t get away with it.\u010a\u010a.\u010a\u010adigby 10/13/",
                    "max_token": ".",
                    "tokens": [
                        "'t",
                        " get",
                        " away",
                        " with",
                        " it",
                        ".",
                        "\u010a",
                        "\u010a",
                        ".",
                        "\u010a",
                        "\u010a",
                        "dig",
                        "by",
                        " 10",
                        "/",
                        "13",
                        "/"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        49.267578125,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " find the complete release notes >here<.\u010a\u010aWe hope that you like our",
                    "max_token": ".",
                    "tokens": [
                        " find",
                        " the",
                        " complete",
                        " release",
                        " notes",
                        " >",
                        "here",
                        "<",
                        ".",
                        "\u010a",
                        "\u010a",
                        "We",
                        " hope",
                        " that",
                        " you",
                        " like",
                        " our"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        18.46417236328125,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " short-term boosts after adding \u00e2\u0122\u013e.com\u00e2\u0122\u013f to their company name.",
                    "max_token": ".",
                    "tokens": [
                        " short",
                        "-",
                        "term",
                        " boosts",
                        " after",
                        " adding",
                        " \u00e2\u0122",
                        "\u013e",
                        ".",
                        "com",
                        "\u00e2\u0122",
                        "\u013f",
                        " to",
                        " their",
                        " company",
                        " name",
                        "."
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        14.85541820526123,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " coat.\u010a\u010aDetective Inspector Steve Christian, from Liverpool CID, said:",
                    "tokens": [
                        " coat",
                        ".",
                        "\u010a",
                        "\u010a",
                        "Det",
                        "ective",
                        " Inspector",
                        " Steve",
                        " Christian",
                        ",",
                        " from",
                        " Liverpool",
                        " C",
                        "ID",
                        ",",
                        " said",
                        ":"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " drop out of the presidential race, according to NBC News ' Andrea Mitchell.\u010a\u010a",
                    "tokens": [
                        " drop",
                        " out",
                        " of",
                        " the",
                        " presidential",
                        " race",
                        ",",
                        " according",
                        " to",
                        " NBC",
                        " News",
                        " '",
                        " Andrea",
                        " Mitchell",
                        ".",
                        "\u010a",
                        "\u010a"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " Rumors that Amazon will soon begin accepting Bitcoin are persistent. If the retail giant does",
                    "tokens": [
                        " Rum",
                        "ors",
                        " that",
                        " Amazon",
                        " will",
                        " soon",
                        " begin",
                        " accepting",
                        " Bitcoin",
                        " are",
                        " persistent",
                        ".",
                        " If",
                        " the",
                        " retail",
                        " giant",
                        " does"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": ", and banking flows was a monthly net TIC (Treasury International Capital)<|endoftext|>",
                    "tokens": [
                        ",",
                        " and",
                        " banking",
                        " flows",
                        " was",
                        " a",
                        " monthly",
                        " net",
                        " T",
                        "IC",
                        " (",
                        "Tre",
                        "asury",
                        " International",
                        " Capital",
                        ")",
                        "<|endoftext|>"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " faces with respect to women.\u010a\u010aA CNN poll released Monday showed Obama moving into",
                    "tokens": [
                        " faces",
                        " with",
                        " respect",
                        " to",
                        " women",
                        ".",
                        "\u010a",
                        "\u010a",
                        "A",
                        " CNN",
                        " poll",
                        " released",
                        " Monday",
                        " showed",
                        " Obama",
                        " moving",
                        " into"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 63.76441955566406
        },
        {
            "feature_index": 899,
            "gpt_predictions": [
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "language related to appreciation and support, especially in the context of online participation and activism",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 9,
                    "sentence_string": " the performance at Sakura-Con we saw so many smiling faces. We just want to",
                    "max_token": " many",
                    "tokens": [
                        " the",
                        " performance",
                        " at",
                        " Sakura",
                        "-",
                        "Con",
                        " we",
                        " saw",
                        " so",
                        " many",
                        " smiling",
                        " faces",
                        ".",
                        " We",
                        " just",
                        " want",
                        " to"
                    ],
                    "values": [
                        2.960752964019775,
                        2.028061389923096,
                        0,
                        0,
                        0,
                        0,
                        4.063567161560059,
                        4.915735244750977,
                        5.021366596221924,
                        6.871029853820801,
                        0,
                        0.5174748301506042,
                        5.93303918838501,
                        5.116055011749268,
                        2.858282089233398,
                        1.392026662826538,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 3,
                    "sentence_string": "'ve received so far from friends and supporters, we're confident that means things will start",
                    "max_token": " far",
                    "tokens": [
                        "'ve",
                        " received",
                        " so",
                        " far",
                        " from",
                        " friends",
                        " and",
                        " supporters",
                        ",",
                        " we",
                        "'re",
                        " confident",
                        " that",
                        " means",
                        " things",
                        " will",
                        " start"
                    ],
                    "values": [
                        4.610176563262939,
                        4.136774063110352,
                        5.386856555938721,
                        6.620738983154297,
                        2.502304792404175,
                        0.7699645757675171,
                        4.149416446685791,
                        4.314229965209961,
                        6.555548667907715,
                        4.779500007629395,
                        4.070214748382568,
                        3.162713289260864,
                        3.180546760559082,
                        1.707332372665405,
                        1.849388599395752,
                        1.332040667533875,
                        2.259527683258057
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 11,
                    "sentence_string": " two, so people are getting the message and supporting it.\u010a\u010aCongrats to",
                    "max_token": ".",
                    "tokens": [
                        " two",
                        ",",
                        " so",
                        " people",
                        " are",
                        " getting",
                        " the",
                        " message",
                        " and",
                        " supporting",
                        " it",
                        ".",
                        "\u010a",
                        "\u010a",
                        "Cong",
                        "rats",
                        " to"
                    ],
                    "values": [
                        0,
                        2.249062538146973,
                        0,
                        0.009178653359413147,
                        0.9415429830551147,
                        1.111906409263611,
                        0,
                        1.763045430183411,
                        4.136889457702637,
                        1.404014468193054,
                        4.413735389709473,
                        5.595401763916016,
                        0,
                        2.711507320404053,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": ",\u00e2\u0122\u013f Hassan said. \u00e2\u0122\u013eHe is very supportive and good with the kids",
                    "max_token": "He",
                    "tokens": [
                        ",",
                        "\u00e2\u0122",
                        "\u013f",
                        " Hassan",
                        " said",
                        ".",
                        " \u00e2\u0122",
                        "\u013e",
                        "He",
                        " is",
                        " very",
                        " supportive",
                        " and",
                        " good",
                        " with",
                        " the",
                        " kids"
                    ],
                    "values": [
                        4.803887844085693,
                        0,
                        0,
                        0,
                        0,
                        1.107071280479431,
                        0,
                        0.5438388586044312,
                        7.392349720001221,
                        6.369600772857666,
                        3.860294580459595,
                        0,
                        2.932373762130737,
                        0.2625709176063538,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 12,
                    "sentence_string": " We're so amazed by all the contributions we've received so far from friends and supporters",
                    "max_token": " far",
                    "tokens": [
                        " We",
                        "'re",
                        " so",
                        " amazed",
                        " by",
                        " all",
                        " the",
                        " contributions",
                        " we",
                        "'ve",
                        " received",
                        " so",
                        " far",
                        " from",
                        " friends",
                        " and",
                        " supporters"
                    ],
                    "values": [
                        0,
                        1.024486184120178,
                        3.371728897094727,
                        1.803508758544922,
                        2.991062879562378,
                        4.760839462280273,
                        4.573484420776367,
                        4.258322715759277,
                        6.615143775939941,
                        4.610176563262939,
                        4.136774063110352,
                        5.386856555938721,
                        6.620738983154297,
                        2.502304792404175,
                        0.7699645757675171,
                        4.149416446685791,
                        4.314229965209961
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " coat.\u010a\u010aDetective Inspector Steve Christian, from Liverpool CID, said:",
                    "tokens": [
                        " coat",
                        ".",
                        "\u010a",
                        "\u010a",
                        "Det",
                        "ective",
                        " Inspector",
                        " Steve",
                        " Christian",
                        ",",
                        " from",
                        " Liverpool",
                        " C",
                        "ID",
                        ",",
                        " said",
                        ":"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " drop out of the presidential race, according to NBC News ' Andrea Mitchell.\u010a\u010a",
                    "tokens": [
                        " drop",
                        " out",
                        " of",
                        " the",
                        " presidential",
                        " race",
                        ",",
                        " according",
                        " to",
                        " NBC",
                        " News",
                        " '",
                        " Andrea",
                        " Mitchell",
                        ".",
                        "\u010a",
                        "\u010a"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " Rumors that Amazon will soon begin accepting Bitcoin are persistent. If the retail giant does",
                    "tokens": [
                        " Rum",
                        "ors",
                        " that",
                        " Amazon",
                        " will",
                        " soon",
                        " begin",
                        " accepting",
                        " Bitcoin",
                        " are",
                        " persistent",
                        ".",
                        " If",
                        " the",
                        " retail",
                        " giant",
                        " does"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": ", and banking flows was a monthly net TIC (Treasury International Capital)<|endoftext|>",
                    "tokens": [
                        ",",
                        " and",
                        " banking",
                        " flows",
                        " was",
                        " a",
                        " monthly",
                        " net",
                        " T",
                        "IC",
                        " (",
                        "Tre",
                        "asury",
                        " International",
                        " Capital",
                        ")",
                        "<|endoftext|>"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " faces with respect to women.\u010a\u010aA CNN poll released Monday showed Obama moving into",
                    "tokens": [
                        " faces",
                        " with",
                        " respect",
                        " to",
                        " women",
                        ".",
                        "\u010a",
                        "\u010a",
                        "A",
                        " CNN",
                        " poll",
                        " released",
                        " Monday",
                        " showed",
                        " Obama",
                        " moving",
                        " into"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 10.72458267211914
        },
        {
            "feature_index": 280,
            "gpt_predictions": [
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "various URLs and related alphanumeric patterns",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 5,
                    "sentence_string": ".com/h14oXOaC74 \u00e2\u0122\u0136 David Leavitt (@",
                    "max_token": "o",
                    "tokens": [
                        ".",
                        "com",
                        "/",
                        "h",
                        "14",
                        "o",
                        "X",
                        "O",
                        "a",
                        "C",
                        "74",
                        " \u00e2\u0122\u0136",
                        " David",
                        " Le",
                        "av",
                        "itt",
                        " (@"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        21.65297698974609,
                        0,
                        9.01735782623291,
                        21.53682899475098,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 1,
                    "sentence_string": "OoBdQyLcPoBuHnAtG..oBr",
                    "max_token": "o",
                    "tokens": [
                        "O",
                        "o",
                        "B",
                        "d",
                        "Q",
                        "y",
                        "L",
                        "c",
                        "Po",
                        "Bu",
                        "H",
                        "n",
                        "At",
                        "G",
                        "..",
                        "o",
                        "Br"
                    ],
                    "values": [
                        2.812402963638306,
                        18.52156829833984,
                        0,
                        0,
                        0,
                        2.273581981658936,
                        0,
                        0,
                        15.22518634796143,
                        11.81688690185547,
                        0,
                        0,
                        0,
                        0,
                        0,
                        14.11349487304688,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "t.co/RprxdEeD3M \u00e2\u0122\u0136 White Castle (@White",
                    "max_token": "e",
                    "tokens": [
                        "t",
                        ".",
                        "co",
                        "/",
                        "R",
                        "pr",
                        "xd",
                        "E",
                        "e",
                        "D",
                        "3",
                        "M",
                        " \u00e2\u0122\u0136",
                        " White",
                        " Castle",
                        " (@",
                        "White"
                    ],
                    "values": [
                        0,
                        0,
                        0.8540973663330078,
                        0,
                        0,
                        0,
                        0,
                        8.118595123291016,
                        17.67161178588867,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " https://t.co/O1iZ2R9cOU \u00e2\u0122\u0136 Eric",
                    "max_token": "i",
                    "tokens": [
                        " https",
                        "://",
                        "t",
                        ".",
                        "co",
                        "/",
                        "O",
                        "1",
                        "i",
                        "Z",
                        "2",
                        "R",
                        "9",
                        "c",
                        "OU",
                        " \u00e2\u0122\u0136",
                        " Eric"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        5.870588302612305,
                        0,
                        22.76553344726562,
                        0,
                        0,
                        0,
                        0,
                        3.692944526672363,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "/iGfE8rAoAu \u00e2\u0122\u0136 Barack Obama (@Barack",
                    "max_token": "o",
                    "tokens": [
                        "/",
                        "i",
                        "G",
                        "f",
                        "E",
                        "8",
                        "r",
                        "A",
                        "o",
                        "A",
                        "u",
                        " \u00e2\u0122\u0136",
                        " Barack",
                        " Obama",
                        " (@",
                        "Bar",
                        "ack"
                    ],
                    "values": [
                        0,
                        17.03671836853027,
                        0,
                        0.1178556978702545,
                        8.159634590148926,
                        0,
                        0,
                        10.98245525360107,
                        20.22340965270996,
                        9.85993766784668,
                        14.12033843994141,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " Trump and Hillary in the first debate, before either said a word, Trump made what",
                    "tokens": [
                        " Trump",
                        " and",
                        " Hillary",
                        " in",
                        " the",
                        " first",
                        " debate",
                        ",",
                        " before",
                        " either",
                        " said",
                        " a",
                        " word",
                        ",",
                        " Trump",
                        " made",
                        " what"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " that kid who felt unwanted, or the grown up who remembers how hard it was to",
                    "tokens": [
                        " that",
                        " kid",
                        " who",
                        " felt",
                        " unwanted",
                        ",",
                        " or",
                        " the",
                        " grown",
                        " up",
                        " who",
                        " remembers",
                        " how",
                        " hard",
                        " it",
                        " was",
                        " to"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "\u010aAldrin was a drug user before, Castillo admitted, but he had",
                    "tokens": [
                        "\u010a",
                        "A",
                        "ld",
                        "rin",
                        " was",
                        " a",
                        " drug",
                        " user",
                        " before",
                        ",",
                        " Cast",
                        "illo",
                        " admitted",
                        ",",
                        " but",
                        " he",
                        " had"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " who genuinely seem not to believe reality. Or Democrats, cowed into silence on issues",
                    "tokens": [
                        " who",
                        " genuinely",
                        " seem",
                        " not",
                        " to",
                        " believe",
                        " reality",
                        ".",
                        " Or",
                        " Democrats",
                        ",",
                        " c",
                        "owed",
                        " into",
                        " silence",
                        " on",
                        " issues"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " the incident, saying the man threatened him before leaving.\u010a\u010a\u00e2\u0122\u013eI think",
                    "tokens": [
                        " the",
                        " incident",
                        ",",
                        " saying",
                        " the",
                        " man",
                        " threatened",
                        " him",
                        " before",
                        " leaving",
                        ".",
                        "\u010a",
                        "\u010a",
                        "\u00e2\u0122",
                        "\u013e",
                        "I",
                        " think"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 26.07519912719727
        },
        {
            "feature_index": 883,
            "gpt_predictions": [
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "mentions of statistical information and survey results",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 12,
                    "sentence_string": " very unscientific poll. Based on my network on LinkedIn, I have 308,",
                    "max_token": ",",
                    "tokens": [
                        " very",
                        " uns",
                        "cient",
                        "ific",
                        " poll",
                        ".",
                        " Based",
                        " on",
                        " my",
                        " network",
                        " on",
                        " LinkedIn",
                        ",",
                        " I",
                        " have",
                        " 308",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0.3283667862415314,
                        0,
                        3.149397850036621,
                        3.172062635421753,
                        5.639120101928711,
                        2.382350444793701,
                        2.204509735107422,
                        2.452968835830688,
                        2.798709392547607,
                        6.353034496307373,
                        3.900843143463135,
                        4.332531929016113,
                        2.647775650024414,
                        2.166211128234863
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 7,
                    "sentence_string": " transit system ranked 3rd in terms of criminal activity. Calgary's transit police reported the",
                    "max_token": " of",
                    "tokens": [
                        " transit",
                        " system",
                        " ranked",
                        " 3",
                        "rd",
                        " in",
                        " terms",
                        " of",
                        " criminal",
                        " activity",
                        ".",
                        " Calgary",
                        "'s",
                        " transit",
                        " police",
                        " reported",
                        " the"
                    ],
                    "values": [
                        0,
                        3.19944953918457,
                        6.001343727111816,
                        1.668833374977112,
                        4.401885032653809,
                        6.580920696258545,
                        7.267714977264404,
                        8.343013763427734,
                        5.31727123260498,
                        6.644227981567383,
                        1.884563565254211,
                        1.444286465644836,
                        2.363841533660889,
                        0,
                        0.7942314743995667,
                        5.492713928222656,
                        4.035117626190186
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 9,
                    "sentence_string": "\u010aDuring<|endoftext|> that occurred between 2012 and 2014 not be repeated.\u010a\u010a\u00e2\u0122\u013e",
                    "max_token": " not",
                    "tokens": [
                        "\u010a",
                        "During",
                        "<|endoftext|>",
                        " that",
                        " occurred",
                        " between",
                        " 2012",
                        " and",
                        " 2014",
                        " not",
                        " be",
                        " repeated",
                        ".",
                        "\u010a",
                        "\u010a",
                        "\u00e2\u0122",
                        "\u013e"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        1.218354344367981,
                        2.326884031295776,
                        2.627386569976807,
                        0,
                        0,
                        0,
                        0.4918507039546967,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": ", WME led the way among agencies with 18 clients on the list, followed by",
                    "max_token": " with",
                    "tokens": [
                        ",",
                        " W",
                        "ME",
                        " led",
                        " the",
                        " way",
                        " among",
                        " agencies",
                        " with",
                        " 18",
                        " clients",
                        " on",
                        " the",
                        " list",
                        ",",
                        " followed",
                        " by"
                    ],
                    "values": [
                        1.220247030258179,
                        0,
                        0,
                        4.689361572265625,
                        5.013821125030518,
                        4.22381067276001,
                        4.752572059631348,
                        4.259562969207764,
                        9.252835273742676,
                        5.06585168838501,
                        3.755340814590454,
                        3.62486457824707,
                        3.198548793792725,
                        1.858097553253174,
                        3.154991149902344,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 10,
                    "sentence_string": " program before prescribing painkillers, the state saw a 75 percent drop in patients who were",
                    "max_token": " 75",
                    "tokens": [
                        " program",
                        " before",
                        " prescribing",
                        " pain",
                        "killers",
                        ",",
                        " the",
                        " state",
                        " saw",
                        " a",
                        " 75",
                        " percent",
                        " drop",
                        " in",
                        " patients",
                        " who",
                        " were"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0.4230591058731079,
                        2.493961095809937,
                        0.7579594254493713,
                        8.638158798217773,
                        8.21623420715332,
                        8.919684410095215,
                        8.173508644104004,
                        6.314700603485107,
                        5.813731670379639,
                        1.694730877876282,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " coat.\u010a\u010aDetective Inspector Steve Christian, from Liverpool CID, said:",
                    "tokens": [
                        " coat",
                        ".",
                        "\u010a",
                        "\u010a",
                        "Det",
                        "ective",
                        " Inspector",
                        " Steve",
                        " Christian",
                        ",",
                        " from",
                        " Liverpool",
                        " C",
                        "ID",
                        ",",
                        " said",
                        ":"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " drop out of the presidential race, according to NBC News ' Andrea Mitchell.\u010a\u010a",
                    "tokens": [
                        " drop",
                        " out",
                        " of",
                        " the",
                        " presidential",
                        " race",
                        ",",
                        " according",
                        " to",
                        " NBC",
                        " News",
                        " '",
                        " Andrea",
                        " Mitchell",
                        ".",
                        "\u010a",
                        "\u010a"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " Rumors that Amazon will soon begin accepting Bitcoin are persistent. If the retail giant does",
                    "tokens": [
                        " Rum",
                        "ors",
                        " that",
                        " Amazon",
                        " will",
                        " soon",
                        " begin",
                        " accepting",
                        " Bitcoin",
                        " are",
                        " persistent",
                        ".",
                        " If",
                        " the",
                        " retail",
                        " giant",
                        " does"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": ", and banking flows was a monthly net TIC (Treasury International Capital)<|endoftext|>",
                    "tokens": [
                        ",",
                        " and",
                        " banking",
                        " flows",
                        " was",
                        " a",
                        " monthly",
                        " net",
                        " T",
                        "IC",
                        " (",
                        "Tre",
                        "asury",
                        " International",
                        " Capital",
                        ")",
                        "<|endoftext|>"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " faces with respect to women.\u010a\u010aA CNN poll released Monday showed Obama moving into",
                    "tokens": [
                        " faces",
                        " with",
                        " respect",
                        " to",
                        " women",
                        ".",
                        "\u010a",
                        "\u010a",
                        "A",
                        " CNN",
                        " poll",
                        " released",
                        " Monday",
                        " showed",
                        " Obama",
                        " moving",
                        " into"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 11.25397968292236
        },
        {
            "feature_index": 761,
            "gpt_predictions": [
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    1.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "words related to voting or expressing a choice",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " Beijing, said in a statement following the vote that it hoped \u00e2\u0122\u013ecross-stra",
                    "max_token": " vote",
                    "tokens": [
                        " Beijing",
                        ",",
                        " said",
                        " in",
                        " a",
                        " statement",
                        " following",
                        " the",
                        " vote",
                        " that",
                        " it",
                        " hoped",
                        " \u00e2\u0122",
                        "\u013e",
                        "cross",
                        "-",
                        "stra"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        13.42282390594482,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " the Federal<|endoftext|> who flatly threatened to vote against the health care bill if it included",
                    "max_token": " vote",
                    "tokens": [
                        " the",
                        " Federal",
                        "<|endoftext|>",
                        " who",
                        " flat",
                        "ly",
                        " threatened",
                        " to",
                        " vote",
                        " against",
                        " the",
                        " health",
                        " care",
                        " bill",
                        " if",
                        " it",
                        " included"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        57.77485275268555,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " the Associated Press earlier this month that he voted for Cochran in the primary and that",
                    "max_token": " voted",
                    "tokens": [
                        " the",
                        " Associated",
                        " Press",
                        " earlier",
                        " this",
                        " month",
                        " that",
                        " he",
                        " voted",
                        " for",
                        " Coch",
                        "ran",
                        " in",
                        " the",
                        " primary",
                        " and",
                        " that"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        58.20137023925781,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " Lewis. Lewis has since said he will vote in favour of the bill on second reading",
                    "max_token": " vote",
                    "tokens": [
                        " Lewis",
                        ".",
                        " Lewis",
                        " has",
                        " since",
                        " said",
                        " he",
                        " will",
                        " vote",
                        " in",
                        " favour",
                        " of",
                        " the",
                        " bill",
                        " on",
                        " second",
                        " reading"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        58.69745254516602,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "pp, R-Bend, who voted no. \u00e2\u0122\u013eWe have not taken",
                    "max_token": " voted",
                    "tokens": [
                        "pp",
                        ",",
                        " R",
                        "-",
                        "B",
                        "end",
                        ",",
                        " who",
                        " voted",
                        " no",
                        ".",
                        " \u00e2\u0122",
                        "\u013e",
                        "We",
                        " have",
                        " not",
                        " taken"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        58.5941276550293,
                        1.305634498596191,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " coat.\u010a\u010aDetective Inspector Steve Christian, from Liverpool CID, said:",
                    "tokens": [
                        " coat",
                        ".",
                        "\u010a",
                        "\u010a",
                        "Det",
                        "ective",
                        " Inspector",
                        " Steve",
                        " Christian",
                        ",",
                        " from",
                        " Liverpool",
                        " C",
                        "ID",
                        ",",
                        " said",
                        ":"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " drop out of the presidential race, according to NBC News ' Andrea Mitchell.\u010a\u010a",
                    "tokens": [
                        " drop",
                        " out",
                        " of",
                        " the",
                        " presidential",
                        " race",
                        ",",
                        " according",
                        " to",
                        " NBC",
                        " News",
                        " '",
                        " Andrea",
                        " Mitchell",
                        ".",
                        "\u010a",
                        "\u010a"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " documenting the protective effect of male circumcision on HIV infection in young adults pose significant challenges to",
                    "tokens": [
                        " documenting",
                        " the",
                        " protective",
                        " effect",
                        " of",
                        " male",
                        " circumcision",
                        " on",
                        " HIV",
                        " infection",
                        " in",
                        " young",
                        " adults",
                        " pose",
                        " significant",
                        " challenges",
                        " to"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " the virus is no laughing matter, a viral video features a Chicago pup named Herbert who",
                    "tokens": [
                        " the",
                        " virus",
                        " is",
                        " no",
                        " laughing",
                        " matter",
                        ",",
                        " a",
                        " viral",
                        " video",
                        " features",
                        " a",
                        " Chicago",
                        " pup",
                        " named",
                        " Herbert",
                        " who"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " faces with respect to women.\u010a\u010aA CNN poll released Monday showed Obama moving into",
                    "tokens": [
                        " faces",
                        " with",
                        " respect",
                        " to",
                        " women",
                        ".",
                        "\u010a",
                        "\u010a",
                        "A",
                        " CNN",
                        " poll",
                        " released",
                        " Monday",
                        " showed",
                        " Obama",
                        " moving",
                        " into"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 63.60680770874023
        },
        {
            "feature_index": 319,
            "gpt_predictions": [
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "mentions of the Syrian President, Bashar al-Assad",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " into 10 villages nearby where members of President Bashar al-Assad\u00e2\u0122\u013bs Alaw",
                    "max_token": " Bashar",
                    "tokens": [
                        " into",
                        " 10",
                        " villages",
                        " nearby",
                        " where",
                        " members",
                        " of",
                        " President",
                        " Bashar",
                        " al",
                        "-",
                        "Assad",
                        "\u00e2\u0122",
                        "\u013b",
                        "s",
                        " Al",
                        "aw"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        75.3973159790039,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " the perpetuators were the government of Bashar al-Assad \u00e2\u0122\u0136 perhaps with Russian assistance",
                    "max_token": " Bashar",
                    "tokens": [
                        " the",
                        " perpet",
                        "u",
                        "ators",
                        " were",
                        " the",
                        " government",
                        " of",
                        " Bashar",
                        " al",
                        "-",
                        "Assad",
                        " \u00e2\u0122\u0136",
                        " perhaps",
                        " with",
                        " Russian",
                        " assistance"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        78.74044036865234,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " government.\u010a\u010aTrump blamed Syrian President Bashar Assad for the attack and called on the",
                    "max_token": " Bashar",
                    "tokens": [
                        " government",
                        ".",
                        "\u010a",
                        "\u010a",
                        "Trump",
                        " blamed",
                        " Syrian",
                        " President",
                        " Bashar",
                        " Assad",
                        " for",
                        " the",
                        " attack",
                        " and",
                        " called",
                        " on",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        81.32562255859375,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": ",\" he said, \"some persons like Hafiz Saeed in our country who do",
                    "max_token": " Haf",
                    "tokens": [
                        ",\"",
                        " he",
                        " said",
                        ",",
                        " \"",
                        "some",
                        " persons",
                        " like",
                        " Haf",
                        "iz",
                        " Sa",
                        "eed",
                        " in",
                        " our",
                        " country",
                        " who",
                        " do"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        20.47746276855469,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " the U.S. Marines who killed Osama bin Laden, produced by A+<|endoftext|>",
                    "max_token": " Osama",
                    "tokens": [
                        " the",
                        " U",
                        ".",
                        "S",
                        ".",
                        " Marines",
                        " who",
                        " killed",
                        " Osama",
                        " bin",
                        " Laden",
                        ",",
                        " produced",
                        " by",
                        " A",
                        "+",
                        "<|endoftext|>"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        14.29779815673828,
                        0.3201088309288025,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "\u00e2\u0122\u013bThe<|endoftext|> potential impact surveys and mitigation and restoration plans\u00e2\u0122\u013b\u00e2\u0122\u013b are",
                    "tokens": [
                        "\u00e2\u0122",
                        "\u013b",
                        "The",
                        "<|endoftext|>",
                        " potential",
                        " impact",
                        " surveys",
                        " and",
                        " mitigation",
                        " and",
                        " restoration",
                        " plans",
                        "\u00e2\u0122",
                        "\u013b",
                        "\u00e2\u0122",
                        "\u013b",
                        " are"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " that kid who felt unwanted, or the grown up who remembers how hard it was to",
                    "tokens": [
                        " that",
                        " kid",
                        " who",
                        " felt",
                        " unwanted",
                        ",",
                        " or",
                        " the",
                        " grown",
                        " up",
                        " who",
                        " remembers",
                        " how",
                        " hard",
                        " it",
                        " was",
                        " to"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " Trump and Hillary in the first debate, before either said a word, Trump made what",
                    "tokens": [
                        " Trump",
                        " and",
                        " Hillary",
                        " in",
                        " the",
                        " first",
                        " debate",
                        ",",
                        " before",
                        " either",
                        " said",
                        " a",
                        " word",
                        ",",
                        " Trump",
                        " made",
                        " what"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "3 billion) installing everything from traffic-management technologies to smart electricity grids.\u010a\u010a",
                    "tokens": [
                        "3",
                        " billion",
                        ")",
                        " installing",
                        " everything",
                        " from",
                        " traffic",
                        "-",
                        "management",
                        " technologies",
                        " to",
                        " smart",
                        " electricity",
                        " grids",
                        ".",
                        "\u010a",
                        "\u010a"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " city staff (Fire, Police, Crime Prevention, and more)\u010a\u010aOption to",
                    "tokens": [
                        " city",
                        " staff",
                        " (",
                        "Fire",
                        ",",
                        " Police",
                        ",",
                        " Crime",
                        " Prevention",
                        ",",
                        " and",
                        " more",
                        ")",
                        "\u010a",
                        "\u010a",
                        "Option",
                        " to"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 87.25408172607422
        },
        {
            "feature_index": 549,
            "gpt_predictions": [
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    1.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "dates or time-related information",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " about the position we have taken.\"\u010a\u010a'Challenge inequality'\u010a\u010aThe",
                    "max_token": "\u010a",
                    "tokens": [
                        " about",
                        " the",
                        " position",
                        " we",
                        " have",
                        " taken",
                        ".\"",
                        "\u010a",
                        "\u010a",
                        "'",
                        "Chall",
                        "enge",
                        " inequality",
                        "'",
                        "\u010a",
                        "\u010a",
                        "The"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        18.62006950378418,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        8.52071475982666,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " Brexit despite having voted to remain\".\u010a\u010aIf Scotland voted for independence in autumn 2018",
                    "max_token": "\u010a",
                    "tokens": [
                        " Brexit",
                        " despite",
                        " having",
                        " voted",
                        " to",
                        " remain",
                        "\".",
                        "\u010a",
                        "\u010a",
                        "If",
                        " Scotland",
                        " voted",
                        " for",
                        " independence",
                        " in",
                        " autumn",
                        " 2018"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        30.18190765380859,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " of the EU against our will.\u010a\u010a\"People in Scotland voted overwhelmingly to remain",
                    "max_token": "\u010a",
                    "tokens": [
                        " of",
                        " the",
                        " EU",
                        " against",
                        " our",
                        " will",
                        ".",
                        "\u010a",
                        "\u010a",
                        "\"",
                        "People",
                        " in",
                        " Scotland",
                        " voted",
                        " overwhelmingly",
                        " to",
                        " remain"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        8.346160888671875,
                        3.440498352050781,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " have to travel to the US.\u010a\u010aThe Hollywood star plans to be a '",
                    "max_token": "\u010a",
                    "tokens": [
                        " have",
                        " to",
                        " travel",
                        " to",
                        " the",
                        " US",
                        ".",
                        "\u010a",
                        "\u010a",
                        "The",
                        " Hollywood",
                        " star",
                        " plans",
                        " to",
                        " be",
                        " a",
                        " '"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        34.55230331420898,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " but to shut it down immediately.\u010a\u010aThe school said the move was due to",
                    "max_token": "\u010a",
                    "tokens": [
                        " but",
                        " to",
                        " shut",
                        " it",
                        " down",
                        " immediately",
                        ".",
                        "\u010a",
                        "\u010a",
                        "The",
                        " school",
                        " said",
                        " the",
                        " move",
                        " was",
                        " due",
                        " to"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0.8599828481674194,
                        0,
                        33.53326416015625,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " quick lead only to have to bow out before the end of the game because they went",
                    "tokens": [
                        " quick",
                        " lead",
                        " only",
                        " to",
                        " have",
                        " to",
                        " bow",
                        " out",
                        " before",
                        " the",
                        " end",
                        " of",
                        " the",
                        " game",
                        " because",
                        " they",
                        " went"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "\u010aAldrin was a drug user before, Castillo admitted, but he had",
                    "tokens": [
                        "\u010a",
                        "A",
                        "ld",
                        "rin",
                        " was",
                        " a",
                        " drug",
                        " user",
                        " before",
                        ",",
                        " Cast",
                        "illo",
                        " admitted",
                        ",",
                        " but",
                        " he",
                        " had"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " height at some international summit.\u010a\u010aOr is it because of these deeply controversial statements",
                    "tokens": [
                        " height",
                        " at",
                        " some",
                        " international",
                        " summit",
                        ".",
                        "\u010a",
                        "\u010a",
                        "Or",
                        " is",
                        " it",
                        " because",
                        " of",
                        " these",
                        " deeply",
                        " controversial",
                        " statements"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " years after he was jailed for espionage, before his release in 1969.\u010a\u010aHe",
                    "tokens": [
                        " years",
                        " after",
                        " he",
                        " was",
                        " jailed",
                        " for",
                        " espionage",
                        ",",
                        " before",
                        " his",
                        " release",
                        " in",
                        " 1969",
                        ".",
                        "\u010a",
                        "\u010a",
                        "He"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " faces with respect to women.\u010a\u010aA CNN poll released Monday showed Obama moving into",
                    "tokens": [
                        " faces",
                        " with",
                        " respect",
                        " to",
                        " women",
                        ".",
                        "\u010a",
                        "\u010a",
                        "A",
                        " CNN",
                        " poll",
                        " released",
                        " Monday",
                        " showed",
                        " Obama",
                        " moving",
                        " into"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 38.57255554199219
        },
        {
            "feature_index": 174,
            "gpt_predictions": [
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "business-related terms, especially those related to financial markets and corporate activities",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " Line, Brookline and the Upper East Side are about $150,000, $",
                    "max_token": " Side",
                    "tokens": [
                        " Line",
                        ",",
                        " Brook",
                        "line",
                        " and",
                        " the",
                        " Upper",
                        " East",
                        " Side",
                        " are",
                        " about",
                        " $",
                        "150",
                        ",",
                        "000",
                        ",",
                        " $"
                    ],
                    "values": [
                        0.06313779205083847,
                        1.948739409446716,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        9.223626136779785,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "\u010a\u00e2\u0122\u013eMurder rates in these areas have risen sharply. [Drug trafficking]",
                    "max_token": " areas",
                    "tokens": [
                        "\u010a",
                        "\u00e2\u0122",
                        "\u013e",
                        "Mur",
                        "der",
                        " rates",
                        " in",
                        " these",
                        " areas",
                        " have",
                        " risen",
                        " sharply",
                        ".",
                        " [",
                        "Drug",
                        " trafficking",
                        "]"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        17.55994033813477,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": ".\u010a\u010aMy week on Soylent: 'I was irritable, gr",
                    "max_token": "ent",
                    "tokens": [
                        ".",
                        "\u010a",
                        "\u010a",
                        "My",
                        " week",
                        " on",
                        " Soy",
                        "l",
                        "ent",
                        ":",
                        " '",
                        "I",
                        " was",
                        " irrit",
                        "able",
                        ",",
                        " gr"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        7.577149868011475,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " by some experts who say risks of such events are rising with greenhouse gas emissions, led",
                    "max_token": " events",
                    "tokens": [
                        " by",
                        " some",
                        " experts",
                        " who",
                        " say",
                        " risks",
                        " of",
                        " such",
                        " events",
                        " are",
                        " rising",
                        " with",
                        " greenhouse",
                        " gas",
                        " emissions",
                        ",",
                        " led"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        13.79122924804688,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " and prices for U.S. East services are often lower than the same services originating",
                    "max_token": " services",
                    "tokens": [
                        " and",
                        " prices",
                        " for",
                        " U",
                        ".",
                        "S",
                        ".",
                        " East",
                        " services",
                        " are",
                        " often",
                        " lower",
                        " than",
                        " the",
                        " same",
                        " services",
                        " originating"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        1.216608047485352,
                        17.91209983825684,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "ESPN, UniMas, UDN) before facing the 2014 FIFA World Cup<|endoftext|> can",
                    "tokens": [
                        "ESPN",
                        ",",
                        " Uni",
                        "Mas",
                        ",",
                        " U",
                        "DN",
                        ")",
                        " before",
                        " facing",
                        " the",
                        " 2014",
                        " FIFA",
                        " World",
                        " Cup",
                        "<|endoftext|>",
                        " can"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " vaccine industry, with the assistance of a sold-out government, is forcing vaccinations on",
                    "tokens": [
                        " vaccine",
                        " industry",
                        ",",
                        " with",
                        " the",
                        " assistance",
                        " of",
                        " a",
                        " sold",
                        "-",
                        "out",
                        " government",
                        ",",
                        " is",
                        " forcing",
                        " vaccinations",
                        " on"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": ", build and maintain effective student groups, advertise and rebrand conservative values, engage in",
                    "tokens": [
                        ",",
                        " build",
                        " and",
                        " maintain",
                        " effective",
                        " student",
                        " groups",
                        ",",
                        " advertise",
                        " and",
                        " re",
                        "brand",
                        " conservative",
                        " values",
                        ",",
                        " engage",
                        " in"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " political protest\u010a\u010aUpdated\u010a\u010aIt was one of the great art heists of",
                    "tokens": [
                        " political",
                        " protest",
                        "\u010a",
                        "\u010a",
                        "Updated",
                        "\u010a",
                        "\u010a",
                        "It",
                        " was",
                        " one",
                        " of",
                        " the",
                        " great",
                        " art",
                        " he",
                        "ists",
                        " of"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " that kid who felt unwanted, or the grown up who remembers how hard it was to",
                    "tokens": [
                        " that",
                        " kid",
                        " who",
                        " felt",
                        " unwanted",
                        ",",
                        " or",
                        " the",
                        " grown",
                        " up",
                        " who",
                        " remembers",
                        " how",
                        " hard",
                        " it",
                        " was",
                        " to"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 24.06775093078613
        },
        {
            "feature_index": 371,
            "gpt_predictions": [
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "words related to elements from the periodic table",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " water poured on his head on<|endoftext|>Celestin wanted to celebrate his wife\u00e2\u0122",
                    "max_token": "el",
                    "tokens": [
                        " water",
                        " poured",
                        " on",
                        " his",
                        " head",
                        " on",
                        "<|endoftext|>",
                        "C",
                        "el",
                        "est",
                        "in",
                        " wanted",
                        " to",
                        " celebrate",
                        " his",
                        " wife",
                        "\u00e2\u0122"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        27.76385688781738,
                        1.999859571456909,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " tram stop, the relocation of the cenotaph, development of the surrounding property",
                    "max_token": "en",
                    "tokens": [
                        " tram",
                        " stop",
                        ",",
                        " the",
                        " relocation",
                        " of",
                        " the",
                        " c",
                        "en",
                        "ot",
                        "aph",
                        ",",
                        " development",
                        " of",
                        " the",
                        " surrounding",
                        " property"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        31.78310394287109,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " peculiar tradition of picnicking in cemeteries. After the death of Queen",
                    "max_token": "em",
                    "tokens": [
                        " peculiar",
                        " tradition",
                        " of",
                        " pic",
                        "n",
                        "icking",
                        " in",
                        " c",
                        "em",
                        "eter",
                        "ies",
                        ".",
                        " After",
                        " the",
                        " death",
                        " of",
                        " Queen"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        27.38066482543945,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " Guerrilla tactics. Self reliance. Censorship will be enforced. There will be",
                    "max_token": "ens",
                    "tokens": [
                        " Gu",
                        "errilla",
                        " tactics",
                        ".",
                        " Self",
                        " reliance",
                        ".",
                        " C",
                        "ens",
                        "orship",
                        " will",
                        " be",
                        " enforced",
                        ".",
                        " There",
                        " will",
                        " be"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        17.63015937805176,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " to restore themselves.\u010a\u010aThe ceaseless attentional demands of modern life put",
                    "max_token": "as",
                    "tokens": [
                        " to",
                        " restore",
                        " themselves",
                        ".",
                        "\u010a",
                        "\u010a",
                        "The",
                        " ce",
                        "as",
                        "eless",
                        " attention",
                        "al",
                        " demands",
                        " of",
                        " modern",
                        " life",
                        " put"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0.02875632792711258,
                        6.565809726715088,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "\u00e2\u0122\u013bThe<|endoftext|> potential impact surveys and mitigation and restoration plans\u00e2\u0122\u013b\u00e2\u0122\u013b are",
                    "tokens": [
                        "\u00e2\u0122",
                        "\u013b",
                        "The",
                        "<|endoftext|>",
                        " potential",
                        " impact",
                        " surveys",
                        " and",
                        " mitigation",
                        " and",
                        " restoration",
                        " plans",
                        "\u00e2\u0122",
                        "\u013b",
                        "\u00e2\u0122",
                        "\u013b",
                        " are"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " that kid who felt unwanted, or the grown up who remembers how hard it was to",
                    "tokens": [
                        " that",
                        " kid",
                        " who",
                        " felt",
                        " unwanted",
                        ",",
                        " or",
                        " the",
                        " grown",
                        " up",
                        " who",
                        " remembers",
                        " how",
                        " hard",
                        " it",
                        " was",
                        " to"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " Trump and Hillary in the first debate, before either said a word, Trump made what",
                    "tokens": [
                        " Trump",
                        " and",
                        " Hillary",
                        " in",
                        " the",
                        " first",
                        " debate",
                        ",",
                        " before",
                        " either",
                        " said",
                        " a",
                        " word",
                        ",",
                        " Trump",
                        " made",
                        " what"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "3 billion) installing everything from traffic-management technologies to smart electricity grids.\u010a\u010a",
                    "tokens": [
                        "3",
                        " billion",
                        ")",
                        " installing",
                        " everything",
                        " from",
                        " traffic",
                        "-",
                        "management",
                        " technologies",
                        " to",
                        " smart",
                        " electricity",
                        " grids",
                        ".",
                        "\u010a",
                        "\u010a"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " city staff (Fire, Police, Crime Prevention, and more)\u010a\u010aOption to",
                    "tokens": [
                        " city",
                        " staff",
                        " (",
                        "Fire",
                        ",",
                        " Police",
                        ",",
                        " Crime",
                        " Prevention",
                        ",",
                        " and",
                        " more",
                        ")",
                        "\u010a",
                        "\u010a",
                        "Option",
                        " to"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 40.47116088867188
        },
        {
            "feature_index": 527,
            "gpt_predictions": [
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "references to cartoon characters",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": ", with large, intense eyes. One cartoon, from February 2012, features a smiling",
                    "max_token": " cartoon",
                    "tokens": [
                        ",",
                        " with",
                        " large",
                        ",",
                        " intense",
                        " eyes",
                        ".",
                        " One",
                        " cartoon",
                        ",",
                        " from",
                        " February",
                        " 2012",
                        ",",
                        " features",
                        " a",
                        " smiling"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        55.91127395629883,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " mental health professionals in schools and producing a cartoon public service announcement that was apparently intended to",
                    "max_token": " cartoon",
                    "tokens": [
                        " mental",
                        " health",
                        " professionals",
                        " in",
                        " schools",
                        " and",
                        " producing",
                        " a",
                        " cartoon",
                        " public",
                        " service",
                        " announcement",
                        " that",
                        " was",
                        " apparently",
                        " intended",
                        " to"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        58.2289924621582,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": ", like a hyper-stylized cartoon version of hate. Mobs of white",
                    "max_token": " cartoon",
                    "tokens": [
                        ",",
                        " like",
                        " a",
                        " hyper",
                        "-",
                        "st",
                        "yl",
                        "ized",
                        " cartoon",
                        " version",
                        " of",
                        " hate",
                        ".",
                        " M",
                        "obs",
                        " of",
                        " white"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        55.12101364135742,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "??\u010a\u010aIt reminded me of old cartoons where Donald Duck or Bugs Bunny would build",
                    "max_token": " cartoons",
                    "tokens": [
                        "??",
                        "\u010a",
                        "\u010a",
                        "It",
                        " reminded",
                        " me",
                        " of",
                        " old",
                        " cartoons",
                        " where",
                        " Donald",
                        " Duck",
                        " or",
                        " Bugs",
                        " Bunny",
                        " would",
                        " build"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        35.9976806640625,
                        0,
                        0,
                        2.120830297470093,
                        0,
                        0,
                        1.361710071563721,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " confusion regarding how to go about getting the sketchbook, and for that, I apologize",
                    "max_token": " sketch",
                    "tokens": [
                        " confusion",
                        " regarding",
                        " how",
                        " to",
                        " go",
                        " about",
                        " getting",
                        " the",
                        " sketch",
                        "book",
                        ",",
                        " and",
                        " for",
                        " that",
                        ",",
                        " I",
                        " apologize"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        8.031245231628418,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " quick lead only to have to bow out before the end of the game because they went",
                    "tokens": [
                        " quick",
                        " lead",
                        " only",
                        " to",
                        " have",
                        " to",
                        " bow",
                        " out",
                        " before",
                        " the",
                        " end",
                        " of",
                        " the",
                        " game",
                        " because",
                        " they",
                        " went"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "\u010aAldrin was a drug user before, Castillo admitted, but he had",
                    "tokens": [
                        "\u010a",
                        "A",
                        "ld",
                        "rin",
                        " was",
                        " a",
                        " drug",
                        " user",
                        " before",
                        ",",
                        " Cast",
                        "illo",
                        " admitted",
                        ",",
                        " but",
                        " he",
                        " had"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " height at some international summit.\u010a\u010aOr is it because of these deeply controversial statements",
                    "tokens": [
                        " height",
                        " at",
                        " some",
                        " international",
                        " summit",
                        ".",
                        "\u010a",
                        "\u010a",
                        "Or",
                        " is",
                        " it",
                        " because",
                        " of",
                        " these",
                        " deeply",
                        " controversial",
                        " statements"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " years after he was jailed for espionage, before his release in 1969.\u010a\u010aHe",
                    "tokens": [
                        " years",
                        " after",
                        " he",
                        " was",
                        " jailed",
                        " for",
                        " espionage",
                        ",",
                        " before",
                        " his",
                        " release",
                        " in",
                        " 1969",
                        ".",
                        "\u010a",
                        "\u010a",
                        "He"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " faces with respect to women.\u010a\u010aA CNN poll released Monday showed Obama moving into",
                    "tokens": [
                        " faces",
                        " with",
                        " respect",
                        " to",
                        " women",
                        ".",
                        "\u010a",
                        "\u010a",
                        "A",
                        " CNN",
                        " poll",
                        " released",
                        " Monday",
                        " showed",
                        " Obama",
                        " moving",
                        " into"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 66.57323455810547
        },
        {
            "feature_index": 210,
            "gpt_predictions": [
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "technical terms related to programming or scripting languages and development tools",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " and 21.4 abortions per 1,000 women aged 25--29 years). Ad",
                    "max_token": "000",
                    "tokens": [
                        " and",
                        " 21",
                        ".",
                        "4",
                        " abortions",
                        " per",
                        " 1",
                        ",",
                        "000",
                        " women",
                        " aged",
                        " 25",
                        "--",
                        "29",
                        " years",
                        ").",
                        " Ad"
                    ],
                    "values": [
                        1.048080921173096,
                        2.893315553665161,
                        0,
                        2.098065853118896,
                        0,
                        0.2167694568634033,
                        1.010624289512634,
                        0,
                        6.918351173400879,
                        1.223344564437866,
                        0,
                        1.13573694229126,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " C00TrainerScript\u010a\u010aC00TrainerScript DGIntimidateQuest",
                    "max_token": "00",
                    "tokens": [
                        " C",
                        "00",
                        "Tr",
                        "ainer",
                        "Script",
                        "\u010a",
                        "\u010a",
                        "C",
                        "00",
                        "Tr",
                        "ainer",
                        "Script",
                        " DG",
                        "Int",
                        "im",
                        "idate",
                        "Quest"
                    ],
                    "values": [
                        0.9971692562103271,
                        0.9045915603637695,
                        0.1886334121227264,
                        1.993083357810974,
                        0,
                        0,
                        3.499995470046997,
                        2.610048055648804,
                        7.593070507049561,
                        5.398924350738525,
                        2.209406137466431,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "8)\u010a\u010aJuly 23, 1917 (51-8) Reported by the joint",
                    "max_token": " (",
                    "tokens": [
                        "8",
                        ")",
                        "\u010a",
                        "\u010a",
                        "July",
                        " 23",
                        ",",
                        " 1917",
                        " (",
                        "51",
                        "-",
                        "8",
                        ")",
                        " Reported",
                        " by",
                        " the",
                        " joint"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0.5405080914497375,
                        0,
                        0,
                        0,
                        0.7325888872146606,
                        6.794729232788086,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 13,
                    "sentence_string": " they occur in both continents, and the little auk is referred to as the dove",
                    "max_token": " to",
                    "tokens": [
                        " they",
                        " occur",
                        " in",
                        " both",
                        " continents",
                        ",",
                        " and",
                        " the",
                        " little",
                        " a",
                        "uk",
                        " is",
                        " referred",
                        " to",
                        " as",
                        " the",
                        " dove"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        1.173985123634338,
                        1.695258975028992,
                        0,
                        0,
                        0.7390210628509521,
                        0,
                        2.225130319595337,
                        1.096085786819458,
                        1.271113634109497,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 16,
                    "sentence_string": "\u010a\u010a(optional) Create script to add application users to DB\u010a\u010acreate_",
                    "max_token": "_",
                    "tokens": [
                        "\u010a",
                        "\u010a",
                        "(",
                        "optional",
                        ")",
                        " Create",
                        " script",
                        " to",
                        " add",
                        " application",
                        " users",
                        " to",
                        " DB",
                        "\u010a",
                        "\u010a",
                        "create",
                        "_"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        4.913967609405518,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0.3680116534233093,
                        0,
                        5.050406932830811
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " Trump and Hillary in the first debate, before either said a word, Trump made what",
                    "tokens": [
                        " Trump",
                        " and",
                        " Hillary",
                        " in",
                        " the",
                        " first",
                        " debate",
                        ",",
                        " before",
                        " either",
                        " said",
                        " a",
                        " word",
                        ",",
                        " Trump",
                        " made",
                        " what"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " that kid who felt unwanted, or the grown up who remembers how hard it was to",
                    "tokens": [
                        " that",
                        " kid",
                        " who",
                        " felt",
                        " unwanted",
                        ",",
                        " or",
                        " the",
                        " grown",
                        " up",
                        " who",
                        " remembers",
                        " how",
                        " hard",
                        " it",
                        " was",
                        " to"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "\u010aAldrin was a drug user before, Castillo admitted, but he had",
                    "tokens": [
                        "\u010a",
                        "A",
                        "ld",
                        "rin",
                        " was",
                        " a",
                        " drug",
                        " user",
                        " before",
                        ",",
                        " Cast",
                        "illo",
                        " admitted",
                        ",",
                        " but",
                        " he",
                        " had"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " who genuinely seem not to believe reality. Or Democrats, cowed into silence on issues",
                    "tokens": [
                        " who",
                        " genuinely",
                        " seem",
                        " not",
                        " to",
                        " believe",
                        " reality",
                        ".",
                        " Or",
                        " Democrats",
                        ",",
                        " c",
                        "owed",
                        " into",
                        " silence",
                        " on",
                        " issues"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " the incident, saying the man threatened him before leaving.\u010a\u010a\u00e2\u0122\u013eI think",
                    "tokens": [
                        " the",
                        " incident",
                        ",",
                        " saying",
                        " the",
                        " man",
                        " threatened",
                        " him",
                        " before",
                        " leaving",
                        ".",
                        "\u010a",
                        "\u010a",
                        "\u00e2\u0122",
                        "\u013e",
                        "I",
                        " think"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 12.5833854675293
        },
        {
            "feature_index": 235,
            "gpt_predictions": [
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "the name \"Ali\" in the text",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " you to see where it actually is,\" Ali Yazdani, the team leader and",
                    "max_token": " Ali",
                    "tokens": [
                        " you",
                        " to",
                        " see",
                        " where",
                        " it",
                        " actually",
                        " is",
                        ",\"",
                        " Ali",
                        " Yaz",
                        "d",
                        "ani",
                        ",",
                        " the",
                        " team",
                        " leader",
                        " and"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        54.69235992431641,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " resources.\u00e2\u0122\u013f\u010a\u010aPlus, Aliababa\u00e2\u0122\u013bs \u00e2\u0122\u013efinancial",
                    "max_token": " Ali",
                    "tokens": [
                        " resources",
                        ".",
                        "\u00e2\u0122",
                        "\u013f",
                        "\u010a",
                        "\u010a",
                        "Plus",
                        ",",
                        " Ali",
                        "ab",
                        "aba",
                        "\u00e2\u0122",
                        "\u013b",
                        "s",
                        " \u00e2\u0122",
                        "\u013e",
                        "financial"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        57.73093795776367,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " man drove them to an apartment rented by Ali Hassan Saeed.\u010a\u010aStarl",
                    "max_token": " Ali",
                    "tokens": [
                        " man",
                        " drove",
                        " them",
                        " to",
                        " an",
                        " apartment",
                        " rented",
                        " by",
                        " Ali",
                        " Hassan",
                        " Sa",
                        "eed",
                        ".",
                        "\u010a",
                        "\u010a",
                        "St",
                        "arl"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        54.28917694091797,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " Mouth of<|endoftext|>How much water does a camel's hump hold? None. A camel",
                    "max_token": " camel",
                    "tokens": [
                        " Mouth",
                        " of",
                        "<|endoftext|>",
                        "How",
                        " much",
                        " water",
                        " does",
                        " a",
                        " camel",
                        "'s",
                        " hump",
                        " hold",
                        "?",
                        " None",
                        ".",
                        " A",
                        " camel"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        8.415749549865723,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        5.257773399353027
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": ",\u00e2\u0122\u013f said Faiza N. Ali, community affairs director of CAIR\u00e2\u0122",
                    "max_token": " Ali",
                    "tokens": [
                        ",",
                        "\u00e2\u0122",
                        "\u013f",
                        " said",
                        " Fa",
                        "iza",
                        " N",
                        ".",
                        " Ali",
                        ",",
                        " community",
                        " affairs",
                        " director",
                        " of",
                        " CA",
                        "IR",
                        "\u00e2\u0122"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        44.19907379150391,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " Trump and Hillary in the first debate, before either said a word, Trump made what",
                    "tokens": [
                        " Trump",
                        " and",
                        " Hillary",
                        " in",
                        " the",
                        " first",
                        " debate",
                        ",",
                        " before",
                        " either",
                        " said",
                        " a",
                        " word",
                        ",",
                        " Trump",
                        " made",
                        " what"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " that kid who felt unwanted, or the grown up who remembers how hard it was to",
                    "tokens": [
                        " that",
                        " kid",
                        " who",
                        " felt",
                        " unwanted",
                        ",",
                        " or",
                        " the",
                        " grown",
                        " up",
                        " who",
                        " remembers",
                        " how",
                        " hard",
                        " it",
                        " was",
                        " to"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "\u010aAldrin was a drug user before, Castillo admitted, but he had",
                    "tokens": [
                        "\u010a",
                        "A",
                        "ld",
                        "rin",
                        " was",
                        " a",
                        " drug",
                        " user",
                        " before",
                        ",",
                        " Cast",
                        "illo",
                        " admitted",
                        ",",
                        " but",
                        " he",
                        " had"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " who genuinely seem not to believe reality. Or Democrats, cowed into silence on issues",
                    "tokens": [
                        " who",
                        " genuinely",
                        " seem",
                        " not",
                        " to",
                        " believe",
                        " reality",
                        ".",
                        " Or",
                        " Democrats",
                        ",",
                        " c",
                        "owed",
                        " into",
                        " silence",
                        " on",
                        " issues"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " the incident, saying the man threatened him before leaving.\u010a\u010a\u00e2\u0122\u013eI think",
                    "tokens": [
                        " the",
                        " incident",
                        ",",
                        " saying",
                        " the",
                        " man",
                        " threatened",
                        " him",
                        " before",
                        " leaving",
                        ".",
                        "\u010a",
                        "\u010a",
                        "\u00e2\u0122",
                        "\u013e",
                        "I",
                        " think"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 60.05088043212891
        },
        {
            "feature_index": 101,
            "gpt_predictions": [
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "words related to style or stylization",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " that he had discussed the matter with the clerk of the Commons.\u010a\u010aBerc",
                    "max_token": " clerk",
                    "tokens": [
                        " that",
                        " he",
                        " had",
                        " discussed",
                        " the",
                        " matter",
                        " with",
                        " the",
                        " clerk",
                        " of",
                        " the",
                        " Commons",
                        ".",
                        "\u010a",
                        "\u010a",
                        "B",
                        "erc"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        12.30235767364502,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " A/T ( 21 ). If sRNAs are the product of transcriptional noise",
                    "max_token": "RN",
                    "tokens": [
                        " A",
                        "/",
                        "T",
                        " (",
                        " 21",
                        " ).",
                        " If",
                        " s",
                        "RN",
                        "As",
                        " are",
                        " the",
                        " product",
                        " of",
                        " transcription",
                        "al",
                        " noise"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        6.621058940887451,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " are published novels by writers with MFAs stylistically similar to published novels by authors without",
                    "max_token": " styl",
                    "tokens": [
                        " are",
                        " published",
                        " novels",
                        " by",
                        " writers",
                        " with",
                        " MF",
                        "As",
                        " styl",
                        "istically",
                        " similar",
                        " to",
                        " published",
                        " novels",
                        " by",
                        " authors",
                        " without"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        30.07724952697754,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " visual expression. It may be the most stylised game Suda has yet embarked on",
                    "max_token": " styl",
                    "tokens": [
                        " visual",
                        " expression",
                        ".",
                        " It",
                        " may",
                        " be",
                        " the",
                        " most",
                        " styl",
                        "ised",
                        " game",
                        " Sud",
                        "a",
                        " has",
                        " yet",
                        " embarked",
                        " on"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        26.87040519714355,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " and GILT; Eugene Tong, stylist and former style director at Details;",
                    "max_token": " styl",
                    "tokens": [
                        " and",
                        " G",
                        "IL",
                        "T",
                        ";",
                        " Eugene",
                        " Tong",
                        ",",
                        " styl",
                        "ist",
                        " and",
                        " former",
                        " style",
                        " director",
                        " at",
                        " Details",
                        ";"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        27.20139312744141,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "ESPN, UniMas, UDN) before facing the 2014 FIFA World Cup<|endoftext|> can",
                    "tokens": [
                        "ESPN",
                        ",",
                        " Uni",
                        "Mas",
                        ",",
                        " U",
                        "DN",
                        ")",
                        " before",
                        " facing",
                        " the",
                        " 2014",
                        " FIFA",
                        " World",
                        " Cup",
                        "<|endoftext|>",
                        " can"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " vaccine industry, with the assistance of a sold-out government, is forcing vaccinations on",
                    "tokens": [
                        " vaccine",
                        " industry",
                        ",",
                        " with",
                        " the",
                        " assistance",
                        " of",
                        " a",
                        " sold",
                        "-",
                        "out",
                        " government",
                        ",",
                        " is",
                        " forcing",
                        " vaccinations",
                        " on"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": ", build and maintain effective student groups, advertise and rebrand conservative values, engage in",
                    "tokens": [
                        ",",
                        " build",
                        " and",
                        " maintain",
                        " effective",
                        " student",
                        " groups",
                        ",",
                        " advertise",
                        " and",
                        " re",
                        "brand",
                        " conservative",
                        " values",
                        ",",
                        " engage",
                        " in"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " political protest\u010a\u010aUpdated\u010a\u010aIt was one of the great art heists of",
                    "tokens": [
                        " political",
                        " protest",
                        "\u010a",
                        "\u010a",
                        "Updated",
                        "\u010a",
                        "\u010a",
                        "It",
                        " was",
                        " one",
                        " of",
                        " the",
                        " great",
                        " art",
                        " he",
                        "ists",
                        " of"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " that kid who felt unwanted, or the grown up who remembers how hard it was to",
                    "tokens": [
                        " that",
                        " kid",
                        " who",
                        " felt",
                        " unwanted",
                        ",",
                        " or",
                        " the",
                        " grown",
                        " up",
                        " who",
                        " remembers",
                        " how",
                        " hard",
                        " it",
                        " was",
                        " to"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 30.65951919555664
        },
        {
            "feature_index": 986,
            "gpt_predictions": [
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "names of specific computer software or platforms",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " gain 114 in net new advisers. LPL now has 13,840 registered reps and",
                    "max_token": "PL",
                    "tokens": [
                        " gain",
                        " 114",
                        " in",
                        " net",
                        " new",
                        " advisers",
                        ".",
                        " L",
                        "PL",
                        " now",
                        " has",
                        " 13",
                        ",",
                        "840",
                        " registered",
                        " reps",
                        " and"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        11.46403980255127,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " said he was sanguine regarding LPL's growth outlook for the second half of",
                    "max_token": "PL",
                    "tokens": [
                        " said",
                        " he",
                        " was",
                        " s",
                        "angu",
                        "ine",
                        " regarding",
                        " L",
                        "PL",
                        "'s",
                        " growth",
                        " outlook",
                        " for",
                        " the",
                        " second",
                        " half",
                        " of"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        17.06571960449219,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": ".Over 350 Sikh Americans attended the LA Clippers game alongside the other 19,000 spectators",
                    "max_token": " Clippers",
                    "tokens": [
                        ".",
                        "Over",
                        " 350",
                        " Sikh",
                        " Americans",
                        " attended",
                        " the",
                        " LA",
                        " Clippers",
                        " game",
                        " alongside",
                        " the",
                        " other",
                        " 19",
                        ",",
                        "000",
                        " spectators"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        1.763243317604065,
                        4.853730201721191,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " its quarterly earnings report Wednesday morning, LPL reported that general and administrative expenses for the",
                    "max_token": "PL",
                    "tokens": [
                        " its",
                        " quarterly",
                        " earnings",
                        " report",
                        " Wednesday",
                        " morning",
                        ",",
                        " L",
                        "PL",
                        " reported",
                        " that",
                        " general",
                        " and",
                        " administrative",
                        " expenses",
                        " for",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        20.2584285736084,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " who headed the local chapter of the SCLC.\u010a\u010aMatthews and Aber",
                    "max_token": "CL",
                    "tokens": [
                        " who",
                        " headed",
                        " the",
                        " local",
                        " chapter",
                        " of",
                        " the",
                        " S",
                        "CL",
                        "C",
                        ".",
                        "\u010a",
                        "\u010a",
                        "Matthew",
                        "s",
                        " and",
                        " Aber"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        19.36446762084961,
                        0.588832437992096,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " coat.\u010a\u010aDetective Inspector Steve Christian, from Liverpool CID, said:",
                    "tokens": [
                        " coat",
                        ".",
                        "\u010a",
                        "\u010a",
                        "Det",
                        "ective",
                        " Inspector",
                        " Steve",
                        " Christian",
                        ",",
                        " from",
                        " Liverpool",
                        " C",
                        "ID",
                        ",",
                        " said",
                        ":"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " drop out of the presidential race, according to NBC News ' Andrea Mitchell.\u010a\u010a",
                    "tokens": [
                        " drop",
                        " out",
                        " of",
                        " the",
                        " presidential",
                        " race",
                        ",",
                        " according",
                        " to",
                        " NBC",
                        " News",
                        " '",
                        " Andrea",
                        " Mitchell",
                        ".",
                        "\u010a",
                        "\u010a"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " Rumors that Amazon will soon begin accepting Bitcoin are persistent. If the retail giant does",
                    "tokens": [
                        " Rum",
                        "ors",
                        " that",
                        " Amazon",
                        " will",
                        " soon",
                        " begin",
                        " accepting",
                        " Bitcoin",
                        " are",
                        " persistent",
                        ".",
                        " If",
                        " the",
                        " retail",
                        " giant",
                        " does"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": ", and banking flows was a monthly net TIC (Treasury International Capital)<|endoftext|>",
                    "tokens": [
                        ",",
                        " and",
                        " banking",
                        " flows",
                        " was",
                        " a",
                        " monthly",
                        " net",
                        " T",
                        "IC",
                        " (",
                        "Tre",
                        "asury",
                        " International",
                        " Capital",
                        ")",
                        "<|endoftext|>"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " faces with respect to women.\u010a\u010aA CNN poll released Monday showed Obama moving into",
                    "tokens": [
                        " faces",
                        " with",
                        " respect",
                        " to",
                        " women",
                        ".",
                        "\u010a",
                        "\u010a",
                        "A",
                        " CNN",
                        " poll",
                        " released",
                        " Monday",
                        " showed",
                        " Obama",
                        " moving",
                        " into"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 23.98987770080566
        },
        {
            "feature_index": 902,
            "gpt_predictions": [
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "phrases related to the concept of location or direction",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " segment for a bit.\u010a\u010aSo where will we see this new type of omn",
                    "max_token": " where",
                    "tokens": [
                        " segment",
                        " for",
                        " a",
                        " bit",
                        ".",
                        "\u010a",
                        "\u010a",
                        "So",
                        " where",
                        " will",
                        " we",
                        " see",
                        " this",
                        " new",
                        " type",
                        " of",
                        " omn"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        13.07079696655273,
                        2.466827392578125,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "\u010aLong before DeSantis even knew where Bhutan was on a map -- it",
                    "max_token": " where",
                    "tokens": [
                        "\u010a",
                        "Long",
                        " before",
                        " De",
                        "S",
                        "antis",
                        " even",
                        " knew",
                        " where",
                        " Bh",
                        "utan",
                        " was",
                        " on",
                        " a",
                        " map",
                        " --",
                        " it"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        52.62291717529297,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " years, it can be tough to gauge where they all fit.\u010a\u010aComing in",
                    "max_token": " where",
                    "tokens": [
                        " years",
                        ",",
                        " it",
                        " can",
                        " be",
                        " tough",
                        " to",
                        " gauge",
                        " where",
                        " they",
                        " all",
                        " fit",
                        ".",
                        "\u010a",
                        "\u010a",
                        "Coming",
                        " in"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        54.37118148803711,
                        0,
                        0,
                        0.1522822976112366,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "\u010a\u010aRosetta has since documented exactly where on the comet Philae landed, showing",
                    "max_token": " where",
                    "tokens": [
                        "\u010a",
                        "\u010a",
                        "Ros",
                        "etta",
                        " has",
                        " since",
                        " documented",
                        " exactly",
                        " where",
                        " on",
                        " the",
                        " comet",
                        " Phil",
                        "ae",
                        " landed",
                        ",",
                        " showing"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        54.58004379272461,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": ", there were a lot of indications of where I was headed. I used to draw",
                    "max_token": " where",
                    "tokens": [
                        ",",
                        " there",
                        " were",
                        " a",
                        " lot",
                        " of",
                        " indications",
                        " of",
                        " where",
                        " I",
                        " was",
                        " headed",
                        ".",
                        " I",
                        " used",
                        " to",
                        " draw"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        54.37477874755859,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " coat.\u010a\u010aDetective Inspector Steve Christian, from Liverpool CID, said:",
                    "tokens": [
                        " coat",
                        ".",
                        "\u010a",
                        "\u010a",
                        "Det",
                        "ective",
                        " Inspector",
                        " Steve",
                        " Christian",
                        ",",
                        " from",
                        " Liverpool",
                        " C",
                        "ID",
                        ",",
                        " said",
                        ":"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " drop out of the presidential race, according to NBC News ' Andrea Mitchell.\u010a\u010a",
                    "tokens": [
                        " drop",
                        " out",
                        " of",
                        " the",
                        " presidential",
                        " race",
                        ",",
                        " according",
                        " to",
                        " NBC",
                        " News",
                        " '",
                        " Andrea",
                        " Mitchell",
                        ".",
                        "\u010a",
                        "\u010a"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " Rumors that Amazon will soon begin accepting Bitcoin are persistent. If the retail giant does",
                    "tokens": [
                        " Rum",
                        "ors",
                        " that",
                        " Amazon",
                        " will",
                        " soon",
                        " begin",
                        " accepting",
                        " Bitcoin",
                        " are",
                        " persistent",
                        ".",
                        " If",
                        " the",
                        " retail",
                        " giant",
                        " does"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": ", and banking flows was a monthly net TIC (Treasury International Capital)<|endoftext|>",
                    "tokens": [
                        ",",
                        " and",
                        " banking",
                        " flows",
                        " was",
                        " a",
                        " monthly",
                        " net",
                        " T",
                        "IC",
                        " (",
                        "Tre",
                        "asury",
                        " International",
                        " Capital",
                        ")",
                        "<|endoftext|>"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " faces with respect to women.\u010a\u010aA CNN poll released Monday showed Obama moving into",
                    "tokens": [
                        " faces",
                        " with",
                        " respect",
                        " to",
                        " women",
                        ".",
                        "\u010a",
                        "\u010a",
                        "A",
                        " CNN",
                        " poll",
                        " released",
                        " Monday",
                        " showed",
                        " Obama",
                        " moving",
                        " into"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 58.33231353759766
        },
        {
            "feature_index": 947,
            "gpt_predictions": [
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "mentions of college-related terms",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " kind of address, not to those leaving college with pomp and circumstance at their backs but",
                    "max_token": " college",
                    "tokens": [
                        " kind",
                        " of",
                        " address",
                        ",",
                        " not",
                        " to",
                        " those",
                        " leaving",
                        " college",
                        " with",
                        " pomp",
                        " and",
                        " circumstance",
                        " at",
                        " their",
                        " backs",
                        " but"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        41.02019882202148,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " STEM degrees within the U.S. college-educated population.6\u010a\u010aThere",
                    "max_token": " college",
                    "tokens": [
                        " STEM",
                        " degrees",
                        " within",
                        " the",
                        " U",
                        ".",
                        "S",
                        ".",
                        " college",
                        "-",
                        "educated",
                        " population",
                        ".",
                        "6",
                        "\u010a",
                        "\u010a",
                        "There"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        60.72803497314453,
                        14.05216026306152,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " the season.<|endoftext|> The Richard Stockton College of New Jersey Stonehill College Swarth",
                    "max_token": " College",
                    "tokens": [
                        " the",
                        " season",
                        ".",
                        "<|endoftext|>",
                        " The",
                        " Richard",
                        " Stock",
                        "ton",
                        " College",
                        " of",
                        " New",
                        " Jersey",
                        " Stone",
                        "hill",
                        " College",
                        " Sw",
                        "arth"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        17.76603889465332,
                        0,
                        0,
                        0,
                        0,
                        0,
                        5.471542358398438,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " first-ever electronic music spring break for college students, Electro Beach, will take place",
                    "max_token": " college",
                    "tokens": [
                        " first",
                        "-",
                        "ever",
                        " electronic",
                        " music",
                        " spring",
                        " break",
                        " for",
                        " college",
                        " students",
                        ",",
                        " Electro",
                        " Beach",
                        ",",
                        " will",
                        " take",
                        " place"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        63.01636123657227,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "But he came to us essentially as a college freshman. And so he fits what the",
                    "max_token": " college",
                    "tokens": [
                        "But",
                        " he",
                        " came",
                        " to",
                        " us",
                        " essentially",
                        " as",
                        " a",
                        " college",
                        " freshman",
                        ".",
                        " And",
                        " so",
                        " he",
                        " fits",
                        " what",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        61.60145568847656,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " coat.\u010a\u010aDetective Inspector Steve Christian, from Liverpool CID, said:",
                    "tokens": [
                        " coat",
                        ".",
                        "\u010a",
                        "\u010a",
                        "Det",
                        "ective",
                        " Inspector",
                        " Steve",
                        " Christian",
                        ",",
                        " from",
                        " Liverpool",
                        " C",
                        "ID",
                        ",",
                        " said",
                        ":"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " drop out of the presidential race, according to NBC News ' Andrea Mitchell.\u010a\u010a",
                    "tokens": [
                        " drop",
                        " out",
                        " of",
                        " the",
                        " presidential",
                        " race",
                        ",",
                        " according",
                        " to",
                        " NBC",
                        " News",
                        " '",
                        " Andrea",
                        " Mitchell",
                        ".",
                        "\u010a",
                        "\u010a"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " Rumors that Amazon will soon begin accepting Bitcoin are persistent. If the retail giant does",
                    "tokens": [
                        " Rum",
                        "ors",
                        " that",
                        " Amazon",
                        " will",
                        " soon",
                        " begin",
                        " accepting",
                        " Bitcoin",
                        " are",
                        " persistent",
                        ".",
                        " If",
                        " the",
                        " retail",
                        " giant",
                        " does"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": ", and banking flows was a monthly net TIC (Treasury International Capital)<|endoftext|>",
                    "tokens": [
                        ",",
                        " and",
                        " banking",
                        " flows",
                        " was",
                        " a",
                        " monthly",
                        " net",
                        " T",
                        "IC",
                        " (",
                        "Tre",
                        "asury",
                        " International",
                        " Capital",
                        ")",
                        "<|endoftext|>"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " faces with respect to women.\u010a\u010aA CNN poll released Monday showed Obama moving into",
                    "tokens": [
                        " faces",
                        " with",
                        " respect",
                        " to",
                        " women",
                        ".",
                        "\u010a",
                        "\u010a",
                        "A",
                        " CNN",
                        " poll",
                        " released",
                        " Monday",
                        " showed",
                        " Obama",
                        " moving",
                        " into"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 68.42340087890625
        },
        {
            "feature_index": 346,
            "gpt_predictions": [
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "statistics and data related terms",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 7,
                    "sentence_string": ", Australia, and Spain. Israel came in sixth. The shortest life expectancy belongs to",
                    "max_token": " came",
                    "tokens": [
                        ",",
                        " Australia",
                        ",",
                        " and",
                        " Spain",
                        ".",
                        " Israel",
                        " came",
                        " in",
                        " sixth",
                        ".",
                        " The",
                        " shortest",
                        " life",
                        " expectancy",
                        " belongs",
                        " to"
                    ],
                    "values": [
                        3.68378472328186,
                        3.195358276367188,
                        3.615007162094116,
                        5.204311847686768,
                        10.27417659759521,
                        10.31820011138916,
                        6.872620105743408,
                        15.08695030212402,
                        14.94017696380615,
                        10.16798973083496,
                        11.0131950378418,
                        9.920610427856445,
                        9.495076179504395,
                        3.516841411590576,
                        8.272693634033203,
                        9.16899299621582,
                        7.181747436523438
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "orest and second-richest countries respectively were $271 in Burundi at the",
                    "max_token": " were",
                    "tokens": [
                        "orest",
                        " and",
                        " second",
                        "-",
                        "ric",
                        "hest",
                        " countries",
                        " respectively",
                        " were",
                        " $",
                        "271",
                        " in",
                        " Bur",
                        "und",
                        "i",
                        " at",
                        " the"
                    ],
                    "values": [
                        5.476662635803223,
                        5.369304656982422,
                        2.80304741859436,
                        2.345639705657959,
                        0,
                        3.821331977844238,
                        6.434623718261719,
                        8.70083236694336,
                        12.44374752044678,
                        7.845950603485107,
                        5.428738117218018,
                        6.141615390777588,
                        0,
                        0,
                        4.808928489685059,
                        5.528367042541504,
                        3.650514125823975
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " UK for the first time this year, behind India and China, according to figures from",
                    "max_token": " behind",
                    "tokens": [
                        " UK",
                        " for",
                        " the",
                        " first",
                        " time",
                        " this",
                        " year",
                        ",",
                        " behind",
                        " India",
                        " and",
                        " China",
                        ",",
                        " according",
                        " to",
                        " figures",
                        " from"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        2.531515836715698,
                        0,
                        0.8394549489021301,
                        4.457921981811523,
                        11.78538131713867,
                        4.571425437927246,
                        9.075204849243164,
                        6.646841049194336,
                        4.597662925720215,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 15,
                    "sentence_string": " Center analysis of Census Bureau data. The next most-multiracial states are far",
                    "max_token": " are",
                    "tokens": [
                        " Center",
                        " analysis",
                        " of",
                        " Census",
                        " Bureau",
                        " data",
                        ".",
                        " The",
                        " next",
                        " most",
                        "-",
                        "mult",
                        "ir",
                        "acial",
                        " states",
                        " are",
                        " far"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        4.60027551651001,
                        4.536262512207031,
                        14.07706260681152,
                        3.117831945419312,
                        0,
                        0,
                        0,
                        3.95152473449707,
                        12.1328239440918,
                        16.12752151489258,
                        13.64967632293701
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 7,
                    "sentence_string": ", had the sixth-largest economy behind the United States, China, Japan, Germany",
                    "max_token": " behind",
                    "tokens": [
                        ",",
                        " had",
                        " the",
                        " sixth",
                        "-",
                        "largest",
                        " economy",
                        " behind",
                        " the",
                        " United",
                        " States",
                        ",",
                        " China",
                        ",",
                        " Japan",
                        ",",
                        " Germany"
                    ],
                    "values": [
                        5.151651382446289,
                        4.684551239013672,
                        5.277929306030273,
                        4.15180778503418,
                        4.45219087600708,
                        1.805790543556213,
                        1.370615720748901,
                        12.51126670837402,
                        12.31405067443848,
                        5.780858516693115,
                        10.09518432617188,
                        12.15318965911865,
                        5.971001625061035,
                        8.415839195251465,
                        0,
                        3.064034700393677,
                        0.598114013671875
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "\u00e2\u0122\u013bThe<|endoftext|> potential impact surveys and mitigation and restoration plans\u00e2\u0122\u013b\u00e2\u0122\u013b are",
                    "tokens": [
                        "\u00e2\u0122",
                        "\u013b",
                        "The",
                        "<|endoftext|>",
                        " potential",
                        " impact",
                        " surveys",
                        " and",
                        " mitigation",
                        " and",
                        " restoration",
                        " plans",
                        "\u00e2\u0122",
                        "\u013b",
                        "\u00e2\u0122",
                        "\u013b",
                        " are"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " that kid who felt unwanted, or the grown up who remembers how hard it was to",
                    "tokens": [
                        " that",
                        " kid",
                        " who",
                        " felt",
                        " unwanted",
                        ",",
                        " or",
                        " the",
                        " grown",
                        " up",
                        " who",
                        " remembers",
                        " how",
                        " hard",
                        " it",
                        " was",
                        " to"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " Trump and Hillary in the first debate, before either said a word, Trump made what",
                    "tokens": [
                        " Trump",
                        " and",
                        " Hillary",
                        " in",
                        " the",
                        " first",
                        " debate",
                        ",",
                        " before",
                        " either",
                        " said",
                        " a",
                        " word",
                        ",",
                        " Trump",
                        " made",
                        " what"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "3 billion) installing everything from traffic-management technologies to smart electricity grids.\u010a\u010a",
                    "tokens": [
                        "3",
                        " billion",
                        ")",
                        " installing",
                        " everything",
                        " from",
                        " traffic",
                        "-",
                        "management",
                        " technologies",
                        " to",
                        " smart",
                        " electricity",
                        " grids",
                        ".",
                        "\u010a",
                        "\u010a"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " city staff (Fire, Police, Crime Prevention, and more)\u010a\u010aOption to",
                    "tokens": [
                        " city",
                        " staff",
                        " (",
                        "Fire",
                        ",",
                        " Police",
                        ",",
                        " Crime",
                        " Prevention",
                        ",",
                        " and",
                        " more",
                        ")",
                        "\u010a",
                        "\u010a",
                        "Option",
                        " to"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 16.49443435668945
        },
        {
            "feature_index": 139,
            "gpt_predictions": [
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "mentions of specific locations, with a focus on New York City",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "-Vice President Dr. Vincent Bourquin in the Interview: \"Digging such a",
                    "max_token": " in",
                    "tokens": [
                        "-",
                        "Vice",
                        " President",
                        " Dr",
                        ".",
                        " Vincent",
                        " Bour",
                        "quin",
                        " in",
                        " the",
                        " Interview",
                        ":",
                        " \"",
                        "Dig",
                        "ging",
                        " such",
                        " a"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        11.19366073608398,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "\u010aHowever, Gray noted that Washington state in 2013 admonished a judge who said he",
                    "max_token": " in",
                    "tokens": [
                        "\u010a",
                        "However",
                        ",",
                        " Gray",
                        " noted",
                        " that",
                        " Washington",
                        " state",
                        " in",
                        " 2013",
                        " admon",
                        "ished",
                        " a",
                        " judge",
                        " who",
                        " said",
                        " he"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        20.91598892211914,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " you will definitely enjoy the tune-age in this film.\u010a\u010a\"This ice",
                    "max_token": " in",
                    "tokens": [
                        " you",
                        " will",
                        " definitely",
                        " enjoy",
                        " the",
                        " tune",
                        "-",
                        "age",
                        " in",
                        " this",
                        " film",
                        ".",
                        "\u010a",
                        "\u010a",
                        "\"",
                        "This",
                        " ice"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        8.040589332580566,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " its opposite, could be right. Politics in India is big and messy: hundreds of",
                    "max_token": " in",
                    "tokens": [
                        " its",
                        " opposite",
                        ",",
                        " could",
                        " be",
                        " right",
                        ".",
                        " Politics",
                        " in",
                        " India",
                        " is",
                        " big",
                        " and",
                        " messy",
                        ":",
                        " hundreds",
                        " of"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        15.40050983428955,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "<|endoftext|>Discussion<|endoftext|> ultimately relents. Trump in September began issuing credentials to outlets he had",
                    "max_token": " in",
                    "tokens": [
                        "<|endoftext|>",
                        "Discussion",
                        "<|endoftext|>",
                        " ultimately",
                        " rel",
                        "ents",
                        ".",
                        " Trump",
                        " in",
                        " September",
                        " began",
                        " issuing",
                        " credentials",
                        " to",
                        " outlets",
                        " he",
                        " had"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        21.02948188781738,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "ESPN, UniMas, UDN) before facing the 2014 FIFA World Cup<|endoftext|> can",
                    "tokens": [
                        "ESPN",
                        ",",
                        " Uni",
                        "Mas",
                        ",",
                        " U",
                        "DN",
                        ")",
                        " before",
                        " facing",
                        " the",
                        " 2014",
                        " FIFA",
                        " World",
                        " Cup",
                        "<|endoftext|>",
                        " can"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " vaccine industry, with the assistance of a sold-out government, is forcing vaccinations on",
                    "tokens": [
                        " vaccine",
                        " industry",
                        ",",
                        " with",
                        " the",
                        " assistance",
                        " of",
                        " a",
                        " sold",
                        "-",
                        "out",
                        " government",
                        ",",
                        " is",
                        " forcing",
                        " vaccinations",
                        " on"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": ", build and maintain effective student groups, advertise and rebrand conservative values, engage in",
                    "tokens": [
                        ",",
                        " build",
                        " and",
                        " maintain",
                        " effective",
                        " student",
                        " groups",
                        ",",
                        " advertise",
                        " and",
                        " re",
                        "brand",
                        " conservative",
                        " values",
                        ",",
                        " engage",
                        " in"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " political protest\u010a\u010aUpdated\u010a\u010aIt was one of the great art heists of",
                    "tokens": [
                        " political",
                        " protest",
                        "\u010a",
                        "\u010a",
                        "Updated",
                        "\u010a",
                        "\u010a",
                        "It",
                        " was",
                        " one",
                        " of",
                        " the",
                        " great",
                        " art",
                        " he",
                        "ists",
                        " of"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " that kid who felt unwanted, or the grown up who remembers how hard it was to",
                    "tokens": [
                        " that",
                        " kid",
                        " who",
                        " felt",
                        " unwanted",
                        ",",
                        " or",
                        " the",
                        " grown",
                        " up",
                        " who",
                        " remembers",
                        " how",
                        " hard",
                        " it",
                        " was",
                        " to"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 28.37088012695312
        },
        {
            "feature_index": 621,
            "gpt_predictions": [
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "phrases related to incidents or events involving violence or law enforcement",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 2,
                    "sentence_string": ".\u010a\u010aArboreal origins\u010a\u010aIt\u00e2\u0122\u013bs unclear how<|endoftext|>i",
                    "max_token": "\u010a",
                    "tokens": [
                        ".",
                        "\u010a",
                        "\u010a",
                        "Ar",
                        "b",
                        "oreal",
                        " origins",
                        "\u010a",
                        "\u010a",
                        "It",
                        "\u00e2\u0122",
                        "\u013b",
                        "s",
                        " unclear",
                        " how",
                        "<|endoftext|>",
                        "i"
                    ],
                    "values": [
                        0,
                        0,
                        5.532754898071289,
                        0,
                        0,
                        0,
                        0,
                        0,
                        4.658682823181152,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "ching had happened on his doorstep.\u010a\u010a\u00e2\u0122\u013eI am aware of the motorcycle",
                    "max_token": "\u010a",
                    "tokens": [
                        "ching",
                        " had",
                        " happened",
                        " on",
                        " his",
                        " doorstep",
                        ".",
                        "\u010a",
                        "\u010a",
                        "\u00e2\u0122",
                        "\u013e",
                        "I",
                        " am",
                        " aware",
                        " of",
                        " the",
                        " motorcycle"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        9.063117980957031,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " of Market neighborhood, police said.\u010a\u010aAround 7:30 p.m.",
                    "max_token": "\u010a",
                    "tokens": [
                        " of",
                        " Market",
                        " neighborhood",
                        ",",
                        " police",
                        " said",
                        ".",
                        "\u010a",
                        "\u010a",
                        "Around",
                        " 7",
                        ":",
                        "30",
                        " p",
                        ".",
                        "m",
                        "."
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        24.4411792755127,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " (Photo: Phoenix Police Department)\u010a\u010aLewis said gunfire erupted after the shooter forced",
                    "max_token": "\u010a",
                    "tokens": [
                        " (",
                        "Photo",
                        ":",
                        " Phoenix",
                        " Police",
                        " Department",
                        ")",
                        "\u010a",
                        "\u010a",
                        "Lewis",
                        " said",
                        " gunfire",
                        " erupted",
                        " after",
                        " the",
                        " shooter",
                        " forced"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        25.48786163330078,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " before he was set on fire.\u010a\u010aNeighbors were shocked.\u010a\u010a\"",
                    "max_token": "\u010a",
                    "tokens": [
                        " before",
                        " he",
                        " was",
                        " set",
                        " on",
                        " fire",
                        ".",
                        "\u010a",
                        "\u010a",
                        "Neigh",
                        "bors",
                        " were",
                        " shocked",
                        ".",
                        "\u010a",
                        "\u010a",
                        "\""
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        1.602504134178162,
                        31.96051216125488,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        14.76782417297363,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " quick lead only to have to bow out before the end of the game because they went",
                    "tokens": [
                        " quick",
                        " lead",
                        " only",
                        " to",
                        " have",
                        " to",
                        " bow",
                        " out",
                        " before",
                        " the",
                        " end",
                        " of",
                        " the",
                        " game",
                        " because",
                        " they",
                        " went"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "\u010aAldrin was a drug user before, Castillo admitted, but he had",
                    "tokens": [
                        "\u010a",
                        "A",
                        "ld",
                        "rin",
                        " was",
                        " a",
                        " drug",
                        " user",
                        " before",
                        ",",
                        " Cast",
                        "illo",
                        " admitted",
                        ",",
                        " but",
                        " he",
                        " had"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " height at some international summit.\u010a\u010aOr is it because of these deeply controversial statements",
                    "tokens": [
                        " height",
                        " at",
                        " some",
                        " international",
                        " summit",
                        ".",
                        "\u010a",
                        "\u010a",
                        "Or",
                        " is",
                        " it",
                        " because",
                        " of",
                        " these",
                        " deeply",
                        " controversial",
                        " statements"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " years after he was jailed for espionage, before his release in 1969.\u010a\u010aHe",
                    "tokens": [
                        " years",
                        " after",
                        " he",
                        " was",
                        " jailed",
                        " for",
                        " espionage",
                        ",",
                        " before",
                        " his",
                        " release",
                        " in",
                        " 1969",
                        ".",
                        "\u010a",
                        "\u010a",
                        "He"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " faces with respect to women.\u010a\u010aA CNN poll released Monday showed Obama moving into",
                    "tokens": [
                        " faces",
                        " with",
                        " respect",
                        " to",
                        " women",
                        ".",
                        "\u010a",
                        "\u010a",
                        "A",
                        " CNN",
                        " poll",
                        " released",
                        " Monday",
                        " showed",
                        " Obama",
                        " moving",
                        " into"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 35.89005279541016
        },
        {
            "feature_index": 499,
            "gpt_predictions": [
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "words related to technology and companies, as well as names of specific technological devices and software",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " two owners of Romanian football team Dinamo Bucharest, was also cited as a victim",
                    "max_token": " Buch",
                    "tokens": [
                        " two",
                        " owners",
                        " of",
                        " Romanian",
                        " football",
                        " team",
                        " Din",
                        "amo",
                        " Buch",
                        "arest",
                        ",",
                        " was",
                        " also",
                        " cited",
                        " as",
                        " a",
                        " victim"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        6.659522533416748,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " Arizona Public Service and Denver-based Xcel Energy, have asked their state regulators to",
                    "max_token": "cel",
                    "tokens": [
                        " Arizona",
                        " Public",
                        " Service",
                        " and",
                        " Denver",
                        "-",
                        "based",
                        " X",
                        "cel",
                        " Energy",
                        ",",
                        " have",
                        " asked",
                        " their",
                        " state",
                        " regulators",
                        " to"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        13.20750141143799,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " E-Ink screen, where previous Kindle displays have been slightly recessed.\u010a",
                    "max_token": " Kindle",
                    "tokens": [
                        " E",
                        "-",
                        "In",
                        "k",
                        " screen",
                        ",",
                        " where",
                        " previous",
                        " Kindle",
                        " displays",
                        " have",
                        " been",
                        " slightly",
                        " rec",
                        "essed",
                        ".",
                        "\u010a"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        11.65620708465576,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "e a una tropa encarcelada de asesinos, mare",
                    "max_token": "cel",
                    "tokens": [
                        "e",
                        " a",
                        " un",
                        "a",
                        " trop",
                        "a",
                        " enc",
                        "ar",
                        "cel",
                        "ada",
                        " de",
                        " as",
                        "es",
                        "inos",
                        ",",
                        " m",
                        "are"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        11.09699535369873,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " take action on DACA.\u010a\u010aCurbelo, who earlier this month butted",
                    "max_token": "urb",
                    "tokens": [
                        " take",
                        " action",
                        " on",
                        " DACA",
                        ".",
                        "\u010a",
                        "\u010a",
                        "C",
                        "urb",
                        "elo",
                        ",",
                        " who",
                        " earlier",
                        " this",
                        " month",
                        " but",
                        "ted"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        7.147871494293213,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " quick lead only to have to bow out before the end of the game because they went",
                    "tokens": [
                        " quick",
                        " lead",
                        " only",
                        " to",
                        " have",
                        " to",
                        " bow",
                        " out",
                        " before",
                        " the",
                        " end",
                        " of",
                        " the",
                        " game",
                        " because",
                        " they",
                        " went"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "\u010aAldrin was a drug user before, Castillo admitted, but he had",
                    "tokens": [
                        "\u010a",
                        "A",
                        "ld",
                        "rin",
                        " was",
                        " a",
                        " drug",
                        " user",
                        " before",
                        ",",
                        " Cast",
                        "illo",
                        " admitted",
                        ",",
                        " but",
                        " he",
                        " had"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " height at some international summit.\u010a\u010aOr is it because of these deeply controversial statements",
                    "tokens": [
                        " height",
                        " at",
                        " some",
                        " international",
                        " summit",
                        ".",
                        "\u010a",
                        "\u010a",
                        "Or",
                        " is",
                        " it",
                        " because",
                        " of",
                        " these",
                        " deeply",
                        " controversial",
                        " statements"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " years after he was jailed for espionage, before his release in 1969.\u010a\u010aHe",
                    "tokens": [
                        " years",
                        " after",
                        " he",
                        " was",
                        " jailed",
                        " for",
                        " espionage",
                        ",",
                        " before",
                        " his",
                        " release",
                        " in",
                        " 1969",
                        ".",
                        "\u010a",
                        "\u010a",
                        "He"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " faces with respect to women.\u010a\u010aA CNN poll released Monday showed Obama moving into",
                    "tokens": [
                        " faces",
                        " with",
                        " respect",
                        " to",
                        " women",
                        ".",
                        "\u010a",
                        "\u010a",
                        "A",
                        " CNN",
                        " poll",
                        " released",
                        " Monday",
                        " showed",
                        " Obama",
                        " moving",
                        " into"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 32.93782043457031
        },
        {
            "feature_index": 370,
            "gpt_predictions": [
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    1.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "instances where something is considered problematic or controversial",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 7,
                    "sentence_string": " \"I did not take steps to stop the CIA\u00e2\u0122\u013bs use of those techniques",
                    "max_token": " stop",
                    "tokens": [
                        " \"",
                        "I",
                        " did",
                        " not",
                        " take",
                        " steps",
                        " to",
                        " stop",
                        " the",
                        " CIA",
                        "\u00e2\u0122",
                        "\u013b",
                        "s",
                        " use",
                        " of",
                        " those",
                        " techniques"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0.380435585975647,
                        0,
                        0,
                        8.705588340759277,
                        3.879846572875977,
                        0.2443419992923737,
                        0,
                        0,
                        0.4724400043487549,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 7,
                    "sentence_string": " interfere with the Office's investigations or prevent it from doing its job,\" he later added",
                    "max_token": " prevent",
                    "tokens": [
                        " interfere",
                        " with",
                        " the",
                        " Office",
                        "'s",
                        " investigations",
                        " or",
                        " prevent",
                        " it",
                        " from",
                        " doing",
                        " its",
                        " job",
                        ",\"",
                        " he",
                        " later",
                        " added"
                    ],
                    "values": [
                        5.75063419342041,
                        14.56009864807129,
                        8.069045066833496,
                        3.322957754135132,
                        6.773427486419678,
                        2.191186904907227,
                        0,
                        15.40006637573242,
                        7.736788272857666,
                        11.44422721862793,
                        1.332419157028198,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " shadows\u00e2\u0122\u013f.\u010a\u010aCan you blame them? https://t.co/",
                    "max_token": " blame",
                    "tokens": [
                        " shadows",
                        "\u00e2\u0122",
                        "\u013f",
                        ".",
                        "\u010a",
                        "\u010a",
                        "Can",
                        " you",
                        " blame",
                        " them",
                        "?",
                        " https",
                        "://",
                        "t",
                        ".",
                        "co",
                        "/"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        5.778809547424316,
                        0.4264005124568939,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": ", and it's not like you can fault them for their distrust. That will be",
                    "max_token": " fault",
                    "tokens": [
                        ",",
                        " and",
                        " it",
                        "'s",
                        " not",
                        " like",
                        " you",
                        " can",
                        " fault",
                        " them",
                        " for",
                        " their",
                        " distrust",
                        ".",
                        " That",
                        " will",
                        " be"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        11.72350120544434,
                        6.467411994934082,
                        11.14633464813232,
                        6.372180461883545,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " cleaned twice a day. I have nothing against the SPCA. They were doing",
                    "max_token": " against",
                    "tokens": [
                        " cleaned",
                        " twice",
                        " a",
                        " day",
                        ".",
                        " I",
                        " have",
                        " nothing",
                        " against",
                        " the",
                        " S",
                        "PC",
                        "A",
                        ".",
                        " They",
                        " were",
                        " doing"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        15.43160629272461,
                        10.54526901245117,
                        0,
                        0,
                        4.234391212463379,
                        0.1397421211004257,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "\u00e2\u0122\u013bThe<|endoftext|> potential impact surveys and mitigation and restoration plans\u00e2\u0122\u013b\u00e2\u0122\u013b are",
                    "tokens": [
                        "\u00e2\u0122",
                        "\u013b",
                        "The",
                        "<|endoftext|>",
                        " potential",
                        " impact",
                        " surveys",
                        " and",
                        " mitigation",
                        " and",
                        " restoration",
                        " plans",
                        "\u00e2\u0122",
                        "\u013b",
                        "\u00e2\u0122",
                        "\u013b",
                        " are"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " that kid who felt unwanted, or the grown up who remembers how hard it was to",
                    "tokens": [
                        " that",
                        " kid",
                        " who",
                        " felt",
                        " unwanted",
                        ",",
                        " or",
                        " the",
                        " grown",
                        " up",
                        " who",
                        " remembers",
                        " how",
                        " hard",
                        " it",
                        " was",
                        " to"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " Trump and Hillary in the first debate, before either said a word, Trump made what",
                    "tokens": [
                        " Trump",
                        " and",
                        " Hillary",
                        " in",
                        " the",
                        " first",
                        " debate",
                        ",",
                        " before",
                        " either",
                        " said",
                        " a",
                        " word",
                        ",",
                        " Trump",
                        " made",
                        " what"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "3 billion) installing everything from traffic-management technologies to smart electricity grids.\u010a\u010a",
                    "tokens": [
                        "3",
                        " billion",
                        ")",
                        " installing",
                        " everything",
                        " from",
                        " traffic",
                        "-",
                        "management",
                        " technologies",
                        " to",
                        " smart",
                        " electricity",
                        " grids",
                        ".",
                        "\u010a",
                        "\u010a"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " city staff (Fire, Police, Crime Prevention, and more)\u010a\u010aOption to",
                    "tokens": [
                        " city",
                        " staff",
                        " (",
                        "Fire",
                        ",",
                        " Police",
                        ",",
                        " Crime",
                        " Prevention",
                        ",",
                        " and",
                        " more",
                        ")",
                        "\u010a",
                        "\u010a",
                        "Option",
                        " to"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 19.22408485412598
        },
        {
            "feature_index": 198,
            "gpt_predictions": [
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "proper names starting with the letter \"B\" associated with various events, individuals, or places",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " series comes from producers Matt Damon and Ben Affleck, and is set in a near",
                    "max_token": " Aff",
                    "tokens": [
                        " series",
                        " comes",
                        " from",
                        " producers",
                        " Matt",
                        " Damon",
                        " and",
                        " Ben",
                        " Aff",
                        "leck",
                        ",",
                        " and",
                        " is",
                        " set",
                        " in",
                        " a",
                        " near"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        23.88891983032227,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " army has slowed their progression somewhat and Benitez has used the recent run of results",
                    "max_token": "ite",
                    "tokens": [
                        " army",
                        " has",
                        " slowed",
                        " their",
                        " progression",
                        " somewhat",
                        " and",
                        " Ben",
                        "ite",
                        "z",
                        " has",
                        " used",
                        " the",
                        " recent",
                        " run",
                        " of",
                        " results"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        30.46777534484863,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " in research and development, Senator Paulo Benigno \"Bam\" Aquino said",
                    "max_token": "ign",
                    "tokens": [
                        " in",
                        " research",
                        " and",
                        " development",
                        ",",
                        " Senator",
                        " Paulo",
                        " Ben",
                        "ign",
                        "o",
                        " \"",
                        "B",
                        "am",
                        "\"",
                        " Aqu",
                        "ino",
                        " said"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        29.21757888793945,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " (in blackface!) explains to Ben Stiller that you \u00e2\u0122\u013enever go full",
                    "max_token": " St",
                    "tokens": [
                        " (",
                        "in",
                        " black",
                        "face",
                        "!)",
                        " explains",
                        " to",
                        " Ben",
                        " St",
                        "iller",
                        " that",
                        " you",
                        " \u00e2\u0122",
                        "\u013e",
                        "never",
                        " go",
                        " full"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        37.80802536010742,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " hit from Director of Game Theory, Ben Stoll.\u010a\u010aHi HEXers",
                    "max_token": " St",
                    "tokens": [
                        " hit",
                        " from",
                        " Director",
                        " of",
                        " Game",
                        " Theory",
                        ",",
                        " Ben",
                        " St",
                        "oll",
                        ".",
                        "\u010a",
                        "\u010a",
                        "Hi",
                        " H",
                        "EX",
                        "ers"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        33.4713249206543,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "ESPN, UniMas, UDN) before facing the 2014 FIFA World Cup<|endoftext|> can",
                    "tokens": [
                        "ESPN",
                        ",",
                        " Uni",
                        "Mas",
                        ",",
                        " U",
                        "DN",
                        ")",
                        " before",
                        " facing",
                        " the",
                        " 2014",
                        " FIFA",
                        " World",
                        " Cup",
                        "<|endoftext|>",
                        " can"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " vaccine industry, with the assistance of a sold-out government, is forcing vaccinations on",
                    "tokens": [
                        " vaccine",
                        " industry",
                        ",",
                        " with",
                        " the",
                        " assistance",
                        " of",
                        " a",
                        " sold",
                        "-",
                        "out",
                        " government",
                        ",",
                        " is",
                        " forcing",
                        " vaccinations",
                        " on"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": ", build and maintain effective student groups, advertise and rebrand conservative values, engage in",
                    "tokens": [
                        ",",
                        " build",
                        " and",
                        " maintain",
                        " effective",
                        " student",
                        " groups",
                        ",",
                        " advertise",
                        " and",
                        " re",
                        "brand",
                        " conservative",
                        " values",
                        ",",
                        " engage",
                        " in"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " political protest\u010a\u010aUpdated\u010a\u010aIt was one of the great art heists of",
                    "tokens": [
                        " political",
                        " protest",
                        "\u010a",
                        "\u010a",
                        "Updated",
                        "\u010a",
                        "\u010a",
                        "It",
                        " was",
                        " one",
                        " of",
                        " the",
                        " great",
                        " art",
                        " he",
                        "ists",
                        " of"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " that kid who felt unwanted, or the grown up who remembers how hard it was to",
                    "tokens": [
                        " that",
                        " kid",
                        " who",
                        " felt",
                        " unwanted",
                        ",",
                        " or",
                        " the",
                        " grown",
                        " up",
                        " who",
                        " remembers",
                        " how",
                        " hard",
                        " it",
                        " was",
                        " to"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 52.58736801147461
        },
        {
            "feature_index": 687,
            "gpt_predictions": [
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "mentions of the news network \"Al Jazeera\"",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " which occurred as the high school hosted a TEDxYouth event on Friday. The",
                    "max_token": " TED",
                    "tokens": [
                        " which",
                        " occurred",
                        " as",
                        " the",
                        " high",
                        " school",
                        " hosted",
                        " a",
                        " TED",
                        "x",
                        "Y",
                        "outh",
                        " event",
                        " on",
                        " Friday",
                        ".",
                        " The"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        15.1087589263916,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " 15-bed detainee hospital, told Al Jazeera that visits to the prison by outside physicians",
                    "max_token": " Jazeera",
                    "tokens": [
                        " 15",
                        "-",
                        "bed",
                        " detainee",
                        " hospital",
                        ",",
                        " told",
                        " Al",
                        " Jazeera",
                        " that",
                        " visits",
                        " to",
                        " the",
                        " prison",
                        " by",
                        " outside",
                        " physicians"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0.1694917976856232,
                        0,
                        0,
                        0,
                        0,
                        45.20980453491211,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "It has been one year since three Al Jazeera journalists were arrested in Egypt in a case",
                    "max_token": " Jazeera",
                    "tokens": [
                        "It",
                        " has",
                        " been",
                        " one",
                        " year",
                        " since",
                        " three",
                        " Al",
                        " Jazeera",
                        " journalists",
                        " were",
                        " arrested",
                        " in",
                        " Egypt",
                        " in",
                        " a",
                        " case"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        46.62572479248047,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " that venerable CBS sleazefest 60 Minutes, profiled me in March 1994,",
                    "max_token": " Minutes",
                    "tokens": [
                        " that",
                        " venerable",
                        " CBS",
                        " sle",
                        "az",
                        "ef",
                        "est",
                        " 60",
                        " Minutes",
                        ",",
                        " prof",
                        "iled",
                        " me",
                        " in",
                        " March",
                        " 1994",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        7.760707855224609,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " caption Robert F. Sisson/National Geographic/Getty Images Robert F. Sisson",
                    "max_token": " Geographic",
                    "tokens": [
                        " caption",
                        " Robert",
                        " F",
                        ".",
                        " S",
                        "isson",
                        "/",
                        "National",
                        " Geographic",
                        "/",
                        "Getty",
                        " Images",
                        " Robert",
                        " F",
                        ".",
                        " S",
                        "isson"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        12.24864101409912,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " quick lead only to have to bow out before the end of the game because they went",
                    "tokens": [
                        " quick",
                        " lead",
                        " only",
                        " to",
                        " have",
                        " to",
                        " bow",
                        " out",
                        " before",
                        " the",
                        " end",
                        " of",
                        " the",
                        " game",
                        " because",
                        " they",
                        " went"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "\u010aAldrin was a drug user before, Castillo admitted, but he had",
                    "tokens": [
                        "\u010a",
                        "A",
                        "ld",
                        "rin",
                        " was",
                        " a",
                        " drug",
                        " user",
                        " before",
                        ",",
                        " Cast",
                        "illo",
                        " admitted",
                        ",",
                        " but",
                        " he",
                        " had"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " height at some international summit.\u010a\u010aOr is it because of these deeply controversial statements",
                    "tokens": [
                        " height",
                        " at",
                        " some",
                        " international",
                        " summit",
                        ".",
                        "\u010a",
                        "\u010a",
                        "Or",
                        " is",
                        " it",
                        " because",
                        " of",
                        " these",
                        " deeply",
                        " controversial",
                        " statements"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " years after he was jailed for espionage, before his release in 1969.\u010a\u010aHe",
                    "tokens": [
                        " years",
                        " after",
                        " he",
                        " was",
                        " jailed",
                        " for",
                        " espionage",
                        ",",
                        " before",
                        " his",
                        " release",
                        " in",
                        " 1969",
                        ".",
                        "\u010a",
                        "\u010a",
                        "He"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " faces with respect to women.\u010a\u010aA CNN poll released Monday showed Obama moving into",
                    "tokens": [
                        " faces",
                        " with",
                        " respect",
                        " to",
                        " women",
                        ".",
                        "\u010a",
                        "\u010a",
                        "A",
                        " CNN",
                        " poll",
                        " released",
                        " Monday",
                        " showed",
                        " Obama",
                        " moving",
                        " into"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 55.45495986938477
        },
        {
            "feature_index": 584,
            "gpt_predictions": [
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "political party names and related terms",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " any sane universe of political discourse, a party that connived in the nomination of a",
                    "max_token": " party",
                    "tokens": [
                        " any",
                        " sane",
                        " universe",
                        " of",
                        " political",
                        " discourse",
                        ",",
                        " a",
                        " party",
                        " that",
                        " conn",
                        "ived",
                        " in",
                        " the",
                        " nomination",
                        " of",
                        " a"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0.4667502045631409,
                        0,
                        0,
                        0,
                        17.89693450927734,
                        1.942716002464294,
                        0,
                        0,
                        0,
                        0,
                        0.2928691506385803,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " Sanders and Hillary Clinton wings of the Democratic Party: the former emphasizing economic redistribution, the",
                    "max_token": " Party",
                    "tokens": [
                        " Sanders",
                        " and",
                        " Hillary",
                        " Clinton",
                        " wings",
                        " of",
                        " the",
                        " Democratic",
                        " Party",
                        ":",
                        " the",
                        " former",
                        " emphasizing",
                        " economic",
                        " redistribution",
                        ",",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        7.419122219085693,
                        34.52630615234375,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " there will be plenty members of the minority party staying home on the 18th, preferring",
                    "max_token": " party",
                    "tokens": [
                        " there",
                        " will",
                        " be",
                        " plenty",
                        " members",
                        " of",
                        " the",
                        " minority",
                        " party",
                        " staying",
                        " home",
                        " on",
                        " the",
                        " 18",
                        "th",
                        ",",
                        " preferring"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        10.94637107849121,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " across the aisle?\u010a\u010aThe Republican Party is more diverse than the Left. I",
                    "max_token": " Party",
                    "tokens": [
                        " across",
                        " the",
                        " aisle",
                        "?",
                        "\u010a",
                        "\u010a",
                        "The",
                        " Republican",
                        " Party",
                        " is",
                        " more",
                        " diverse",
                        " than",
                        " the",
                        " Left",
                        ".",
                        " I"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        1.163267135620117,
                        36.66074752807617,
                        4.327189445495605,
                        0,
                        0,
                        0,
                        0,
                        1.739980936050415,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": ", \u00e2\u0122\u013eThis is called the Republican Party. It\u00e2\u0122\u013bs not called the",
                    "max_token": " Party",
                    "tokens": [
                        ",",
                        " \u00e2\u0122",
                        "\u013e",
                        "This",
                        " is",
                        " called",
                        " the",
                        " Republican",
                        " Party",
                        ".",
                        " It",
                        "\u00e2\u0122",
                        "\u013b",
                        "s",
                        " not",
                        " called",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        9.022576332092285,
                        36.34481811523438,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " quick lead only to have to bow out before the end of the game because they went",
                    "tokens": [
                        " quick",
                        " lead",
                        " only",
                        " to",
                        " have",
                        " to",
                        " bow",
                        " out",
                        " before",
                        " the",
                        " end",
                        " of",
                        " the",
                        " game",
                        " because",
                        " they",
                        " went"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "\u010aAldrin was a drug user before, Castillo admitted, but he had",
                    "tokens": [
                        "\u010a",
                        "A",
                        "ld",
                        "rin",
                        " was",
                        " a",
                        " drug",
                        " user",
                        " before",
                        ",",
                        " Cast",
                        "illo",
                        " admitted",
                        ",",
                        " but",
                        " he",
                        " had"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " height at some international summit.\u010a\u010aOr is it because of these deeply controversial statements",
                    "tokens": [
                        " height",
                        " at",
                        " some",
                        " international",
                        " summit",
                        ".",
                        "\u010a",
                        "\u010a",
                        "Or",
                        " is",
                        " it",
                        " because",
                        " of",
                        " these",
                        " deeply",
                        " controversial",
                        " statements"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " years after he was jailed for espionage, before his release in 1969.\u010a\u010aHe",
                    "tokens": [
                        " years",
                        " after",
                        " he",
                        " was",
                        " jailed",
                        " for",
                        " espionage",
                        ",",
                        " before",
                        " his",
                        " release",
                        " in",
                        " 1969",
                        ".",
                        "\u010a",
                        "\u010a",
                        "He"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " faces with respect to women.\u010a\u010aA CNN poll released Monday showed Obama moving into",
                    "tokens": [
                        " faces",
                        " with",
                        " respect",
                        " to",
                        " women",
                        ".",
                        "\u010a",
                        "\u010a",
                        "A",
                        " CNN",
                        " poll",
                        " released",
                        " Monday",
                        " showed",
                        " Obama",
                        " moving",
                        " into"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 42.64680480957031
        },
        {
            "feature_index": 901,
            "gpt_predictions": [
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "adjectives describing the quality of a job or task being done",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 7,
                    "sentence_string": " the most part, each fault does its own thing, independently of those around it.",
                    "max_token": " its",
                    "tokens": [
                        " the",
                        " most",
                        " part",
                        ",",
                        " each",
                        " fault",
                        " does",
                        " its",
                        " own",
                        " thing",
                        ",",
                        " independently",
                        " of",
                        " those",
                        " around",
                        " it",
                        "."
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        23.45248603820801,
                        21.893798828125,
                        2.48556923866272,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "He was a big center and he did a great job. I think he was really",
                    "max_token": " a",
                    "tokens": [
                        "He",
                        " was",
                        " a",
                        " big",
                        " center",
                        " and",
                        " he",
                        " did",
                        " a",
                        " great",
                        " job",
                        ".",
                        " I",
                        " think",
                        " he",
                        " was",
                        " really"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        43.86823272705078,
                        40.04322052001953,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " Twitter \u00e2\u0122\u0135 and they\u00e2\u0122\u013bve done brilliantly.\u010a\u010aIndeed. So well,",
                    "max_token": " brilliantly",
                    "tokens": [
                        " Twitter",
                        " \u00e2\u0122\u0135",
                        " and",
                        " they",
                        "\u00e2\u0122",
                        "\u013b",
                        "ve",
                        " done",
                        " brilliantly",
                        ".",
                        "\u010a",
                        "\u010a",
                        "Indeed",
                        ".",
                        " So",
                        " well",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0.2437818944454193,
                        12.51330471038818,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 7,
                    "sentence_string": "\u010a\u010aKemba Walker does a good job of defending Foye, but",
                    "max_token": " a",
                    "tokens": [
                        "\u010a",
                        "\u010a",
                        "K",
                        "em",
                        "ba",
                        " Walker",
                        " does",
                        " a",
                        " good",
                        " job",
                        " of",
                        " defending",
                        " F",
                        "oy",
                        "e",
                        ",",
                        " but"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        49.38801956176758,
                        48.49913787841797,
                        3.758361101150513,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " by her selfishness and the film does a good job at portraying it as a major",
                    "max_token": " a",
                    "tokens": [
                        " by",
                        " her",
                        " selfish",
                        "ness",
                        " and",
                        " the",
                        " film",
                        " does",
                        " a",
                        " good",
                        " job",
                        " at",
                        " portraying",
                        " it",
                        " as",
                        " a",
                        " major"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        47.74206924438477,
                        47.38775634765625,
                        2.97706413269043,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " coat.\u010a\u010aDetective Inspector Steve Christian, from Liverpool CID, said:",
                    "tokens": [
                        " coat",
                        ".",
                        "\u010a",
                        "\u010a",
                        "Det",
                        "ective",
                        " Inspector",
                        " Steve",
                        " Christian",
                        ",",
                        " from",
                        " Liverpool",
                        " C",
                        "ID",
                        ",",
                        " said",
                        ":"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " drop out of the presidential race, according to NBC News ' Andrea Mitchell.\u010a\u010a",
                    "tokens": [
                        " drop",
                        " out",
                        " of",
                        " the",
                        " presidential",
                        " race",
                        ",",
                        " according",
                        " to",
                        " NBC",
                        " News",
                        " '",
                        " Andrea",
                        " Mitchell",
                        ".",
                        "\u010a",
                        "\u010a"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " Rumors that Amazon will soon begin accepting Bitcoin are persistent. If the retail giant does",
                    "tokens": [
                        " Rum",
                        "ors",
                        " that",
                        " Amazon",
                        " will",
                        " soon",
                        " begin",
                        " accepting",
                        " Bitcoin",
                        " are",
                        " persistent",
                        ".",
                        " If",
                        " the",
                        " retail",
                        " giant",
                        " does"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": ", and banking flows was a monthly net TIC (Treasury International Capital)<|endoftext|>",
                    "tokens": [
                        ",",
                        " and",
                        " banking",
                        " flows",
                        " was",
                        " a",
                        " monthly",
                        " net",
                        " T",
                        "IC",
                        " (",
                        "Tre",
                        "asury",
                        " International",
                        " Capital",
                        ")",
                        "<|endoftext|>"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " faces with respect to women.\u010a\u010aA CNN poll released Monday showed Obama moving into",
                    "tokens": [
                        " faces",
                        " with",
                        " respect",
                        " to",
                        " women",
                        ".",
                        "\u010a",
                        "\u010a",
                        "A",
                        " CNN",
                        " poll",
                        " released",
                        " Monday",
                        " showed",
                        " Obama",
                        " moving",
                        " into"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 53.46586608886719
        },
        {
            "feature_index": 59,
            "gpt_predictions": [
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "references to email sharing",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " alerts and special reports. The news and stories that matter, delivered weekday mornings.\u010a",
                    "max_token": " stories",
                    "tokens": [
                        " alerts",
                        " and",
                        " special",
                        " reports",
                        ".",
                        " The",
                        " news",
                        " and",
                        " stories",
                        " that",
                        " matter",
                        ",",
                        " delivered",
                        " weekday",
                        " mornings",
                        ".",
                        "\u010a"
                    ],
                    "values": [
                        4.20653247833252,
                        0,
                        1.058934569358826,
                        3.560750961303711,
                        0.1195794865489006,
                        0.2958556413650513,
                        0.2190287709236145,
                        0,
                        5.876523017883301,
                        0,
                        0.6579366326332092,
                        0,
                        1.465109705924988,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " ability<|endoftext|> Johansen added 18 points in 13 games, ninth in league scoring and first",
                    "max_token": " 13",
                    "tokens": [
                        " ability",
                        "<|endoftext|>",
                        " Joh",
                        "ansen",
                        " added",
                        " 18",
                        " points",
                        " in",
                        " 13",
                        " games",
                        ",",
                        " ninth",
                        " in",
                        " league",
                        " scoring",
                        " and",
                        " first"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        1.255898714065552,
                        3.382923126220703,
                        5.772627830505371,
                        1.654296875,
                        3.57974648475647,
                        0.4104248881340027,
                        3.567639589309692,
                        0.2763160765171051,
                        1.03640878200531,
                        1.324415683746338,
                        0.7011014223098755
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "'s first-round selection (10th overall) in the 2016 NHL Draft. He",
                    "max_token": " overall",
                    "tokens": [
                        "'s",
                        " first",
                        "-",
                        "round",
                        " selection",
                        " (",
                        "10",
                        "th",
                        " overall",
                        ")",
                        " in",
                        " the",
                        " 2016",
                        " NHL",
                        " Draft",
                        ".",
                        " He"
                    ],
                    "values": [
                        0,
                        2.216239213943481,
                        0,
                        0,
                        0.6200358867645264,
                        2.235641717910767,
                        1.251647114753723,
                        4.850188255310059,
                        6.97010612487793,
                        2.142103433609009,
                        2.416465759277344,
                        2.241150617599487,
                        0.7779490351676941,
                        0,
                        0.4610557556152344,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " for<|endoftext|> energy from non-fossil fuels by 2030 regardless of the US decision",
                    "max_token": "il",
                    "tokens": [
                        " for",
                        "<|endoftext|>",
                        " energy",
                        " from",
                        " non",
                        "-",
                        "f",
                        "oss",
                        "il",
                        " fuels",
                        " by",
                        " 2030",
                        " regardless",
                        " of",
                        " the",
                        " US",
                        " decision"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        3.041830778121948,
                        6.201667308807373,
                        0,
                        1.250392556190491,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 9,
                    "sentence_string": "<|endoftext|>. With art by Ivan Ulyanov and Ben Chandler, and music by",
                    "max_token": "ov",
                    "tokens": [
                        "<|endoftext|>",
                        ".",
                        " With",
                        " art",
                        " by",
                        " Ivan",
                        " U",
                        "ly",
                        "an",
                        "ov",
                        " and",
                        " Ben",
                        " Chandler",
                        ",",
                        " and",
                        " music",
                        " by"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        2.213485240936279,
                        2.995278120040894,
                        0,
                        0,
                        0,
                        0,
                        2.699727773666382,
                        0,
                        0.6925396919250488
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "ESPN, UniMas, UDN) before facing the 2014 FIFA World Cup<|endoftext|> can",
                    "tokens": [
                        "ESPN",
                        ",",
                        " Uni",
                        "Mas",
                        ",",
                        " U",
                        "DN",
                        ")",
                        " before",
                        " facing",
                        " the",
                        " 2014",
                        " FIFA",
                        " World",
                        " Cup",
                        "<|endoftext|>",
                        " can"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " vaccine industry, with the assistance of a sold-out government, is forcing vaccinations on",
                    "tokens": [
                        " vaccine",
                        " industry",
                        ",",
                        " with",
                        " the",
                        " assistance",
                        " of",
                        " a",
                        " sold",
                        "-",
                        "out",
                        " government",
                        ",",
                        " is",
                        " forcing",
                        " vaccinations",
                        " on"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": ", build and maintain effective student groups, advertise and rebrand conservative values, engage in",
                    "tokens": [
                        ",",
                        " build",
                        " and",
                        " maintain",
                        " effective",
                        " student",
                        " groups",
                        ",",
                        " advertise",
                        " and",
                        " re",
                        "brand",
                        " conservative",
                        " values",
                        ",",
                        " engage",
                        " in"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " political protest\u010a\u010aUpdated\u010a\u010aIt was one of the great art heists of",
                    "tokens": [
                        " political",
                        " protest",
                        "\u010a",
                        "\u010a",
                        "Updated",
                        "\u010a",
                        "\u010a",
                        "It",
                        " was",
                        " one",
                        " of",
                        " the",
                        " great",
                        " art",
                        " he",
                        "ists",
                        " of"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " that kid who felt unwanted, or the grown up who remembers how hard it was to",
                    "tokens": [
                        " that",
                        " kid",
                        " who",
                        " felt",
                        " unwanted",
                        ",",
                        " or",
                        " the",
                        " grown",
                        " up",
                        " who",
                        " remembers",
                        " how",
                        " hard",
                        " it",
                        " was",
                        " to"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 7.628561019897461
        },
        {
            "feature_index": 328,
            "gpt_predictions": [
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    1.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "terms related to names and specific locations",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " one of Castro's famous statements, Pinarayi said, \"Adieu Com",
                    "max_token": "ar",
                    "tokens": [
                        " one",
                        " of",
                        " Castro",
                        "'s",
                        " famous",
                        " statements",
                        ",",
                        " Pin",
                        "ar",
                        "ay",
                        "i",
                        " said",
                        ",",
                        " \"",
                        "Ad",
                        "ieu",
                        " Com"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        17.10658645629883,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " Marat at his last breath, when Corday and many others were still nearby (",
                    "max_token": " Cord",
                    "tokens": [
                        " Mar",
                        "at",
                        " at",
                        " his",
                        " last",
                        " breath",
                        ",",
                        " when",
                        " Cord",
                        "ay",
                        " and",
                        " many",
                        " others",
                        " were",
                        " still",
                        " nearby",
                        " ("
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        14.99333000183105,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " piano, and other works.\u010a\u010aGranados was an important influence on at",
                    "max_token": "G",
                    "tokens": [
                        " piano",
                        ",",
                        " and",
                        " other",
                        " works",
                        ".",
                        "\u010a",
                        "\u010a",
                        "G",
                        "ran",
                        "ados",
                        " was",
                        " an",
                        " important",
                        " influence",
                        " on",
                        " at"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        5.427258968353271,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "oe passed his days praying at T\u00c5\u012fdai-ji and also at the temples",
                    "max_token": "d",
                    "tokens": [
                        "oe",
                        " passed",
                        " his",
                        " days",
                        " praying",
                        " at",
                        " T",
                        "\u00c5\u012f",
                        "d",
                        "ai",
                        "-",
                        "ji",
                        " and",
                        " also",
                        " at",
                        " the",
                        " temples"
                    ],
                    "values": [
                        3.489696741104126,
                        0,
                        4.666729927062988,
                        9.831226348876953,
                        1.769579529762268,
                        0,
                        0,
                        0,
                        18.21634292602539,
                        0,
                        0,
                        0,
                        0,
                        1.56995415687561,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " that a person or family celebrating the nikah or baraat or organising the",
                    "max_token": "ik",
                    "tokens": [
                        " that",
                        " a",
                        " person",
                        " or",
                        " family",
                        " celebrating",
                        " the",
                        " n",
                        "ik",
                        "ah",
                        " or",
                        " bar",
                        "a",
                        "at",
                        " or",
                        " organising",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        13.75096893310547,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "\u00e2\u0122\u013bThe<|endoftext|> potential impact surveys and mitigation and restoration plans\u00e2\u0122\u013b\u00e2\u0122\u013b are",
                    "tokens": [
                        "\u00e2\u0122",
                        "\u013b",
                        "The",
                        "<|endoftext|>",
                        " potential",
                        " impact",
                        " surveys",
                        " and",
                        " mitigation",
                        " and",
                        " restoration",
                        " plans",
                        "\u00e2\u0122",
                        "\u013b",
                        "\u00e2\u0122",
                        "\u013b",
                        " are"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " that kid who felt unwanted, or the grown up who remembers how hard it was to",
                    "tokens": [
                        " that",
                        " kid",
                        " who",
                        " felt",
                        " unwanted",
                        ",",
                        " or",
                        " the",
                        " grown",
                        " up",
                        " who",
                        " remembers",
                        " how",
                        " hard",
                        " it",
                        " was",
                        " to"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " Trump and Hillary in the first debate, before either said a word, Trump made what",
                    "tokens": [
                        " Trump",
                        " and",
                        " Hillary",
                        " in",
                        " the",
                        " first",
                        " debate",
                        ",",
                        " before",
                        " either",
                        " said",
                        " a",
                        " word",
                        ",",
                        " Trump",
                        " made",
                        " what"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "3 billion) installing everything from traffic-management technologies to smart electricity grids.\u010a\u010a",
                    "tokens": [
                        "3",
                        " billion",
                        ")",
                        " installing",
                        " everything",
                        " from",
                        " traffic",
                        "-",
                        "management",
                        " technologies",
                        " to",
                        " smart",
                        " electricity",
                        " grids",
                        ".",
                        "\u010a",
                        "\u010a"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " city staff (Fire, Police, Crime Prevention, and more)\u010a\u010aOption to",
                    "tokens": [
                        " city",
                        " staff",
                        " (",
                        "Fire",
                        ",",
                        " Police",
                        ",",
                        " Crime",
                        " Prevention",
                        ",",
                        " and",
                        " more",
                        ")",
                        "\u010a",
                        "\u010a",
                        "Option",
                        " to"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 25.62469482421875
        },
        {
            "feature_index": 96,
            "gpt_predictions": [
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "mentions of people in different professional roles or positions, potentially in quotes or attributed statements",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " him?\u00e2\u0122\u013f asked Bill Lacy, a GOP veteran who ran presidential campaigns for",
                    "max_token": ",",
                    "tokens": [
                        " him",
                        "?",
                        "\u00e2\u0122",
                        "\u013f",
                        " asked",
                        " Bill",
                        " L",
                        "acy",
                        ",",
                        " a",
                        " GOP",
                        " veteran",
                        " who",
                        " ran",
                        " presidential",
                        " campaigns",
                        " for"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        24.12298583984375,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "<|endoftext|> problem,\u00e2\u0122\u013f said Erin Rank, President and CEO of Habitat for Humanity",
                    "max_token": ",",
                    "tokens": [
                        "<|endoftext|>",
                        " problem",
                        ",",
                        "\u00e2\u0122",
                        "\u013f",
                        " said",
                        " Erin",
                        " Rank",
                        ",",
                        " President",
                        " and",
                        " CEO",
                        " of",
                        " Habit",
                        "at",
                        " for",
                        " Humanity"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        35.47389602661133,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " is constantly denied,\" Victor Panyella, a professor taking part in the rally,",
                    "max_token": ",",
                    "tokens": [
                        " is",
                        " constantly",
                        " denied",
                        ",\"",
                        " Victor",
                        " P",
                        "any",
                        "ella",
                        ",",
                        " a",
                        " professor",
                        " taking",
                        " part",
                        " in",
                        " the",
                        " rally",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        9.432891845703125,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " Republican Party,\u00e2\u0122\u013f said Bill Burton, a Democratic strategist. \u00e2\u0122\u013eTo the",
                    "max_token": ",",
                    "tokens": [
                        " Republican",
                        " Party",
                        ",",
                        "\u00e2\u0122",
                        "\u013f",
                        " said",
                        " Bill",
                        " Burton",
                        ",",
                        " a",
                        " Democratic",
                        " strategist",
                        ".",
                        " \u00e2\u0122",
                        "\u013e",
                        "To",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        37.33058929443359,
                        3.183883666992188,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "<|endoftext|> system,\u00e2\u0122\u013f said Jason Baron, a former director of litigation at the Archives",
                    "max_token": ",",
                    "tokens": [
                        "<|endoftext|>",
                        " system",
                        ",",
                        "\u00e2\u0122",
                        "\u013f",
                        " said",
                        " Jason",
                        " Baron",
                        ",",
                        " a",
                        " former",
                        " director",
                        " of",
                        " litigation",
                        " at",
                        " the",
                        " Archives"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        36.78835678100586,
                        2.625023603439331,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "ESPN, UniMas, UDN) before facing the 2014 FIFA World Cup<|endoftext|> can",
                    "tokens": [
                        "ESPN",
                        ",",
                        " Uni",
                        "Mas",
                        ",",
                        " U",
                        "DN",
                        ")",
                        " before",
                        " facing",
                        " the",
                        " 2014",
                        " FIFA",
                        " World",
                        " Cup",
                        "<|endoftext|>",
                        " can"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " vaccine industry, with the assistance of a sold-out government, is forcing vaccinations on",
                    "tokens": [
                        " vaccine",
                        " industry",
                        ",",
                        " with",
                        " the",
                        " assistance",
                        " of",
                        " a",
                        " sold",
                        "-",
                        "out",
                        " government",
                        ",",
                        " is",
                        " forcing",
                        " vaccinations",
                        " on"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": ", build and maintain effective student groups, advertise and rebrand conservative values, engage in",
                    "tokens": [
                        ",",
                        " build",
                        " and",
                        " maintain",
                        " effective",
                        " student",
                        " groups",
                        ",",
                        " advertise",
                        " and",
                        " re",
                        "brand",
                        " conservative",
                        " values",
                        ",",
                        " engage",
                        " in"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " political protest\u010a\u010aUpdated\u010a\u010aIt was one of the great art heists of",
                    "tokens": [
                        " political",
                        " protest",
                        "\u010a",
                        "\u010a",
                        "Updated",
                        "\u010a",
                        "\u010a",
                        "It",
                        " was",
                        " one",
                        " of",
                        " the",
                        " great",
                        " art",
                        " he",
                        "ists",
                        " of"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " that kid who felt unwanted, or the grown up who remembers how hard it was to",
                    "tokens": [
                        " that",
                        " kid",
                        " who",
                        " felt",
                        " unwanted",
                        ",",
                        " or",
                        " the",
                        " grown",
                        " up",
                        " who",
                        " remembers",
                        " how",
                        " hard",
                        " it",
                        " was",
                        " to"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 39.72341156005859
        },
        {
            "feature_index": 312,
            "gpt_predictions": [
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "mentions of \"Disney\" related content",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": ". There's a reason Scar is considered Disney's most evil villain, and it's",
                    "max_token": " Disney",
                    "tokens": [
                        ".",
                        " There",
                        "'s",
                        " a",
                        " reason",
                        " Scar",
                        " is",
                        " considered",
                        " Disney",
                        "'s",
                        " most",
                        " evil",
                        " villain",
                        ",",
                        " and",
                        " it",
                        "'s"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        48.84595489501953,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "\u010a\u010aIt\u00e2\u0122\u013bs my first Disney writing project, first Marvel comic, and",
                    "max_token": " Disney",
                    "tokens": [
                        "\u010a",
                        "\u010a",
                        "It",
                        "\u00e2\u0122",
                        "\u013b",
                        "s",
                        " my",
                        " first",
                        " Disney",
                        " writing",
                        " project",
                        ",",
                        " first",
                        " Marvel",
                        " comic",
                        ",",
                        " and"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        63.95864868164062,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " Korean leadership by trying to sneak into Tokyo Disneyland with a fake passport \u00e2\u0122\u0136 might also be",
                    "max_token": " Disneyland",
                    "tokens": [
                        " Korean",
                        " leadership",
                        " by",
                        " trying",
                        " to",
                        " sneak",
                        " into",
                        " Tokyo",
                        " Disneyland",
                        " with",
                        " a",
                        " fake",
                        " passport",
                        " \u00e2\u0122\u0136",
                        " might",
                        " also",
                        " be"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        22.92797470092773,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 0,
                    "sentence_string": " Disney's animation powerhouse combination of Pixar and Disney Animation Studios.\u010a\u010aOn the flip",
                    "max_token": " Disney",
                    "tokens": [
                        " Disney",
                        "'s",
                        " animation",
                        " powerhouse",
                        " combination",
                        " of",
                        " Pixar",
                        " and",
                        " Disney",
                        " Animation",
                        " Studios",
                        ".",
                        "\u010a",
                        "\u010a",
                        "On",
                        " the",
                        " flip"
                    ],
                    "values": [
                        61.10309219360352,
                        0.5737603902816772,
                        0,
                        0,
                        0,
                        0,
                        11.78900527954102,
                        0,
                        54.97734069824219,
                        3.803654670715332,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " longer has rights to franchise.)\u010a\u010aDisney Interactive\u010a\u010aStar Wars: Clone Wars",
                    "max_token": "Disney",
                    "tokens": [
                        " longer",
                        " has",
                        " rights",
                        " to",
                        " franchise",
                        ".)",
                        "\u010a",
                        "\u010a",
                        "Disney",
                        " Interactive",
                        "\u010a",
                        "\u010a",
                        "Star",
                        " Wars",
                        ":",
                        " Clone",
                        " Wars"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        65.51378631591797,
                        3.604790687561035,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "\u00e2\u0122\u013bThe<|endoftext|> potential impact surveys and mitigation and restoration plans\u00e2\u0122\u013b\u00e2\u0122\u013b are",
                    "tokens": [
                        "\u00e2\u0122",
                        "\u013b",
                        "The",
                        "<|endoftext|>",
                        " potential",
                        " impact",
                        " surveys",
                        " and",
                        " mitigation",
                        " and",
                        " restoration",
                        " plans",
                        "\u00e2\u0122",
                        "\u013b",
                        "\u00e2\u0122",
                        "\u013b",
                        " are"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " that kid who felt unwanted, or the grown up who remembers how hard it was to",
                    "tokens": [
                        " that",
                        " kid",
                        " who",
                        " felt",
                        " unwanted",
                        ",",
                        " or",
                        " the",
                        " grown",
                        " up",
                        " who",
                        " remembers",
                        " how",
                        " hard",
                        " it",
                        " was",
                        " to"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " Trump and Hillary in the first debate, before either said a word, Trump made what",
                    "tokens": [
                        " Trump",
                        " and",
                        " Hillary",
                        " in",
                        " the",
                        " first",
                        " debate",
                        ",",
                        " before",
                        " either",
                        " said",
                        " a",
                        " word",
                        ",",
                        " Trump",
                        " made",
                        " what"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "3 billion) installing everything from traffic-management technologies to smart electricity grids.\u010a\u010a",
                    "tokens": [
                        "3",
                        " billion",
                        ")",
                        " installing",
                        " everything",
                        " from",
                        " traffic",
                        "-",
                        "management",
                        " technologies",
                        " to",
                        " smart",
                        " electricity",
                        " grids",
                        ".",
                        "\u010a",
                        "\u010a"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " city staff (Fire, Police, Crime Prevention, and more)\u010a\u010aOption to",
                    "tokens": [
                        " city",
                        " staff",
                        " (",
                        "Fire",
                        ",",
                        " Police",
                        ",",
                        " Crime",
                        " Prevention",
                        ",",
                        " and",
                        " more",
                        ")",
                        "\u010a",
                        "\u010a",
                        "Option",
                        " to"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 70.11211395263672
        },
        {
            "feature_index": 974,
            "gpt_predictions": [
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "websites or URLs",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " www.seithcg.com/wordpress.\u010a\u010aHere\u00e2\u0122\u013bs",
                    "max_token": "/",
                    "tokens": [
                        " www",
                        ".",
                        "se",
                        "ith",
                        "c",
                        "g",
                        ".",
                        "com",
                        "/",
                        "wordpress",
                        ".",
                        "\u010a",
                        "\u010a",
                        "Here",
                        "\u00e2\u0122",
                        "\u013b",
                        "s"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        45.30218505859375,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 6,
                    "sentence_string": ".bbvforums.org/forums/messages/8/80797.",
                    "max_token": "/",
                    "tokens": [
                        ".",
                        "bb",
                        "v",
                        "forums",
                        ".",
                        "org",
                        "/",
                        "forums",
                        "/",
                        "mess",
                        "ages",
                        "/",
                        "8",
                        "/",
                        "80",
                        "797",
                        "."
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        1.483306646347046,
                        56.7104606628418,
                        0,
                        27.05339241027832,
                        0,
                        0,
                        9.115799903869629,
                        0,
                        3.022544860839844,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 5,
                    "sentence_string": " and demonstrate a<|endoftext|>gov/documents/10-3725op.pdf<|endoftext|>",
                    "max_token": "/",
                    "tokens": [
                        " and",
                        " demonstrate",
                        " a",
                        "<|endoftext|>",
                        "gov",
                        "/",
                        "doc",
                        "uments",
                        "/",
                        "10",
                        "-",
                        "37",
                        "25",
                        "op",
                        ".",
                        "pdf",
                        "<|endoftext|>"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        21.5333194732666,
                        0,
                        0,
                        7.353664398193359,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": ".socialistparty.org.uk/articles/20566\u010a\u010aPosted on",
                    "max_token": "/",
                    "tokens": [
                        ".",
                        "social",
                        "ist",
                        "party",
                        ".",
                        "org",
                        ".",
                        "uk",
                        "/",
                        "articles",
                        "/",
                        "205",
                        "66",
                        "\u010a",
                        "\u010a",
                        "Posted",
                        " on"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0.4955632984638214,
                        2.358976125717163,
                        60.34051895141602,
                        1.145951747894287,
                        12.76787948608398,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "www.tusc.org.uk/16861/14-11-",
                    "max_token": "/",
                    "tokens": [
                        "www",
                        ".",
                        "t",
                        "usc",
                        ".",
                        "org",
                        ".",
                        "uk",
                        "/",
                        "16",
                        "86",
                        "1",
                        "/",
                        "14",
                        "-",
                        "11",
                        "-"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0.5439556837081909,
                        59.6654052734375,
                        0,
                        0,
                        0,
                        13.26562595367432,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " coat.\u010a\u010aDetective Inspector Steve Christian, from Liverpool CID, said:",
                    "tokens": [
                        " coat",
                        ".",
                        "\u010a",
                        "\u010a",
                        "Det",
                        "ective",
                        " Inspector",
                        " Steve",
                        " Christian",
                        ",",
                        " from",
                        " Liverpool",
                        " C",
                        "ID",
                        ",",
                        " said",
                        ":"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " drop out of the presidential race, according to NBC News ' Andrea Mitchell.\u010a\u010a",
                    "tokens": [
                        " drop",
                        " out",
                        " of",
                        " the",
                        " presidential",
                        " race",
                        ",",
                        " according",
                        " to",
                        " NBC",
                        " News",
                        " '",
                        " Andrea",
                        " Mitchell",
                        ".",
                        "\u010a",
                        "\u010a"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " Rumors that Amazon will soon begin accepting Bitcoin are persistent. If the retail giant does",
                    "tokens": [
                        " Rum",
                        "ors",
                        " that",
                        " Amazon",
                        " will",
                        " soon",
                        " begin",
                        " accepting",
                        " Bitcoin",
                        " are",
                        " persistent",
                        ".",
                        " If",
                        " the",
                        " retail",
                        " giant",
                        " does"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": ", and banking flows was a monthly net TIC (Treasury International Capital)<|endoftext|>",
                    "tokens": [
                        ",",
                        " and",
                        " banking",
                        " flows",
                        " was",
                        " a",
                        " monthly",
                        " net",
                        " T",
                        "IC",
                        " (",
                        "Tre",
                        "asury",
                        " International",
                        " Capital",
                        ")",
                        "<|endoftext|>"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " faces with respect to women.\u010a\u010aA CNN poll released Monday showed Obama moving into",
                    "tokens": [
                        " faces",
                        " with",
                        " respect",
                        " to",
                        " women",
                        ".",
                        "\u010a",
                        "\u010a",
                        "A",
                        " CNN",
                        " poll",
                        " released",
                        " Monday",
                        " showed",
                        " Obama",
                        " moving",
                        " into"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 67.06290435791016
        },
        {
            "feature_index": 299,
            "gpt_predictions": [
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    0,
                    1.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "mentions of specific historical figures, particularly related to political contexts",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " roster of celebrity speakers: Mikhail Gorbachev and Rudy Giuliani are among the regulars,",
                    "max_token": "achev",
                    "tokens": [
                        " roster",
                        " of",
                        " celebrity",
                        " speakers",
                        ":",
                        " Mikhail",
                        " Gor",
                        "b",
                        "achev",
                        " and",
                        " Rudy",
                        " Giuliani",
                        " are",
                        " among",
                        " the",
                        " regulars",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        2.319943428039551,
                        0,
                        16.5485725402832,
                        0,
                        0,
                        6.185073375701904,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " and energy costs, fell substantially from the Reagan recovery through the bursting tech bubble, and",
                    "max_token": " Reagan",
                    "tokens": [
                        " and",
                        " energy",
                        " costs",
                        ",",
                        " fell",
                        " substantially",
                        " from",
                        " the",
                        " Reagan",
                        " recovery",
                        " through",
                        " the",
                        " bursting",
                        " tech",
                        " bubble",
                        ",",
                        " and"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        38.60811614990234,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": ", he intoned.\u010a\u010aBill Clinton has tried to blunt criticism of his wife",
                    "max_token": " Clinton",
                    "tokens": [
                        ",",
                        " he",
                        " int",
                        "oned",
                        ".",
                        "\u010a",
                        "\u010a",
                        "Bill",
                        " Clinton",
                        " has",
                        " tried",
                        " to",
                        " blunt",
                        " criticism",
                        " of",
                        " his",
                        " wife"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        9.81607437133789,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " believe there were similar fears in 1980 when Reagan won the nomination. The elites that year",
                    "max_token": " Reagan",
                    "tokens": [
                        " believe",
                        " there",
                        " were",
                        " similar",
                        " fears",
                        " in",
                        " 1980",
                        " when",
                        " Reagan",
                        " won",
                        " the",
                        " nomination",
                        ".",
                        " The",
                        " elites",
                        " that",
                        " year"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        3.498912572860718,
                        0,
                        44.21978378295898,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "\u013f\u010a\u010aHogan points to Ronald Reagan\u00e2\u0122\u013bs first debate in 1984,",
                    "max_token": " Reagan",
                    "tokens": [
                        "\u013f",
                        "\u010a",
                        "\u010a",
                        "H",
                        "ogan",
                        " points",
                        " to",
                        " Ronald",
                        " Reagan",
                        "\u00e2\u0122",
                        "\u013b",
                        "s",
                        " first",
                        " debate",
                        " in",
                        " 1984",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        41.81748199462891,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " Trump and Hillary in the first debate, before either said a word, Trump made what",
                    "tokens": [
                        " Trump",
                        " and",
                        " Hillary",
                        " in",
                        " the",
                        " first",
                        " debate",
                        ",",
                        " before",
                        " either",
                        " said",
                        " a",
                        " word",
                        ",",
                        " Trump",
                        " made",
                        " what"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " that kid who felt unwanted, or the grown up who remembers how hard it was to",
                    "tokens": [
                        " that",
                        " kid",
                        " who",
                        " felt",
                        " unwanted",
                        ",",
                        " or",
                        " the",
                        " grown",
                        " up",
                        " who",
                        " remembers",
                        " how",
                        " hard",
                        " it",
                        " was",
                        " to"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "\u010aAldrin was a drug user before, Castillo admitted, but he had",
                    "tokens": [
                        "\u010a",
                        "A",
                        "ld",
                        "rin",
                        " was",
                        " a",
                        " drug",
                        " user",
                        " before",
                        ",",
                        " Cast",
                        "illo",
                        " admitted",
                        ",",
                        " but",
                        " he",
                        " had"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " who genuinely seem not to believe reality. Or Democrats, cowed into silence on issues",
                    "tokens": [
                        " who",
                        " genuinely",
                        " seem",
                        " not",
                        " to",
                        " believe",
                        " reality",
                        ".",
                        " Or",
                        " Democrats",
                        ",",
                        " c",
                        "owed",
                        " into",
                        " silence",
                        " on",
                        " issues"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " the incident, saying the man threatened him before leaving.\u010a\u010a\u00e2\u0122\u013eI think",
                    "tokens": [
                        " the",
                        " incident",
                        ",",
                        " saying",
                        " the",
                        " man",
                        " threatened",
                        " him",
                        " before",
                        " leaving",
                        ".",
                        "\u010a",
                        "\u010a",
                        "\u00e2\u0122",
                        "\u013e",
                        "I",
                        " think"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 48.3935661315918
        },
        {
            "feature_index": 277,
            "gpt_predictions": [
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    1.0
                ]
            ],
            "description": "words related to responses, reactions, and attitudes towards events or announcements",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": ", 2014\u010a\u010aThe post has drawn widespread praise on Facebook and Twitter. One friend",
                    "max_token": " widespread",
                    "tokens": [
                        ",",
                        " 2014",
                        "\u010a",
                        "\u010a",
                        "The",
                        " post",
                        " has",
                        " drawn",
                        " widespread",
                        " praise",
                        " on",
                        " Facebook",
                        " and",
                        " Twitter",
                        ".",
                        " One",
                        " friend"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        19.7289867401123,
                        22.62990379333496,
                        0.6098066568374634,
                        0,
                        0,
                        2.898053646087646,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " of his contemporaries, he wasn't met with as much institutional resistance as one might expect",
                    "max_token": " with",
                    "tokens": [
                        " of",
                        " his",
                        " contemporaries",
                        ",",
                        " he",
                        " wasn",
                        "'t",
                        " met",
                        " with",
                        " as",
                        " much",
                        " institutional",
                        " resistance",
                        " as",
                        " one",
                        " might",
                        " expect"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        14.13602066040039,
                        28.67621231079102,
                        6.545726299285889,
                        15.52198505401611,
                        4.561711311340332,
                        2.801668167114258,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 9,
                    "sentence_string": ", this same shift to the ordinary was met with resistance which continues to this day.",
                    "max_token": " with",
                    "tokens": [
                        ",",
                        " this",
                        " same",
                        " shift",
                        " to",
                        " the",
                        " ordinary",
                        " was",
                        " met",
                        " with",
                        " resistance",
                        " which",
                        " continues",
                        " to",
                        " this",
                        " day",
                        "."
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        19.75360679626465,
                        28.56168937683105,
                        11.04547691345215,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " advocates came up<|endoftext|>. She was met with resistance she couldn't understand.\u010a\u010a",
                    "max_token": " with",
                    "tokens": [
                        " advocates",
                        " came",
                        " up",
                        "<|endoftext|>",
                        ".",
                        " She",
                        " was",
                        " met",
                        " with",
                        " resistance",
                        " she",
                        " couldn",
                        "'t",
                        " understand",
                        ".",
                        "\u010a",
                        "\u010a"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        18.0175666809082,
                        34.70315170288086,
                        11.15716361999512,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": ", D.C., principal was met with cheers as he came out to his students",
                    "max_token": " with",
                    "tokens": [
                        ",",
                        " D",
                        ".",
                        "C",
                        ".,",
                        " principal",
                        " was",
                        " met",
                        " with",
                        " cheers",
                        " as",
                        " he",
                        " came",
                        " out",
                        " to",
                        " his",
                        " students"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        16.15780830383301,
                        29.82061195373535,
                        5.948788166046143,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " Trump and Hillary in the first debate, before either said a word, Trump made what",
                    "tokens": [
                        " Trump",
                        " and",
                        " Hillary",
                        " in",
                        " the",
                        " first",
                        " debate",
                        ",",
                        " before",
                        " either",
                        " said",
                        " a",
                        " word",
                        ",",
                        " Trump",
                        " made",
                        " what"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " that kid who felt unwanted, or the grown up who remembers how hard it was to",
                    "tokens": [
                        " that",
                        " kid",
                        " who",
                        " felt",
                        " unwanted",
                        ",",
                        " or",
                        " the",
                        " grown",
                        " up",
                        " who",
                        " remembers",
                        " how",
                        " hard",
                        " it",
                        " was",
                        " to"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "\u010aAldrin was a drug user before, Castillo admitted, but he had",
                    "tokens": [
                        "\u010a",
                        "A",
                        "ld",
                        "rin",
                        " was",
                        " a",
                        " drug",
                        " user",
                        " before",
                        ",",
                        " Cast",
                        "illo",
                        " admitted",
                        ",",
                        " but",
                        " he",
                        " had"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " who genuinely seem not to believe reality. Or Democrats, cowed into silence on issues",
                    "tokens": [
                        " who",
                        " genuinely",
                        " seem",
                        " not",
                        " to",
                        " believe",
                        " reality",
                        ".",
                        " Or",
                        " Democrats",
                        ",",
                        " c",
                        "owed",
                        " into",
                        " silence",
                        " on",
                        " issues"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " the incident, saying the man threatened him before leaving.\u010a\u010a\u00e2\u0122\u013eI think",
                    "tokens": [
                        " the",
                        " incident",
                        ",",
                        " saying",
                        " the",
                        " man",
                        " threatened",
                        " him",
                        " before",
                        " leaving",
                        ".",
                        "\u010a",
                        "\u010a",
                        "\u00e2\u0122",
                        "\u013e",
                        "I",
                        " think"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 44.61680603027344
        },
        {
            "feature_index": 924,
            "gpt_predictions": [
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "proper nouns related to fantasy worlds and characters",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " that must<|endoftext|>an Husky and Malamute-type sled dogs at the location",
                    "max_token": "am",
                    "tokens": [
                        " that",
                        " must",
                        "<|endoftext|>",
                        "an",
                        " Hus",
                        "ky",
                        " and",
                        " Mal",
                        "am",
                        "ute",
                        "-",
                        "type",
                        " sled",
                        " dogs",
                        " at",
                        " the",
                        " location"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        13.57971572875977,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " said Chicago Police News Affairs Officer John Mirabelli.\u010a\u010aPolice are still searching",
                    "max_token": "ab",
                    "tokens": [
                        " said",
                        " Chicago",
                        " Police",
                        " News",
                        " Affairs",
                        " Officer",
                        " John",
                        " Mir",
                        "ab",
                        "elli",
                        ".",
                        "\u010a",
                        "\u010a",
                        "Police",
                        " are",
                        " still",
                        " searching"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        15.18739318847656,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " 19:33) A video showing Bonifacio Global City guards restraining a Caucasian man",
                    "max_token": "if",
                    "tokens": [
                        " 19",
                        ":",
                        "33",
                        ")",
                        " A",
                        " video",
                        " showing",
                        " Bon",
                        "if",
                        "acio",
                        " Global",
                        " City",
                        " guards",
                        " restraining",
                        " a",
                        " Caucasian",
                        " man"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        17.36355018615723,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " I know Gandalf's pal, Radagast the Brown, agrees with me,",
                    "max_token": "ag",
                    "tokens": [
                        " I",
                        " know",
                        " Gand",
                        "alf",
                        "'s",
                        " pal",
                        ",",
                        " Rad",
                        "ag",
                        "ast",
                        " the",
                        " Brown",
                        ",",
                        " agrees",
                        " with",
                        " me",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        19.75417518615723,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "lestick displaying bullish dominance with a Marubozu, suggesting that it may be",
                    "max_token": "ub",
                    "tokens": [
                        "lest",
                        "ick",
                        " displaying",
                        " bullish",
                        " dominance",
                        " with",
                        " a",
                        " Mar",
                        "ub",
                        "oz",
                        "u",
                        ",",
                        " suggesting",
                        " that",
                        " it",
                        " may",
                        " be"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        22.9067554473877,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " coat.\u010a\u010aDetective Inspector Steve Christian, from Liverpool CID, said:",
                    "tokens": [
                        " coat",
                        ".",
                        "\u010a",
                        "\u010a",
                        "Det",
                        "ective",
                        " Inspector",
                        " Steve",
                        " Christian",
                        ",",
                        " from",
                        " Liverpool",
                        " C",
                        "ID",
                        ",",
                        " said",
                        ":"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " drop out of the presidential race, according to NBC News ' Andrea Mitchell.\u010a\u010a",
                    "tokens": [
                        " drop",
                        " out",
                        " of",
                        " the",
                        " presidential",
                        " race",
                        ",",
                        " according",
                        " to",
                        " NBC",
                        " News",
                        " '",
                        " Andrea",
                        " Mitchell",
                        ".",
                        "\u010a",
                        "\u010a"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " Rumors that Amazon will soon begin accepting Bitcoin are persistent. If the retail giant does",
                    "tokens": [
                        " Rum",
                        "ors",
                        " that",
                        " Amazon",
                        " will",
                        " soon",
                        " begin",
                        " accepting",
                        " Bitcoin",
                        " are",
                        " persistent",
                        ".",
                        " If",
                        " the",
                        " retail",
                        " giant",
                        " does"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": ", and banking flows was a monthly net TIC (Treasury International Capital)<|endoftext|>",
                    "tokens": [
                        ",",
                        " and",
                        " banking",
                        " flows",
                        " was",
                        " a",
                        " monthly",
                        " net",
                        " T",
                        "IC",
                        " (",
                        "Tre",
                        "asury",
                        " International",
                        " Capital",
                        ")",
                        "<|endoftext|>"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " faces with respect to women.\u010a\u010aA CNN poll released Monday showed Obama moving into",
                    "tokens": [
                        " faces",
                        " with",
                        " respect",
                        " to",
                        " women",
                        ".",
                        "\u010a",
                        "\u010a",
                        "A",
                        " CNN",
                        " poll",
                        " released",
                        " Monday",
                        " showed",
                        " Obama",
                        " moving",
                        " into"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 25.11111450195312
        },
        {
            "feature_index": 601,
            "gpt_predictions": [
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "attributes related to products such as being inexpensive, lasting, versatile, creamy, light, and resistant",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": ". They are scented, slippery, and full of viscous digestive fluids.<|endoftext|>",
                    "max_token": " and",
                    "tokens": [
                        ".",
                        " They",
                        " are",
                        " sc",
                        "ented",
                        ",",
                        " slippery",
                        ",",
                        " and",
                        " full",
                        " of",
                        " visc",
                        "ous",
                        " digestive",
                        " fluids",
                        ".",
                        "<|endoftext|>"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        7.326846599578857,
                        0,
                        10.66367816925049,
                        17.61353302001953,
                        0,
                        1.724838018417358,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "uffs that are chewy on the inside and lightly crisp on the outside. Mixing",
                    "max_token": " and",
                    "tokens": [
                        "uffs",
                        " that",
                        " are",
                        " che",
                        "wy",
                        " on",
                        " the",
                        " inside",
                        " and",
                        " lightly",
                        " crisp",
                        " on",
                        " the",
                        " outside",
                        ".",
                        " Mix",
                        "ing"
                    ],
                    "values": [
                        0,
                        0.2455925494432449,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0.658999502658844,
                        18.65002250671387,
                        0,
                        0,
                        1.947137594223022,
                        3.41329550743103,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": ".\u010a\u010aThe tool must be portable and small enough to fit in our storage space",
                    "max_token": " and",
                    "tokens": [
                        ".",
                        "\u010a",
                        "\u010a",
                        "The",
                        " tool",
                        " must",
                        " be",
                        " portable",
                        " and",
                        " small",
                        " enough",
                        " to",
                        " fit",
                        " in",
                        " our",
                        " storage",
                        " space"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        3.287133693695068,
                        0,
                        0,
                        19.95919799804688,
                        0.4703934490680695,
                        3.422603607177734,
                        7.551342964172363,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "\u00e2\u0122\u013bs creamy, steamy, and savory-starch deliciousness.",
                    "max_token": " and",
                    "tokens": [
                        "\u00e2\u0122",
                        "\u013b",
                        "s",
                        " creamy",
                        ",",
                        " steam",
                        "y",
                        ",",
                        " and",
                        " sav",
                        "ory",
                        "-",
                        "st",
                        "arch",
                        " delicious",
                        "ness",
                        "."
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        12.64519023895264,
                        0,
                        0,
                        11.5807580947876,
                        21.1386890411377,
                        0,
                        0,
                        2.02824854850769,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": ": it\u00e2\u0122\u013bs highly moisturizing and perfect for bleached hair, and you",
                    "max_token": " and",
                    "tokens": [
                        ":",
                        " it",
                        "\u00e2\u0122",
                        "\u013b",
                        "s",
                        " highly",
                        " moistur",
                        "izing",
                        " and",
                        " perfect",
                        " for",
                        " ble",
                        "ached",
                        " hair",
                        ",",
                        " and",
                        " you"
                    ],
                    "values": [
                        0,
                        1.463090419769287,
                        0,
                        4.818115711212158,
                        3.510923385620117,
                        0,
                        0,
                        1.90003776550293,
                        24.27341461181641,
                        2.721040964126587,
                        0,
                        0,
                        0,
                        0,
                        0,
                        9.695978164672852,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " quick lead only to have to bow out before the end of the game because they went",
                    "tokens": [
                        " quick",
                        " lead",
                        " only",
                        " to",
                        " have",
                        " to",
                        " bow",
                        " out",
                        " before",
                        " the",
                        " end",
                        " of",
                        " the",
                        " game",
                        " because",
                        " they",
                        " went"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "\u010aAldrin was a drug user before, Castillo admitted, but he had",
                    "tokens": [
                        "\u010a",
                        "A",
                        "ld",
                        "rin",
                        " was",
                        " a",
                        " drug",
                        " user",
                        " before",
                        ",",
                        " Cast",
                        "illo",
                        " admitted",
                        ",",
                        " but",
                        " he",
                        " had"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " height at some international summit.\u010a\u010aOr is it because of these deeply controversial statements",
                    "tokens": [
                        " height",
                        " at",
                        " some",
                        " international",
                        " summit",
                        ".",
                        "\u010a",
                        "\u010a",
                        "Or",
                        " is",
                        " it",
                        " because",
                        " of",
                        " these",
                        " deeply",
                        " controversial",
                        " statements"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " years after he was jailed for espionage, before his release in 1969.\u010a\u010aHe",
                    "tokens": [
                        " years",
                        " after",
                        " he",
                        " was",
                        " jailed",
                        " for",
                        " espionage",
                        ",",
                        " before",
                        " his",
                        " release",
                        " in",
                        " 1969",
                        ".",
                        "\u010a",
                        "\u010a",
                        "He"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " faces with respect to women.\u010a\u010aA CNN poll released Monday showed Obama moving into",
                    "tokens": [
                        " faces",
                        " with",
                        " respect",
                        " to",
                        " women",
                        ".",
                        "\u010a",
                        "\u010a",
                        "A",
                        " CNN",
                        " poll",
                        " released",
                        " Monday",
                        " showed",
                        " Obama",
                        " moving",
                        " into"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 28.94306755065918
        },
        {
            "feature_index": 439,
            "gpt_predictions": [
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    1.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "phrases indicating uncertainty or anticipation of future events",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 9,
                    "sentence_string": " combo ability. His animations leave a lot to be desired and like Iron Fist, I",
                    "max_token": " be",
                    "tokens": [
                        " combo",
                        " ability",
                        ".",
                        " His",
                        " animations",
                        " leave",
                        " a",
                        " lot",
                        " to",
                        " be",
                        " desired",
                        " and",
                        " like",
                        " Iron",
                        " Fist",
                        ",",
                        " I"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        17.94599533081055,
                        27.38888549804688,
                        0.7333395481109619,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 9,
                    "sentence_string": " this \"game\" goes, it remains to be known what will happen at the launch",
                    "max_token": " be",
                    "tokens": [
                        " this",
                        " \"",
                        "game",
                        "\"",
                        " goes",
                        ",",
                        " it",
                        " remains",
                        " to",
                        " be",
                        " known",
                        " what",
                        " will",
                        " happen",
                        " at",
                        " the",
                        " launch"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        41.37228393554688,
                        53.04798126220703,
                        5.124235153198242,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " the photocathode layer is yet to be determined. Although, they think a thin",
                    "max_token": " be",
                    "tokens": [
                        " the",
                        " photoc",
                        "ath",
                        "ode",
                        " layer",
                        " is",
                        " yet",
                        " to",
                        " be",
                        " determined",
                        ".",
                        " Although",
                        ",",
                        " they",
                        " think",
                        " a",
                        " thin"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        2.655563592910767,
                        15.2370080947876,
                        20.27809715270996,
                        1.911930680274963,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " indication of its wide release is yet to be seen.\u010a\u010aWar Room brought in",
                    "max_token": " be",
                    "tokens": [
                        " indication",
                        " of",
                        " its",
                        " wide",
                        " release",
                        " is",
                        " yet",
                        " to",
                        " be",
                        " seen",
                        ".",
                        "\u010a",
                        "\u010a",
                        "War",
                        " Room",
                        " brought",
                        " in"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        1.295321822166443,
                        15.70130825042725,
                        31.02478408813477,
                        38.65061569213867,
                        4.574274063110352,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 9,
                    "sentence_string": " a single translated page of these texts remains to be seen. Obama, who from ages",
                    "max_token": " be",
                    "tokens": [
                        " a",
                        " single",
                        " translated",
                        " page",
                        " of",
                        " these",
                        " texts",
                        " remains",
                        " to",
                        " be",
                        " seen",
                        ".",
                        " Obama",
                        ",",
                        " who",
                        " from",
                        " ages"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        3.284528255462646,
                        47.04870223999023,
                        53.51831436157227,
                        11.43284702301025,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "\u00e2\u0122\u013bThe<|endoftext|> potential impact surveys and mitigation and restoration plans\u00e2\u0122\u013b\u00e2\u0122\u013b are",
                    "tokens": [
                        "\u00e2\u0122",
                        "\u013b",
                        "The",
                        "<|endoftext|>",
                        " potential",
                        " impact",
                        " surveys",
                        " and",
                        " mitigation",
                        " and",
                        " restoration",
                        " plans",
                        "\u00e2\u0122",
                        "\u013b",
                        "\u00e2\u0122",
                        "\u013b",
                        " are"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " that kid who felt unwanted, or the grown up who remembers how hard it was to",
                    "tokens": [
                        " that",
                        " kid",
                        " who",
                        " felt",
                        " unwanted",
                        ",",
                        " or",
                        " the",
                        " grown",
                        " up",
                        " who",
                        " remembers",
                        " how",
                        " hard",
                        " it",
                        " was",
                        " to"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " Trump and Hillary in the first debate, before either said a word, Trump made what",
                    "tokens": [
                        " Trump",
                        " and",
                        " Hillary",
                        " in",
                        " the",
                        " first",
                        " debate",
                        ",",
                        " before",
                        " either",
                        " said",
                        " a",
                        " word",
                        ",",
                        " Trump",
                        " made",
                        " what"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "3 billion) installing everything from traffic-management technologies to smart electricity grids.\u010a\u010a",
                    "tokens": [
                        "3",
                        " billion",
                        ")",
                        " installing",
                        " everything",
                        " from",
                        " traffic",
                        "-",
                        "management",
                        " technologies",
                        " to",
                        " smart",
                        " electricity",
                        " grids",
                        ".",
                        "\u010a",
                        "\u010a"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " city staff (Fire, Police, Crime Prevention, and more)\u010a\u010aOption to",
                    "tokens": [
                        " city",
                        " staff",
                        " (",
                        "Fire",
                        ",",
                        " Police",
                        ",",
                        " Crime",
                        " Prevention",
                        ",",
                        " and",
                        " more",
                        ")",
                        "\u010a",
                        "\u010a",
                        "Option",
                        " to"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 57.37928771972656
        },
        {
            "feature_index": 837,
            "gpt_predictions": [
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "non-profit organizations or initiatives",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "olt explained that by involving teachers in their nonprofit\u00e2\u0122\u013bs mission, they\u00e2\u0122\u013b",
                    "max_token": " nonprofit",
                    "tokens": [
                        "olt",
                        " explained",
                        " that",
                        " by",
                        " involving",
                        " teachers",
                        " in",
                        " their",
                        " nonprofit",
                        "\u00e2\u0122",
                        "\u013b",
                        "s",
                        " mission",
                        ",",
                        " they",
                        "\u00e2\u0122",
                        "\u013b"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        34.86174011230469,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " tax subsidies to help buy coverage. The nonprofit Families USA, which supports the health-",
                    "max_token": " nonprofit",
                    "tokens": [
                        " tax",
                        " subsidies",
                        " to",
                        " help",
                        " buy",
                        " coverage",
                        ".",
                        " The",
                        " nonprofit",
                        " Families",
                        " USA",
                        ",",
                        " which",
                        " supports",
                        " the",
                        " health",
                        "-"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        41.34698867797852,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " to be working with a Netherlands-based NGO.\u010a\u010aAccording to the SITE",
                    "max_token": " NGO",
                    "tokens": [
                        " to",
                        " be",
                        " working",
                        " with",
                        " a",
                        " Netherlands",
                        "-",
                        "based",
                        " NGO",
                        ".",
                        "\u010a",
                        "\u010a",
                        "According",
                        " to",
                        " the",
                        " S",
                        "ITE"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        9.604657173156738,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "ielution Community Farm, a non-profit organization that believes everyone deserves to eat healthy",
                    "max_token": "profit",
                    "tokens": [
                        "iel",
                        "ution",
                        " Community",
                        " Farm",
                        ",",
                        " a",
                        " non",
                        "-",
                        "profit",
                        " organization",
                        " that",
                        " believes",
                        " everyone",
                        " deserves",
                        " to",
                        " eat",
                        " healthy"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        44.47846984863281,
                        5.810364246368408,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "PA), a Washington-based non-profit group formed to protect poker players\u00e2\u0122\u013b",
                    "max_token": "profit",
                    "tokens": [
                        "PA",
                        "),",
                        " a",
                        " Washington",
                        "-",
                        "based",
                        " non",
                        "-",
                        "profit",
                        " group",
                        " formed",
                        " to",
                        " protect",
                        " poker",
                        " players",
                        "\u00e2\u0122",
                        "\u013b"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        43.88997268676758,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " coat.\u010a\u010aDetective Inspector Steve Christian, from Liverpool CID, said:",
                    "tokens": [
                        " coat",
                        ".",
                        "\u010a",
                        "\u010a",
                        "Det",
                        "ective",
                        " Inspector",
                        " Steve",
                        " Christian",
                        ",",
                        " from",
                        " Liverpool",
                        " C",
                        "ID",
                        ",",
                        " said",
                        ":"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " drop out of the presidential race, according to NBC News ' Andrea Mitchell.\u010a\u010a",
                    "tokens": [
                        " drop",
                        " out",
                        " of",
                        " the",
                        " presidential",
                        " race",
                        ",",
                        " according",
                        " to",
                        " NBC",
                        " News",
                        " '",
                        " Andrea",
                        " Mitchell",
                        ".",
                        "\u010a",
                        "\u010a"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " Rumors that Amazon will soon begin accepting Bitcoin are persistent. If the retail giant does",
                    "tokens": [
                        " Rum",
                        "ors",
                        " that",
                        " Amazon",
                        " will",
                        " soon",
                        " begin",
                        " accepting",
                        " Bitcoin",
                        " are",
                        " persistent",
                        ".",
                        " If",
                        " the",
                        " retail",
                        " giant",
                        " does"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": ", and banking flows was a monthly net TIC (Treasury International Capital)<|endoftext|>",
                    "tokens": [
                        ",",
                        " and",
                        " banking",
                        " flows",
                        " was",
                        " a",
                        " monthly",
                        " net",
                        " T",
                        "IC",
                        " (",
                        "Tre",
                        "asury",
                        " International",
                        " Capital",
                        ")",
                        "<|endoftext|>"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " faces with respect to women.\u010a\u010aA CNN poll released Monday showed Obama moving into",
                    "tokens": [
                        " faces",
                        " with",
                        " respect",
                        " to",
                        " women",
                        ".",
                        "\u010a",
                        "\u010a",
                        "A",
                        " CNN",
                        " poll",
                        " released",
                        " Monday",
                        " showed",
                        " Obama",
                        " moving",
                        " into"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 47.99362182617188
        },
        {
            "feature_index": 570,
            "gpt_predictions": [
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "phrases related to inclusivity and equality for all",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "\u013bs ultimate goal is to predispose all public dialogue toward truth telling\u010a\u010aTr",
                    "max_token": " all",
                    "tokens": [
                        "\u013b",
                        "s",
                        " ultimate",
                        " goal",
                        " is",
                        " to",
                        " predis",
                        "pose",
                        " all",
                        " public",
                        " dialogue",
                        " toward",
                        " truth",
                        " telling",
                        "\u010a",
                        "\u010a",
                        "Tr"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        27.82762336730957,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " in the region has made life harder for all Protestant groups. \u00e2\u0122\u013eProtestant",
                    "max_token": " all",
                    "tokens": [
                        " in",
                        " the",
                        " region",
                        " has",
                        " made",
                        " life",
                        " harder",
                        " for",
                        " all",
                        " Protestant",
                        " groups",
                        ".",
                        " \u00e2\u0122",
                        "\u013e",
                        "Pro",
                        "test",
                        "ant"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        36.56023025512695,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " the Ground Forces of Ukraine, to which all the recruitment offices are subordinate, confirmed that",
                    "max_token": " all",
                    "tokens": [
                        " the",
                        " Ground",
                        " Forces",
                        " of",
                        " Ukraine",
                        ",",
                        " to",
                        " which",
                        " all",
                        " the",
                        " recruitment",
                        " offices",
                        " are",
                        " subordinate",
                        ",",
                        " confirmed",
                        " that"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        11.38445281982422,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " release<|endoftext|> to foster respect and appreciation on all sides. There\u00e2\u0122\u013bs no artistic",
                    "max_token": " all",
                    "tokens": [
                        " release",
                        "<|endoftext|>",
                        " to",
                        " foster",
                        " respect",
                        " and",
                        " appreciation",
                        " on",
                        " all",
                        " sides",
                        ".",
                        " There",
                        "\u00e2\u0122",
                        "\u013b",
                        "s",
                        " no",
                        " artistic"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        41.76294326782227,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " university\u00e2\u0122\u013f because it is available to all sections of the society regardless of age,",
                    "max_token": " all",
                    "tokens": [
                        " university",
                        "\u00e2\u0122",
                        "\u013f",
                        " because",
                        " it",
                        " is",
                        " available",
                        " to",
                        " all",
                        " sections",
                        " of",
                        " the",
                        " society",
                        " regardless",
                        " of",
                        " age",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        40.10082626342773,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " quick lead only to have to bow out before the end of the game because they went",
                    "tokens": [
                        " quick",
                        " lead",
                        " only",
                        " to",
                        " have",
                        " to",
                        " bow",
                        " out",
                        " before",
                        " the",
                        " end",
                        " of",
                        " the",
                        " game",
                        " because",
                        " they",
                        " went"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "\u010aAldrin was a drug user before, Castillo admitted, but he had",
                    "tokens": [
                        "\u010a",
                        "A",
                        "ld",
                        "rin",
                        " was",
                        " a",
                        " drug",
                        " user",
                        " before",
                        ",",
                        " Cast",
                        "illo",
                        " admitted",
                        ",",
                        " but",
                        " he",
                        " had"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " height at some international summit.\u010a\u010aOr is it because of these deeply controversial statements",
                    "tokens": [
                        " height",
                        " at",
                        " some",
                        " international",
                        " summit",
                        ".",
                        "\u010a",
                        "\u010a",
                        "Or",
                        " is",
                        " it",
                        " because",
                        " of",
                        " these",
                        " deeply",
                        " controversial",
                        " statements"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " years after he was jailed for espionage, before his release in 1969.\u010a\u010aHe",
                    "tokens": [
                        " years",
                        " after",
                        " he",
                        " was",
                        " jailed",
                        " for",
                        " espionage",
                        ",",
                        " before",
                        " his",
                        " release",
                        " in",
                        " 1969",
                        ".",
                        "\u010a",
                        "\u010a",
                        "He"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " faces with respect to women.\u010a\u010aA CNN poll released Monday showed Obama moving into",
                    "tokens": [
                        " faces",
                        " with",
                        " respect",
                        " to",
                        " women",
                        ".",
                        "\u010a",
                        "\u010a",
                        "A",
                        " CNN",
                        " poll",
                        " released",
                        " Monday",
                        " showed",
                        " Obama",
                        " moving",
                        " into"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 49.73934555053711
        },
        {
            "feature_index": 879,
            "gpt_predictions": [
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "phrases related to accepting terms and conditions for receiving updates",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " three meaningful actions you can take each week. You will receive occasional promotional offers for programs",
                    "max_token": ".",
                    "tokens": [
                        " three",
                        " meaningful",
                        " actions",
                        " you",
                        " can",
                        " take",
                        " each",
                        " week",
                        ".",
                        " You",
                        " will",
                        " receive",
                        " occasional",
                        " promotional",
                        " offers",
                        " for",
                        " programs"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        5.269466400146484,
                        27.95818519592285,
                        7.567116260528564,
                        1.75899863243103,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " and stories that matter, delivered weekday mornings.\u010a\u010aFeb. 25, 2015,",
                    "max_token": ".",
                    "tokens": [
                        " and",
                        " stories",
                        " that",
                        " matter",
                        ",",
                        " delivered",
                        " weekday",
                        " mornings",
                        ".",
                        "\u010a",
                        "\u010a",
                        "Feb",
                        ".",
                        " 25",
                        ",",
                        " 2015",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        23.29630851745605,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " charge. You may opt out at anytime. You also agree to this site's Privacy",
                    "max_token": ".",
                    "tokens": [
                        " charge",
                        ".",
                        " You",
                        " may",
                        " opt",
                        " out",
                        " at",
                        " anytime",
                        ".",
                        " You",
                        " also",
                        " agree",
                        " to",
                        " this",
                        " site",
                        "'s",
                        " Privacy"
                    ],
                    "values": [
                        2.570804834365845,
                        8.53053092956543,
                        0,
                        0,
                        0.9383602142333984,
                        0,
                        0.8325555324554443,
                        7.817044734954834,
                        40.57173156738281,
                        20.53899765014648,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " three meaningful actions you can take each week.\u010a\u010aThank you for signing up.",
                    "max_token": ".",
                    "tokens": [
                        " three",
                        " meaningful",
                        " actions",
                        " you",
                        " can",
                        " take",
                        " each",
                        " week",
                        ".",
                        "\u010a",
                        "\u010a",
                        "Thank",
                        " you",
                        " for",
                        " signing",
                        " up",
                        "."
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        7.250517845153809,
                        31.70653915405273,
                        7.223830223083496,
                        10.35417652130127,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " for as little as $2 a month!\u010a\u010aSupport Progressive Journalism The Nation is",
                    "max_token": "!",
                    "tokens": [
                        " for",
                        " as",
                        " little",
                        " as",
                        " $",
                        "2",
                        " a",
                        " month",
                        "!",
                        "\u010a",
                        "\u010a",
                        "Support",
                        " Progressive",
                        " Journalism",
                        " The",
                        " Nation",
                        " is"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        7.79451322555542,
                        0.7573151588439941,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " coat.\u010a\u010aDetective Inspector Steve Christian, from Liverpool CID, said:",
                    "tokens": [
                        " coat",
                        ".",
                        "\u010a",
                        "\u010a",
                        "Det",
                        "ective",
                        " Inspector",
                        " Steve",
                        " Christian",
                        ",",
                        " from",
                        " Liverpool",
                        " C",
                        "ID",
                        ",",
                        " said",
                        ":"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " drop out of the presidential race, according to NBC News ' Andrea Mitchell.\u010a\u010a",
                    "tokens": [
                        " drop",
                        " out",
                        " of",
                        " the",
                        " presidential",
                        " race",
                        ",",
                        " according",
                        " to",
                        " NBC",
                        " News",
                        " '",
                        " Andrea",
                        " Mitchell",
                        ".",
                        "\u010a",
                        "\u010a"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " Rumors that Amazon will soon begin accepting Bitcoin are persistent. If the retail giant does",
                    "tokens": [
                        " Rum",
                        "ors",
                        " that",
                        " Amazon",
                        " will",
                        " soon",
                        " begin",
                        " accepting",
                        " Bitcoin",
                        " are",
                        " persistent",
                        ".",
                        " If",
                        " the",
                        " retail",
                        " giant",
                        " does"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": ", and banking flows was a monthly net TIC (Treasury International Capital)<|endoftext|>",
                    "tokens": [
                        ",",
                        " and",
                        " banking",
                        " flows",
                        " was",
                        " a",
                        " monthly",
                        " net",
                        " T",
                        "IC",
                        " (",
                        "Tre",
                        "asury",
                        " International",
                        " Capital",
                        ")",
                        "<|endoftext|>"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " faces with respect to women.\u010a\u010aA CNN poll released Monday showed Obama moving into",
                    "tokens": [
                        " faces",
                        " with",
                        " respect",
                        " to",
                        " women",
                        ".",
                        "\u010a",
                        "\u010a",
                        "A",
                        " CNN",
                        " poll",
                        " released",
                        " Monday",
                        " showed",
                        " Obama",
                        " moving",
                        " into"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 75.2142105102539
        },
        {
            "feature_index": 261,
            "gpt_predictions": [
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "phrases related to organizations or official entities",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": ", explained that the C.E.O.s were either confused or lying:",
                    "max_token": "O",
                    "tokens": [
                        ",",
                        " explained",
                        " that",
                        " the",
                        " C",
                        ".",
                        "E",
                        ".",
                        "O",
                        ".",
                        "s",
                        " were",
                        " either",
                        " confused",
                        " or",
                        " lying",
                        ":"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        16.68161392211914,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": ", and the Missouri-based PROMO \u00e2\u0122\u0136 didn\u00e2\u0122\u013bt offer an official",
                    "max_token": "O",
                    "tokens": [
                        ",",
                        " and",
                        " the",
                        " Missouri",
                        "-",
                        "based",
                        " PR",
                        "OM",
                        "O",
                        " \u00e2\u0122\u0136",
                        " didn",
                        "\u00e2\u0122",
                        "\u013b",
                        "t",
                        " offer",
                        " an",
                        " official"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        6.71220874786377,
                        19.95271492004395,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " Tropical Deforestation. NASA\u00ef\u00bf\u00bds Earth Observatory.<|endoftext|>ASSEMBLY set to",
                    "max_token": " Observatory",
                    "tokens": [
                        " Tropical",
                        " De",
                        "forestation",
                        ".",
                        " NASA",
                        "\u00ef\u00bf\u00bd",
                        "s",
                        " Earth",
                        " Observatory",
                        ".",
                        "<|endoftext|>",
                        "ASS",
                        "EM",
                        "BL",
                        "Y",
                        " set",
                        " to"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        6.277830600738525,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "<|endoftext|>ronesque smell.\u010a\u010aIPO investors, for example, won\u00e2\u0122\u013b",
                    "max_token": "O",
                    "tokens": [
                        "<|endoftext|>",
                        "rones",
                        "que",
                        " smell",
                        ".",
                        "\u010a",
                        "\u010a",
                        "IP",
                        "O",
                        " investors",
                        ",",
                        " for",
                        " example",
                        ",",
                        " won",
                        "\u00e2\u0122",
                        "\u013b"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        23.06670188903809,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " 3 critical adults VEH into pedestrians AVOID AREA pic.twitter.com",
                    "max_token": "O",
                    "tokens": [
                        " 3",
                        " critical",
                        " adults",
                        " V",
                        "EH",
                        " into",
                        " pedestrians",
                        " AV",
                        "O",
                        "ID",
                        " ARE",
                        "A",
                        " pic",
                        ".",
                        "twitter",
                        ".",
                        "com"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        22.6049861907959,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " Trump and Hillary in the first debate, before either said a word, Trump made what",
                    "tokens": [
                        " Trump",
                        " and",
                        " Hillary",
                        " in",
                        " the",
                        " first",
                        " debate",
                        ",",
                        " before",
                        " either",
                        " said",
                        " a",
                        " word",
                        ",",
                        " Trump",
                        " made",
                        " what"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " that kid who felt unwanted, or the grown up who remembers how hard it was to",
                    "tokens": [
                        " that",
                        " kid",
                        " who",
                        " felt",
                        " unwanted",
                        ",",
                        " or",
                        " the",
                        " grown",
                        " up",
                        " who",
                        " remembers",
                        " how",
                        " hard",
                        " it",
                        " was",
                        " to"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "\u010aAldrin was a drug user before, Castillo admitted, but he had",
                    "tokens": [
                        "\u010a",
                        "A",
                        "ld",
                        "rin",
                        " was",
                        " a",
                        " drug",
                        " user",
                        " before",
                        ",",
                        " Cast",
                        "illo",
                        " admitted",
                        ",",
                        " but",
                        " he",
                        " had"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " who genuinely seem not to believe reality. Or Democrats, cowed into silence on issues",
                    "tokens": [
                        " who",
                        " genuinely",
                        " seem",
                        " not",
                        " to",
                        " believe",
                        " reality",
                        ".",
                        " Or",
                        " Democrats",
                        ",",
                        " c",
                        "owed",
                        " into",
                        " silence",
                        " on",
                        " issues"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " the incident, saying the man threatened him before leaving.\u010a\u010a\u00e2\u0122\u013eI think",
                    "tokens": [
                        " the",
                        " incident",
                        ",",
                        " saying",
                        " the",
                        " man",
                        " threatened",
                        " him",
                        " before",
                        " leaving",
                        ".",
                        "\u010a",
                        "\u010a",
                        "\u00e2\u0122",
                        "\u013e",
                        "I",
                        " think"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 27.36114883422852
        },
        {
            "feature_index": 578,
            "gpt_predictions": [
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "reference to notable people or famous figures",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " obtained by Judicial Watch via a FOIA request, that lists people who embrace \u00e2\u0122\u013eindividual",
                    "max_token": ",",
                    "tokens": [
                        " obtained",
                        " by",
                        " Judicial",
                        " Watch",
                        " via",
                        " a",
                        " FOIA",
                        " request",
                        ",",
                        " that",
                        " lists",
                        " people",
                        " who",
                        " embrace",
                        " \u00e2\u0122",
                        "\u013e",
                        "individual"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        8.167778015136719,
                        0.1820115447044373,
                        0,
                        0.759060800075531,
                        0,
                        0,
                        0,
                        0.4703297019004822,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " Trump has taken to Twitter and made public statements challenging Senate Republicans to support an overhaul or",
                    "max_token": " statements",
                    "tokens": [
                        " Trump",
                        " has",
                        " taken",
                        " to",
                        " Twitter",
                        " and",
                        " made",
                        " public",
                        " statements",
                        " challenging",
                        " Senate",
                        " Republicans",
                        " to",
                        " support",
                        " an",
                        " overhaul",
                        " or"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        2.084008932113647,
                        0,
                        0.3292269110679626,
                        0.5363486409187317,
                        7.182592391967773,
                        0,
                        0,
                        5.00562572479248,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " it\u00e2\u0122\u013bs the only one in Austria and is located in the northwest part of",
                    "max_token": " Austria",
                    "tokens": [
                        " it",
                        "\u00e2\u0122",
                        "\u013b",
                        "s",
                        " the",
                        " only",
                        " one",
                        " in",
                        " Austria",
                        " and",
                        " is",
                        " located",
                        " in",
                        " the",
                        " northwest",
                        " part",
                        " of"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0.6562408208847046,
                        0.6125317215919495,
                        0,
                        0,
                        0,
                        6.404332637786865,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "\u010a\u010a\"I will go to my grave defending Adrian, but at the<|endoftext|>'s",
                    "max_token": " grave",
                    "tokens": [
                        "\u010a",
                        "\u010a",
                        "\"",
                        "I",
                        " will",
                        " go",
                        " to",
                        " my",
                        " grave",
                        " defending",
                        " Adrian",
                        ",",
                        " but",
                        " at",
                        " the",
                        "<|endoftext|>",
                        "'s"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        9.154139518737793,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": ". The characters spend the majority of the episode trapped in these places, but since the",
                    "max_token": " episode",
                    "tokens": [
                        ".",
                        " The",
                        " characters",
                        " spend",
                        " the",
                        " majority",
                        " of",
                        " the",
                        " episode",
                        " trapped",
                        " in",
                        " these",
                        " places",
                        ",",
                        " but",
                        " since",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        7.457150936126709,
                        0.1067382171750069,
                        1.248528361320496,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " quick lead only to have to bow out before the end of the game because they went",
                    "tokens": [
                        " quick",
                        " lead",
                        " only",
                        " to",
                        " have",
                        " to",
                        " bow",
                        " out",
                        " before",
                        " the",
                        " end",
                        " of",
                        " the",
                        " game",
                        " because",
                        " they",
                        " went"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "\u010aAldrin was a drug user before, Castillo admitted, but he had",
                    "tokens": [
                        "\u010a",
                        "A",
                        "ld",
                        "rin",
                        " was",
                        " a",
                        " drug",
                        " user",
                        " before",
                        ",",
                        " Cast",
                        "illo",
                        " admitted",
                        ",",
                        " but",
                        " he",
                        " had"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " height at some international summit.\u010a\u010aOr is it because of these deeply controversial statements",
                    "tokens": [
                        " height",
                        " at",
                        " some",
                        " international",
                        " summit",
                        ".",
                        "\u010a",
                        "\u010a",
                        "Or",
                        " is",
                        " it",
                        " because",
                        " of",
                        " these",
                        " deeply",
                        " controversial",
                        " statements"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " years after he was jailed for espionage, before his release in 1969.\u010a\u010aHe",
                    "tokens": [
                        " years",
                        " after",
                        " he",
                        " was",
                        " jailed",
                        " for",
                        " espionage",
                        ",",
                        " before",
                        " his",
                        " release",
                        " in",
                        " 1969",
                        ".",
                        "\u010a",
                        "\u010a",
                        "He"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " faces with respect to women.\u010a\u010aA CNN poll released Monday showed Obama moving into",
                    "tokens": [
                        " faces",
                        " with",
                        " respect",
                        " to",
                        " women",
                        ".",
                        "\u010a",
                        "\u010a",
                        "A",
                        " CNN",
                        " poll",
                        " released",
                        " Monday",
                        " showed",
                        " Obama",
                        " moving",
                        " into"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 10.48886775970459
        },
        {
            "feature_index": 23,
            "gpt_predictions": [
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "phrases indicating accessibility to the public",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "\u00e2\u0122\u013f The talk is free and open to the public, at 5:45 pm",
                    "max_token": " to",
                    "tokens": [
                        "\u00e2\u0122",
                        "\u013f",
                        " The",
                        " talk",
                        " is",
                        " free",
                        " and",
                        " open",
                        " to",
                        " the",
                        " public",
                        ",",
                        " at",
                        " 5",
                        ":",
                        "45",
                        " pm"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        1.843157052993774,
                        3.044290065765381,
                        41.70375061035156,
                        23.08276748657227,
                        8.295818328857422,
                        5.873425006866455,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " passed laws enabling the establishment of libraries open to the public.\u010a\u010aYet, India",
                    "max_token": " to",
                    "tokens": [
                        " passed",
                        " laws",
                        " enabling",
                        " the",
                        " establishment",
                        " of",
                        " libraries",
                        " open",
                        " to",
                        " the",
                        " public",
                        ".",
                        "\u010a",
                        "\u010a",
                        "Yet",
                        ",",
                        " India"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0.08694688975811005,
                        0,
                        0,
                        0,
                        0,
                        36.59488677978516,
                        22.46848297119141,
                        10.84982395172119,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " see the Patent Office open up the process to those who might not be filing patents themselves",
                    "max_token": " to",
                    "tokens": [
                        " see",
                        " the",
                        " Patent",
                        " Office",
                        " open",
                        " up",
                        " the",
                        " process",
                        " to",
                        " those",
                        " who",
                        " might",
                        " not",
                        " be",
                        " filing",
                        " patents",
                        " themselves"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0.6820249557495117,
                        0,
                        32.24424362182617,
                        2.769473314285278,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": ", past and present, will be open to all players.\u010a\u010aFor those that",
                    "max_token": " to",
                    "tokens": [
                        ",",
                        " past",
                        " and",
                        " present",
                        ",",
                        " will",
                        " be",
                        " open",
                        " to",
                        " all",
                        " players",
                        ".",
                        "\u010a",
                        "\u010a",
                        "For",
                        " those",
                        " that"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0.7973553538322449,
                        44.37259674072266,
                        11.02454948425293,
                        5.897202491760254,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " Democrats. However, the debate remained open to all five party leaders and the Conservatives eventually",
                    "max_token": " to",
                    "tokens": [
                        " Democrats",
                        ".",
                        " However",
                        ",",
                        " the",
                        " debate",
                        " remained",
                        " open",
                        " to",
                        " all",
                        " five",
                        " party",
                        " leaders",
                        " and",
                        " the",
                        " Conservatives",
                        " eventually"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        37.74655914306641,
                        8.239481925964355,
                        0,
                        0.4596827030181885,
                        0.4372069835662842,
                        0.01279160007834435,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "ESPN, UniMas, UDN) before facing the 2014 FIFA World Cup<|endoftext|> can",
                    "tokens": [
                        "ESPN",
                        ",",
                        " Uni",
                        "Mas",
                        ",",
                        " U",
                        "DN",
                        ")",
                        " before",
                        " facing",
                        " the",
                        " 2014",
                        " FIFA",
                        " World",
                        " Cup",
                        "<|endoftext|>",
                        " can"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " vaccine industry, with the assistance of a sold-out government, is forcing vaccinations on",
                    "tokens": [
                        " vaccine",
                        " industry",
                        ",",
                        " with",
                        " the",
                        " assistance",
                        " of",
                        " a",
                        " sold",
                        "-",
                        "out",
                        " government",
                        ",",
                        " is",
                        " forcing",
                        " vaccinations",
                        " on"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": ", build and maintain effective student groups, advertise and rebrand conservative values, engage in",
                    "tokens": [
                        ",",
                        " build",
                        " and",
                        " maintain",
                        " effective",
                        " student",
                        " groups",
                        ",",
                        " advertise",
                        " and",
                        " re",
                        "brand",
                        " conservative",
                        " values",
                        ",",
                        " engage",
                        " in"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " political protest\u010a\u010aUpdated\u010a\u010aIt was one of the great art heists of",
                    "tokens": [
                        " political",
                        " protest",
                        "\u010a",
                        "\u010a",
                        "Updated",
                        "\u010a",
                        "\u010a",
                        "It",
                        " was",
                        " one",
                        " of",
                        " the",
                        " great",
                        " art",
                        " he",
                        "ists",
                        " of"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " that kid who felt unwanted, or the grown up who remembers how hard it was to",
                    "tokens": [
                        " that",
                        " kid",
                        " who",
                        " felt",
                        " unwanted",
                        ",",
                        " or",
                        " the",
                        " grown",
                        " up",
                        " who",
                        " remembers",
                        " how",
                        " hard",
                        " it",
                        " was",
                        " to"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 48.91312789916992
        },
        {
            "feature_index": 30,
            "gpt_predictions": [
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    1.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    1.0
                ]
            ],
            "description": "points or main ideas within the text",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "it would be excellent to get to the point where there is no longer impunity for (",
                    "max_token": " point",
                    "tokens": [
                        "it",
                        " would",
                        " be",
                        " excellent",
                        " to",
                        " get",
                        " to",
                        " the",
                        " point",
                        " where",
                        " there",
                        " is",
                        " no",
                        " longer",
                        " impunity",
                        " for",
                        " ("
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        23.5241813659668,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " Maybe none! That\u00e2\u0122\u013bs the point.\u010a\u010aAll of this flies in",
                    "max_token": " point",
                    "tokens": [
                        " Maybe",
                        " none",
                        "!",
                        " That",
                        "\u00e2\u0122",
                        "\u013b",
                        "s",
                        " the",
                        " point",
                        ".",
                        "\u010a",
                        "\u010a",
                        "All",
                        " of",
                        " this",
                        " flies",
                        " in"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        50.37979125976562,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " am scared of love, more to the point true intimacy; because it is one thing",
                    "max_token": " point",
                    "tokens": [
                        " am",
                        " scared",
                        " of",
                        " love",
                        ",",
                        " more",
                        " to",
                        " the",
                        " point",
                        " true",
                        " intimacy",
                        ";",
                        " because",
                        " it",
                        " is",
                        " one",
                        " thing"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        13.23785400390625,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " considerations.\u010a\u010aBut here's the point: the strip club on Murray Street is",
                    "max_token": " point",
                    "tokens": [
                        " considerations",
                        ".",
                        "\u010a",
                        "\u010a",
                        "But",
                        " here",
                        "'s",
                        " the",
                        " point",
                        ":",
                        " the",
                        " strip",
                        " club",
                        " on",
                        " Murray",
                        " Street",
                        " is"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        56.66488265991211,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "\u010aIn fact, that is really the point of this article. That the Gandhi-",
                    "max_token": " point",
                    "tokens": [
                        "\u010a",
                        "In",
                        " fact",
                        ",",
                        " that",
                        " is",
                        " really",
                        " the",
                        " point",
                        " of",
                        " this",
                        " article",
                        ".",
                        " That",
                        " the",
                        " Gandhi",
                        "-"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        55.21958541870117,
                        3.571071624755859,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "ESPN, UniMas, UDN) before facing the 2014 FIFA World Cup<|endoftext|> can",
                    "tokens": [
                        "ESPN",
                        ",",
                        " Uni",
                        "Mas",
                        ",",
                        " U",
                        "DN",
                        ")",
                        " before",
                        " facing",
                        " the",
                        " 2014",
                        " FIFA",
                        " World",
                        " Cup",
                        "<|endoftext|>",
                        " can"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " vaccine industry, with the assistance of a sold-out government, is forcing vaccinations on",
                    "tokens": [
                        " vaccine",
                        " industry",
                        ",",
                        " with",
                        " the",
                        " assistance",
                        " of",
                        " a",
                        " sold",
                        "-",
                        "out",
                        " government",
                        ",",
                        " is",
                        " forcing",
                        " vaccinations",
                        " on"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": ", build and maintain effective student groups, advertise and rebrand conservative values, engage in",
                    "tokens": [
                        ",",
                        " build",
                        " and",
                        " maintain",
                        " effective",
                        " student",
                        " groups",
                        ",",
                        " advertise",
                        " and",
                        " re",
                        "brand",
                        " conservative",
                        " values",
                        ",",
                        " engage",
                        " in"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " political protest\u010a\u010aUpdated\u010a\u010aIt was one of the great art heists of",
                    "tokens": [
                        " political",
                        " protest",
                        "\u010a",
                        "\u010a",
                        "Updated",
                        "\u010a",
                        "\u010a",
                        "It",
                        " was",
                        " one",
                        " of",
                        " the",
                        " great",
                        " art",
                        " he",
                        "ists",
                        " of"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " that kid who felt unwanted, or the grown up who remembers how hard it was to",
                    "tokens": [
                        " that",
                        " kid",
                        " who",
                        " felt",
                        " unwanted",
                        ",",
                        " or",
                        " the",
                        " grown",
                        " up",
                        " who",
                        " remembers",
                        " how",
                        " hard",
                        " it",
                        " was",
                        " to"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 63.10544586181641
        },
        {
            "feature_index": 617,
            "gpt_predictions": [
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    1.0
                ]
            ],
            "description": "terms related to social issues and political concepts",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": ", and--during lunch-time and the late afternoon--computer games.\u010a\u010a",
                    "max_token": " the",
                    "tokens": [
                        ",",
                        " and",
                        "--",
                        "during",
                        " lunch",
                        "-",
                        "time",
                        " and",
                        " the",
                        " late",
                        " afternoon",
                        "--",
                        "computer",
                        " games",
                        ".",
                        "\u010a",
                        "\u010a"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        11.7814359664917,
                        1.479962110519409,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " programs, economic policies, political priorities and the euro, individual European countries look altogether different",
                    "max_token": " the",
                    "tokens": [
                        " programs",
                        ",",
                        " economic",
                        " policies",
                        ",",
                        " political",
                        " priorities",
                        " and",
                        " the",
                        " euro",
                        ",",
                        " individual",
                        " European",
                        " countries",
                        " look",
                        " altogether",
                        " different"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        15.62819480895996,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "ifies construction, and minimizes or eliminates the need for mortar, thus reducing both the",
                    "max_token": " the",
                    "tokens": [
                        "ifies",
                        " construction",
                        ",",
                        " and",
                        " minim",
                        "izes",
                        " or",
                        " eliminates",
                        " the",
                        " need",
                        " for",
                        " mortar",
                        ",",
                        " thus",
                        " reducing",
                        " both",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        5.338282585144043,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        1.935667157173157
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " the intersections of race, gender, and the left, I asked four leftists of color",
                    "max_token": " the",
                    "tokens": [
                        " the",
                        " intersections",
                        " of",
                        " race",
                        ",",
                        " gender",
                        ",",
                        " and",
                        " the",
                        " left",
                        ",",
                        " I",
                        " asked",
                        " four",
                        " leftists",
                        " of",
                        " color"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        18.00368118286133,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " of roadblocks, gates, checkpoints, the Wall and other obstacles to movement in the",
                    "max_token": " the",
                    "tokens": [
                        " of",
                        " road",
                        "blocks",
                        ",",
                        " gates",
                        ",",
                        " checkpoints",
                        ",",
                        " the",
                        " Wall",
                        " and",
                        " other",
                        " obstacles",
                        " to",
                        " movement",
                        " in",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        17.67330932617188,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0.3263855576515198
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " quick lead only to have to bow out before the end of the game because they went",
                    "tokens": [
                        " quick",
                        " lead",
                        " only",
                        " to",
                        " have",
                        " to",
                        " bow",
                        " out",
                        " before",
                        " the",
                        " end",
                        " of",
                        " the",
                        " game",
                        " because",
                        " they",
                        " went"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "\u010aAldrin was a drug user before, Castillo admitted, but he had",
                    "tokens": [
                        "\u010a",
                        "A",
                        "ld",
                        "rin",
                        " was",
                        " a",
                        " drug",
                        " user",
                        " before",
                        ",",
                        " Cast",
                        "illo",
                        " admitted",
                        ",",
                        " but",
                        " he",
                        " had"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " height at some international summit.\u010a\u010aOr is it because of these deeply controversial statements",
                    "tokens": [
                        " height",
                        " at",
                        " some",
                        " international",
                        " summit",
                        ".",
                        "\u010a",
                        "\u010a",
                        "Or",
                        " is",
                        " it",
                        " because",
                        " of",
                        " these",
                        " deeply",
                        " controversial",
                        " statements"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " years after he was jailed for espionage, before his release in 1969.\u010a\u010aHe",
                    "tokens": [
                        " years",
                        " after",
                        " he",
                        " was",
                        " jailed",
                        " for",
                        " espionage",
                        ",",
                        " before",
                        " his",
                        " release",
                        " in",
                        " 1969",
                        ".",
                        "\u010a",
                        "\u010a",
                        "He"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " faces with respect to women.\u010a\u010aA CNN poll released Monday showed Obama moving into",
                    "tokens": [
                        " faces",
                        " with",
                        " respect",
                        " to",
                        " women",
                        ".",
                        "\u010a",
                        "\u010a",
                        "A",
                        " CNN",
                        " poll",
                        " released",
                        " Monday",
                        " showed",
                        " Obama",
                        " moving",
                        " into"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 21.4062671661377
        },
        {
            "feature_index": 10,
            "gpt_predictions": [
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    1.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "mentions of business-related terms and concepts",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " a hell of a franchise that they weren't really taking advantage of,\u00e2\u0122\u013f he",
                    "max_token": "'t",
                    "tokens": [
                        " a",
                        " hell",
                        " of",
                        " a",
                        " franchise",
                        " that",
                        " they",
                        " weren",
                        "'t",
                        " really",
                        " taking",
                        " advantage",
                        " of",
                        ",",
                        "\u00e2\u0122",
                        "\u013f",
                        " he"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        2.073591947555542,
                        3.025766372680664,
                        1.01378607749939,
                        4.577149868011475,
                        2.597209453582764,
                        2.997036218643188,
                        0,
                        1.698418617248535,
                        2.389986038208008,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " There is a market for it, but it is a niche market.\u010a\u010aK",
                    "max_token": " it",
                    "tokens": [
                        " There",
                        " is",
                        " a",
                        " market",
                        " for",
                        " it",
                        ",",
                        " but",
                        " it",
                        " is",
                        " a",
                        " niche",
                        " market",
                        ".",
                        "\u010a",
                        "\u010a",
                        "K"
                    ],
                    "values": [
                        4.783759117126465,
                        3.956760406494141,
                        3.483583211898804,
                        2.39358377456665,
                        1.973166108131409,
                        2.890912055969238,
                        3.466363668441772,
                        2.132380247116089,
                        5.771387577056885,
                        4.67353343963623,
                        5.28143310546875,
                        2.997722148895264,
                        2.742278337478638,
                        3.183762550354004,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " together. Free! is clearly not aimed at me, but I still enjoyed it because",
                    "max_token": " at",
                    "tokens": [
                        " together",
                        ".",
                        " Free",
                        "!",
                        " is",
                        " clearly",
                        " not",
                        " aimed",
                        " at",
                        " me",
                        ",",
                        " but",
                        " I",
                        " still",
                        " enjoyed",
                        " it",
                        " because"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        2.128807544708252,
                        0.02166435122489929,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " don\u00e2\u0122\u013bt have revenue, and without revenue you don\u00e2\u0122\u013bt hit profit",
                    "max_token": " without",
                    "tokens": [
                        " don",
                        "\u00e2\u0122",
                        "\u013b",
                        "t",
                        " have",
                        " revenue",
                        ",",
                        " and",
                        " without",
                        " revenue",
                        " you",
                        " don",
                        "\u00e2\u0122",
                        "\u013b",
                        "t",
                        " hit",
                        " profit"
                    ],
                    "values": [
                        0.526878297328949,
                        0,
                        0,
                        2.320152044296265,
                        1.480010509490967,
                        2.4524085521698,
                        5.143487930297852,
                        4.997438430786133,
                        6.196841716766357,
                        6.056609630584717,
                        4.100099563598633,
                        0,
                        0,
                        0.1345169097185135,
                        3.130489349365234,
                        2.384819984436035,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " buy an overpriced product they don't need from an insurance company is patriotism.\u010a",
                    "max_token": " need",
                    "tokens": [
                        " buy",
                        " an",
                        " over",
                        "priced",
                        " product",
                        " they",
                        " don",
                        "'t",
                        " need",
                        " from",
                        " an",
                        " insurance",
                        " company",
                        " is",
                        " patriotism",
                        ".",
                        "\u010a"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0.6346296668052673,
                        3.641487836837769,
                        0.01831088960170746,
                        2.767725229263306,
                        6.018251895904541,
                        1.446823000907898,
                        4.268200874328613,
                        0,
                        0.08660465478897095,
                        1.762612819671631,
                        1.08669912815094,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "ESPN, UniMas, UDN) before facing the 2014 FIFA World Cup<|endoftext|> can",
                    "tokens": [
                        "ESPN",
                        ",",
                        " Uni",
                        "Mas",
                        ",",
                        " U",
                        "DN",
                        ")",
                        " before",
                        " facing",
                        " the",
                        " 2014",
                        " FIFA",
                        " World",
                        " Cup",
                        "<|endoftext|>",
                        " can"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " vaccine industry, with the assistance of a sold-out government, is forcing vaccinations on",
                    "tokens": [
                        " vaccine",
                        " industry",
                        ",",
                        " with",
                        " the",
                        " assistance",
                        " of",
                        " a",
                        " sold",
                        "-",
                        "out",
                        " government",
                        ",",
                        " is",
                        " forcing",
                        " vaccinations",
                        " on"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": ", build and maintain effective student groups, advertise and rebrand conservative values, engage in",
                    "tokens": [
                        ",",
                        " build",
                        " and",
                        " maintain",
                        " effective",
                        " student",
                        " groups",
                        ",",
                        " advertise",
                        " and",
                        " re",
                        "brand",
                        " conservative",
                        " values",
                        ",",
                        " engage",
                        " in"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " political protest\u010a\u010aUpdated\u010a\u010aIt was one of the great art heists of",
                    "tokens": [
                        " political",
                        " protest",
                        "\u010a",
                        "\u010a",
                        "Updated",
                        "\u010a",
                        "\u010a",
                        "It",
                        " was",
                        " one",
                        " of",
                        " the",
                        " great",
                        " art",
                        " he",
                        "ists",
                        " of"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " that kid who felt unwanted, or the grown up who remembers how hard it was to",
                    "tokens": [
                        " that",
                        " kid",
                        " who",
                        " felt",
                        " unwanted",
                        ",",
                        " or",
                        " the",
                        " grown",
                        " up",
                        " who",
                        " remembers",
                        " how",
                        " hard",
                        " it",
                        " was",
                        " to"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 7.490533828735352
        },
        {
            "feature_index": 221,
            "gpt_predictions": [
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "historical events or timelines",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " authoritarian Indonesian leader, Suharto, in 1998, resulting in greater religious and political",
                    "max_token": " in",
                    "tokens": [
                        " authoritarian",
                        " Indonesian",
                        " leader",
                        ",",
                        " Su",
                        "hart",
                        "o",
                        ",",
                        " in",
                        " 1998",
                        ",",
                        " resulting",
                        " in",
                        " greater",
                        " religious",
                        " and",
                        " political"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        1.546947956085205,
                        0,
                        0,
                        0,
                        0,
                        17.79425430297852,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "\u010a\u010aWhen rebel forces toppled his regime in 2011, the stockpiles were thrown open",
                    "max_token": " in",
                    "tokens": [
                        "\u010a",
                        "\u010a",
                        "When",
                        " rebel",
                        " forces",
                        " toppled",
                        " his",
                        " regime",
                        " in",
                        " 2011",
                        ",",
                        " the",
                        " stockp",
                        "iles",
                        " were",
                        " thrown",
                        " open"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        18.56162452697754,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "some captured on video) of Michael Brown in Ferguson, Mo.; Laquan McDonald",
                    "max_token": " in",
                    "tokens": [
                        "some",
                        " captured",
                        " on",
                        " video",
                        ")",
                        " of",
                        " Michael",
                        " Brown",
                        " in",
                        " Ferguson",
                        ",",
                        " Mo",
                        ".;",
                        " La",
                        "qu",
                        "an",
                        " McDonald"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        20.81401252746582,
                        0,
                        10.26786422729492,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " since the explosion and fire at Chernobyl in Ukraine in 1986, led to widespread contamination",
                    "max_token": " in",
                    "tokens": [
                        " since",
                        " the",
                        " explosion",
                        " and",
                        " fire",
                        " at",
                        " Chern",
                        "obyl",
                        " in",
                        " Ukraine",
                        " in",
                        " 1986",
                        ",",
                        " led",
                        " to",
                        " widespread",
                        " contamination"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        22.52501106262207,
                        0.6398838758468628,
                        19.7540454864502,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "75, and the subsequent Declaration of Independence in 1776. The war ended in 17",
                    "max_token": " in",
                    "tokens": [
                        "75",
                        ",",
                        " and",
                        " the",
                        " subsequent",
                        " Declaration",
                        " of",
                        " Independence",
                        " in",
                        " 17",
                        "76",
                        ".",
                        " The",
                        " war",
                        " ended",
                        " in",
                        " 17"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0.0061350017786026,
                        25.01781845092773,
                        12.42274570465088,
                        0,
                        0,
                        0,
                        0,
                        0,
                        10.3803243637085,
                        6.237916469573975
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " Trump and Hillary in the first debate, before either said a word, Trump made what",
                    "tokens": [
                        " Trump",
                        " and",
                        " Hillary",
                        " in",
                        " the",
                        " first",
                        " debate",
                        ",",
                        " before",
                        " either",
                        " said",
                        " a",
                        " word",
                        ",",
                        " Trump",
                        " made",
                        " what"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " that kid who felt unwanted, or the grown up who remembers how hard it was to",
                    "tokens": [
                        " that",
                        " kid",
                        " who",
                        " felt",
                        " unwanted",
                        ",",
                        " or",
                        " the",
                        " grown",
                        " up",
                        " who",
                        " remembers",
                        " how",
                        " hard",
                        " it",
                        " was",
                        " to"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "\u010aAldrin was a drug user before, Castillo admitted, but he had",
                    "tokens": [
                        "\u010a",
                        "A",
                        "ld",
                        "rin",
                        " was",
                        " a",
                        " drug",
                        " user",
                        " before",
                        ",",
                        " Cast",
                        "illo",
                        " admitted",
                        ",",
                        " but",
                        " he",
                        " had"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " who genuinely seem not to believe reality. Or Democrats, cowed into silence on issues",
                    "tokens": [
                        " who",
                        " genuinely",
                        " seem",
                        " not",
                        " to",
                        " believe",
                        " reality",
                        ".",
                        " Or",
                        " Democrats",
                        ",",
                        " c",
                        "owed",
                        " into",
                        " silence",
                        " on",
                        " issues"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " the incident, saying the man threatened him before leaving.\u010a\u010a\u00e2\u0122\u013eI think",
                    "tokens": [
                        " the",
                        " incident",
                        ",",
                        " saying",
                        " the",
                        " man",
                        " threatened",
                        " him",
                        " before",
                        " leaving",
                        ".",
                        "\u010a",
                        "\u010a",
                        "\u00e2\u0122",
                        "\u013e",
                        "I",
                        " think"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 30.89262771606445
        },
        {
            "feature_index": 820,
            "gpt_predictions": [
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    0,
                    1.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    1.0
                ],
                [
                    0,
                    1.0
                ]
            ],
            "description": "phrases that seem to be randomly generated or lack coherent meaning",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "00 Philippine Sports Commission (PSC) Php 182,313,000.00",
                    "max_token": " Ph",
                    "tokens": [
                        "00",
                        " Philippine",
                        " Sports",
                        " Commission",
                        " (",
                        "PS",
                        "C",
                        ")",
                        " Ph",
                        "p",
                        " 182",
                        ",",
                        "313",
                        ",",
                        "000",
                        ".",
                        "00"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        14.1274471282959,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " questions and give advice on plant selection. \u00ef\u00bb\u00bf\u010a\u010aAll sale proceeds support",
                    "max_token": " \u00ef",
                    "tokens": [
                        " questions",
                        " and",
                        " give",
                        " advice",
                        " on",
                        " plant",
                        " selection",
                        ".",
                        " \u00ef",
                        "\u00bb",
                        "\u00bf",
                        "\u010a",
                        "\u010a",
                        "All",
                        " sale",
                        " proceeds",
                        " support"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        18.84597206115723,
                        0,
                        2.435437440872192,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " g(class){ }; true threw exception SyntaxError: Cannot use the reserved word",
                    "max_token": " Synt",
                    "tokens": [
                        " g",
                        "(",
                        "class",
                        "){",
                        " };",
                        " true",
                        " threw",
                        " exception",
                        " Synt",
                        "ax",
                        "Error",
                        ":",
                        " Cannot",
                        " use",
                        " the",
                        " reserved",
                        " word"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        18.46637535095215,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " Apostacon at apostacon.org. Question: is the child/student tickets in",
                    "max_token": " Question",
                    "tokens": [
                        " Apost",
                        "acon",
                        " at",
                        " apost",
                        "acon",
                        ".",
                        "org",
                        ".",
                        " Question",
                        ":",
                        " is",
                        " the",
                        " child",
                        "/",
                        "student",
                        " tickets",
                        " in"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        22.4482536315918,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "EBP-E70],ECX 1000B7CC . 33C0 X",
                    "max_token": " 1000",
                    "tokens": [
                        "E",
                        "BP",
                        "-",
                        "E",
                        "70",
                        "],",
                        "EC",
                        "X",
                        " 1000",
                        "B",
                        "7",
                        "CC",
                        " .",
                        " 33",
                        "C",
                        "0",
                        " X"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        20.03401947021484,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " coat.\u010a\u010aDetective Inspector Steve Christian, from Liverpool CID, said:",
                    "tokens": [
                        " coat",
                        ".",
                        "\u010a",
                        "\u010a",
                        "Det",
                        "ective",
                        " Inspector",
                        " Steve",
                        " Christian",
                        ",",
                        " from",
                        " Liverpool",
                        " C",
                        "ID",
                        ",",
                        " said",
                        ":"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " drop out of the presidential race, according to NBC News ' Andrea Mitchell.\u010a\u010a",
                    "tokens": [
                        " drop",
                        " out",
                        " of",
                        " the",
                        " presidential",
                        " race",
                        ",",
                        " according",
                        " to",
                        " NBC",
                        " News",
                        " '",
                        " Andrea",
                        " Mitchell",
                        ".",
                        "\u010a",
                        "\u010a"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " Rumors that Amazon will soon begin accepting Bitcoin are persistent. If the retail giant does",
                    "tokens": [
                        " Rum",
                        "ors",
                        " that",
                        " Amazon",
                        " will",
                        " soon",
                        " begin",
                        " accepting",
                        " Bitcoin",
                        " are",
                        " persistent",
                        ".",
                        " If",
                        " the",
                        " retail",
                        " giant",
                        " does"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": ", and banking flows was a monthly net TIC (Treasury International Capital)<|endoftext|>",
                    "tokens": [
                        ",",
                        " and",
                        " banking",
                        " flows",
                        " was",
                        " a",
                        " monthly",
                        " net",
                        " T",
                        "IC",
                        " (",
                        "Tre",
                        "asury",
                        " International",
                        " Capital",
                        ")",
                        "<|endoftext|>"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " faces with respect to women.\u010a\u010aA CNN poll released Monday showed Obama moving into",
                    "tokens": [
                        " faces",
                        " with",
                        " respect",
                        " to",
                        " women",
                        ".",
                        "\u010a",
                        "\u010a",
                        "A",
                        " CNN",
                        " poll",
                        " released",
                        " Monday",
                        " showed",
                        " Obama",
                        " moving",
                        " into"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 31.34087181091309
        },
        {
            "feature_index": 296,
            "gpt_predictions": [
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "names of cities and places",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 9,
                    "sentence_string": " Pok\u00c3\u00a9mon title, is officially out worldwide, and that means thousands of budding trainers will be",
                    "max_token": " that",
                    "tokens": [
                        " Pok\u00c3\u00a9mon",
                        " title",
                        ",",
                        " is",
                        " officially",
                        " out",
                        " worldwide",
                        ",",
                        " and",
                        " that",
                        " means",
                        " thousands",
                        " of",
                        " budding",
                        " trainers",
                        " will",
                        " be"
                    ],
                    "values": [
                        0.4460476636886597,
                        0.9114803671836853,
                        3.939152240753174,
                        3.671143770217896,
                        0,
                        0,
                        0,
                        4.57602071762085,
                        7.003044605255127,
                        7.024696826934814,
                        2.686491250991821,
                        0,
                        1.112548828125,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " would usher in a new era of racism? Remember how stupid that was?\u010a\u010a",
                    "max_token": "?",
                    "tokens": [
                        " would",
                        " usher",
                        " in",
                        " a",
                        " new",
                        " era",
                        " of",
                        " racism",
                        "?",
                        " Remember",
                        " how",
                        " stupid",
                        " that",
                        " was",
                        "?",
                        "\u010a",
                        "\u010a"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        8.706448554992676,
                        1.13297700881958,
                        1.962912678718567,
                        1.38158118724823,
                        1.384738922119141,
                        0,
                        2.75066065788269,
                        1.841389417648315,
                        3.802542924880981
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 9,
                    "sentence_string": " heard of ARK: Survival Evolved, that's about to change. The game",
                    "max_token": " that",
                    "tokens": [
                        " heard",
                        " of",
                        " AR",
                        "K",
                        ":",
                        " Survival",
                        " Ev",
                        "olved",
                        ",",
                        " that",
                        "'s",
                        " about",
                        " to",
                        " change",
                        ".",
                        " The",
                        " game"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0.8630356788635254,
                        7.92231273651123,
                        8.534213066101074,
                        5.247824192047119,
                        0.09421785175800323,
                        0,
                        0,
                        0.1028440594673157,
                        1.455195069313049,
                        1.804517149925232
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " kicked off the Summer of Rift Sale, and the momentum of game launches, announcements,",
                    "max_token": " and",
                    "tokens": [
                        " kicked",
                        " off",
                        " the",
                        " Summer",
                        " of",
                        " Rift",
                        " Sale",
                        ",",
                        " and",
                        " the",
                        " momentum",
                        " of",
                        " game",
                        " launches",
                        ",",
                        " announcements",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0.5136309266090393,
                        8.678645133972168,
                        10.31584644317627,
                        5.255669116973877,
                        0.7309227585792542,
                        2.65215539932251,
                        0,
                        0.5780500769615173,
                        4.039223670959473,
                        0.1241106167435646,
                        2.445516347885132
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " or two for the last couple of months. Most of the changes haven't been very",
                    "max_token": ".",
                    "tokens": [
                        " or",
                        " two",
                        " for",
                        " the",
                        " last",
                        " couple",
                        " of",
                        " months",
                        ".",
                        " Most",
                        " of",
                        " the",
                        " changes",
                        " haven",
                        "'t",
                        " been",
                        " very"
                    ],
                    "values": [
                        0.6025421023368835,
                        1.374567151069641,
                        2.717178344726562,
                        0.9917951226234436,
                        0,
                        0.3597261905670166,
                        0,
                        4.133777141571045,
                        9.25666332244873,
                        5.125489711761475,
                        5.90070104598999,
                        5.029334545135498,
                        2.34590220451355,
                        2.596648693084717,
                        1.707043647766113,
                        0,
                        0.02291792631149292
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " Trump and Hillary in the first debate, before either said a word, Trump made what",
                    "tokens": [
                        " Trump",
                        " and",
                        " Hillary",
                        " in",
                        " the",
                        " first",
                        " debate",
                        ",",
                        " before",
                        " either",
                        " said",
                        " a",
                        " word",
                        ",",
                        " Trump",
                        " made",
                        " what"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " that kid who felt unwanted, or the grown up who remembers how hard it was to",
                    "tokens": [
                        " that",
                        " kid",
                        " who",
                        " felt",
                        " unwanted",
                        ",",
                        " or",
                        " the",
                        " grown",
                        " up",
                        " who",
                        " remembers",
                        " how",
                        " hard",
                        " it",
                        " was",
                        " to"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "\u010aAldrin was a drug user before, Castillo admitted, but he had",
                    "tokens": [
                        "\u010a",
                        "A",
                        "ld",
                        "rin",
                        " was",
                        " a",
                        " drug",
                        " user",
                        " before",
                        ",",
                        " Cast",
                        "illo",
                        " admitted",
                        ",",
                        " but",
                        " he",
                        " had"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " who genuinely seem not to believe reality. Or Democrats, cowed into silence on issues",
                    "tokens": [
                        " who",
                        " genuinely",
                        " seem",
                        " not",
                        " to",
                        " believe",
                        " reality",
                        ".",
                        " Or",
                        " Democrats",
                        ",",
                        " c",
                        "owed",
                        " into",
                        " silence",
                        " on",
                        " issues"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " the incident, saying the man threatened him before leaving.\u010a\u010a\u00e2\u0122\u013eI think",
                    "tokens": [
                        " the",
                        " incident",
                        ",",
                        " saying",
                        " the",
                        " man",
                        " threatened",
                        " him",
                        " before",
                        " leaving",
                        ".",
                        "\u010a",
                        "\u010a",
                        "\u00e2\u0122",
                        "\u013e",
                        "I",
                        " think"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 14.74485397338867
        },
        {
            "feature_index": 54,
            "gpt_predictions": [
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "phrases related to genetics and DNA, particularly focusing on the word \"gene.\"",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " such cultural variables<|endoftext|> borders less readily than genes. There are very few consistent genetic differences",
                    "max_token": " genes",
                    "tokens": [
                        " such",
                        " cultural",
                        " variables",
                        "<|endoftext|>",
                        " borders",
                        " less",
                        " readily",
                        " than",
                        " genes",
                        ".",
                        " There",
                        " are",
                        " very",
                        " few",
                        " consistent",
                        " genetic",
                        " differences"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        37.02486419677734,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0.515317440032959,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "right asymmetric expression of the dpp gene) could be the most likely candidate for",
                    "max_token": " gene",
                    "tokens": [
                        "right",
                        " asymm",
                        "etric",
                        " expression",
                        " of",
                        " the",
                        " d",
                        "pp",
                        " gene",
                        ")",
                        " could",
                        " be",
                        " the",
                        " most",
                        " likely",
                        " candidate",
                        " for"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        4.174747943878174,
                        0,
                        0,
                        0,
                        0,
                        46.08839416503906,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " the dynamics of the interacting defects in the genome and in the proteome can be written",
                    "max_token": " genome",
                    "tokens": [
                        " the",
                        " dynamics",
                        " of",
                        " the",
                        " interacting",
                        " defects",
                        " in",
                        " the",
                        " genome",
                        " and",
                        " in",
                        " the",
                        " prote",
                        "ome",
                        " can",
                        " be",
                        " written"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        2.161602973937988,
                        0,
                        0,
                        14.04998302459717,
                        0,
                        0,
                        0,
                        11.9063196182251,
                        3.200996160507202,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "\u010a\u010aThe researchers also showed that the gene FoxP, active in a small set",
                    "max_token": " gene",
                    "tokens": [
                        "\u010a",
                        "\u010a",
                        "The",
                        " researchers",
                        " also",
                        " showed",
                        " that",
                        " the",
                        " gene",
                        " Fox",
                        "P",
                        ",",
                        " active",
                        " in",
                        " a",
                        " small",
                        " set"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        55.85538101196289,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " happens if a scientist mutates a particular gene or \u00e2\u0122\u013eturns off\u00e2\u0122\u013f",
                    "max_token": " gene",
                    "tokens": [
                        " happens",
                        " if",
                        " a",
                        " scientist",
                        " mut",
                        "ates",
                        " a",
                        " particular",
                        " gene",
                        " or",
                        " \u00e2\u0122",
                        "\u013e",
                        "turn",
                        "s",
                        " off",
                        "\u00e2\u0122",
                        "\u013f"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        51.99342346191406,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "ESPN, UniMas, UDN) before facing the 2014 FIFA World Cup<|endoftext|> can",
                    "tokens": [
                        "ESPN",
                        ",",
                        " Uni",
                        "Mas",
                        ",",
                        " U",
                        "DN",
                        ")",
                        " before",
                        " facing",
                        " the",
                        " 2014",
                        " FIFA",
                        " World",
                        " Cup",
                        "<|endoftext|>",
                        " can"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " vaccine industry, with the assistance of a sold-out government, is forcing vaccinations on",
                    "tokens": [
                        " vaccine",
                        " industry",
                        ",",
                        " with",
                        " the",
                        " assistance",
                        " of",
                        " a",
                        " sold",
                        "-",
                        "out",
                        " government",
                        ",",
                        " is",
                        " forcing",
                        " vaccinations",
                        " on"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": ", build and maintain effective student groups, advertise and rebrand conservative values, engage in",
                    "tokens": [
                        ",",
                        " build",
                        " and",
                        " maintain",
                        " effective",
                        " student",
                        " groups",
                        ",",
                        " advertise",
                        " and",
                        " re",
                        "brand",
                        " conservative",
                        " values",
                        ",",
                        " engage",
                        " in"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " political protest\u010a\u010aUpdated\u010a\u010aIt was one of the great art heists of",
                    "tokens": [
                        " political",
                        " protest",
                        "\u010a",
                        "\u010a",
                        "Updated",
                        "\u010a",
                        "\u010a",
                        "It",
                        " was",
                        " one",
                        " of",
                        " the",
                        " great",
                        " art",
                        " he",
                        "ists",
                        " of"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " that kid who felt unwanted, or the grown up who remembers how hard it was to",
                    "tokens": [
                        " that",
                        " kid",
                        " who",
                        " felt",
                        " unwanted",
                        ",",
                        " or",
                        " the",
                        " grown",
                        " up",
                        " who",
                        " remembers",
                        " how",
                        " hard",
                        " it",
                        " was",
                        " to"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 61.39282608032227
        },
        {
            "feature_index": 542,
            "gpt_predictions": [
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "phrases related to statistical margins of error",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "\u013f can be white, gray, black, honey, amber or water colors. fl",
                    "max_token": ",",
                    "tokens": [
                        "\u013f",
                        " can",
                        " be",
                        " white",
                        ",",
                        " gray",
                        ",",
                        " black",
                        ",",
                        " honey",
                        ",",
                        " amber",
                        " or",
                        " water",
                        " colors",
                        ".",
                        " fl"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0.6006473302841187,
                        0,
                        7.213735103607178,
                        0,
                        7.214187145233154,
                        0,
                        2.319928646087646,
                        0,
                        3.632871627807617,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 3,
                    "sentence_string": ", Very Good, Good, Fair, or Poor?\u00e2\u0122\u013f\u00e2\u0122\u0136and a single",
                    "max_token": ",",
                    "tokens": [
                        ",",
                        " Very",
                        " Good",
                        ",",
                        " Good",
                        ",",
                        " Fair",
                        ",",
                        " or",
                        " Poor",
                        "?",
                        "\u00e2\u0122",
                        "\u013f",
                        "\u00e2\u0122\u0136",
                        "and",
                        " a",
                        " single"
                    ],
                    "values": [
                        5.549039363861084,
                        3.043114185333252,
                        1.546980381011963,
                        13.62166118621826,
                        5.425189018249512,
                        12.14288520812988,
                        2.640298843383789,
                        9.848930358886719,
                        12.98224639892578,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " a woman that is either not as attractive or treats him like shit because she was in",
                    "max_token": " or",
                    "tokens": [
                        " a",
                        " woman",
                        " that",
                        " is",
                        " either",
                        " not",
                        " as",
                        " attractive",
                        " or",
                        " treats",
                        " him",
                        " like",
                        " shit",
                        " because",
                        " she",
                        " was",
                        " in"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        4.442543983459473,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " results into a rating system ranging from A to D. Communities with A ratings represented the",
                    "max_token": " to",
                    "tokens": [
                        " results",
                        " into",
                        " a",
                        " rating",
                        " system",
                        " ranging",
                        " from",
                        " A",
                        " to",
                        " D",
                        ".",
                        " Communities",
                        " with",
                        " A",
                        " ratings",
                        " represented",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        3.127610206604004,
                        0,
                        10.67281818389893,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " than (eg, the participant reported fair/poor health and the physician assessed the participant",
                    "max_token": "/",
                    "tokens": [
                        " than",
                        " (",
                        "eg",
                        ",",
                        " the",
                        " participant",
                        " reported",
                        " fair",
                        "/",
                        "poor",
                        " health",
                        " and",
                        " the",
                        " physician",
                        " assessed",
                        " the",
                        " participant"
                    ],
                    "values": [
                        1.074720621109009,
                        7.472472667694092,
                        0,
                        1.104475140571594,
                        0,
                        0,
                        3.308032035827637,
                        3.300323486328125,
                        14.22863006591797,
                        0,
                        1.313123226165771,
                        1.191304922103882,
                        0,
                        0,
                        0,
                        0,
                        1.149514675140381
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " quick lead only to have to bow out before the end of the game because they went",
                    "tokens": [
                        " quick",
                        " lead",
                        " only",
                        " to",
                        " have",
                        " to",
                        " bow",
                        " out",
                        " before",
                        " the",
                        " end",
                        " of",
                        " the",
                        " game",
                        " because",
                        " they",
                        " went"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "\u010aAldrin was a drug user before, Castillo admitted, but he had",
                    "tokens": [
                        "\u010a",
                        "A",
                        "ld",
                        "rin",
                        " was",
                        " a",
                        " drug",
                        " user",
                        " before",
                        ",",
                        " Cast",
                        "illo",
                        " admitted",
                        ",",
                        " but",
                        " he",
                        " had"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " height at some international summit.\u010a\u010aOr is it because of these deeply controversial statements",
                    "tokens": [
                        " height",
                        " at",
                        " some",
                        " international",
                        " summit",
                        ".",
                        "\u010a",
                        "\u010a",
                        "Or",
                        " is",
                        " it",
                        " because",
                        " of",
                        " these",
                        " deeply",
                        " controversial",
                        " statements"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " years after he was jailed for espionage, before his release in 1969.\u010a\u010aHe",
                    "tokens": [
                        " years",
                        " after",
                        " he",
                        " was",
                        " jailed",
                        " for",
                        " espionage",
                        ",",
                        " before",
                        " his",
                        " release",
                        " in",
                        " 1969",
                        ".",
                        "\u010a",
                        "\u010a",
                        "He"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " faces with respect to women.\u010a\u010aA CNN poll released Monday showed Obama moving into",
                    "tokens": [
                        " faces",
                        " with",
                        " respect",
                        " to",
                        " women",
                        ".",
                        "\u010a",
                        "\u010a",
                        "A",
                        " CNN",
                        " poll",
                        " released",
                        " Monday",
                        " showed",
                        " Obama",
                        " moving",
                        " into"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 18.42534446716309
        },
        {
            "feature_index": 209,
            "gpt_predictions": [
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    1.0
                ],
                [
                    0,
                    1.0
                ],
                [
                    0,
                    1.0
                ],
                [
                    0,
                    1.0
                ]
            ],
            "description": "words signaling conflicting or contrasting information",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " with no gameplay to convince them? (Or do you have a demo?)\u010a\u010a",
                    "max_token": "Or",
                    "tokens": [
                        " with",
                        " no",
                        " gameplay",
                        " to",
                        " convince",
                        " them",
                        "?",
                        " (",
                        "Or",
                        " do",
                        " you",
                        " have",
                        " a",
                        " demo",
                        "?)",
                        "\u010a",
                        "\u010a"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        41.84932708740234,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " somehow going to make us more free. Or why forcing millions of families to pay thousands",
                    "max_token": " Or",
                    "tokens": [
                        " somehow",
                        " going",
                        " to",
                        " make",
                        " us",
                        " more",
                        " free",
                        ".",
                        " Or",
                        " why",
                        " forcing",
                        " millions",
                        " of",
                        " families",
                        " to",
                        " pay",
                        " thousands"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        51.76192855834961,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " a votre bienveillance\" or in English, \"Given that I am",
                    "max_token": " or",
                    "tokens": [
                        " a",
                        " vot",
                        "re",
                        " b",
                        "ien",
                        "ve",
                        "illance",
                        "\"",
                        " or",
                        " in",
                        " English",
                        ",",
                        " \"",
                        "Given",
                        " that",
                        " I",
                        " am"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        14.95826244354248,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "mosphere\u00e2\u0122\u013f of a space. Or, what we like to think of as",
                    "max_token": " Or",
                    "tokens": [
                        "mosp",
                        "here",
                        "\u00e2\u0122",
                        "\u013f",
                        " of",
                        " a",
                        " space",
                        ".",
                        " Or",
                        ",",
                        " what",
                        " we",
                        " like",
                        " to",
                        " think",
                        " of",
                        " as"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        59.78366470336914,
                        15.52479839324951,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " responsibility is to the team.\"\u010a\u010aOrton has devolved into an inaccurate check",
                    "max_token": "Or",
                    "tokens": [
                        " responsibility",
                        " is",
                        " to",
                        " the",
                        " team",
                        ".\"",
                        "\u010a",
                        "\u010a",
                        "Or",
                        "ton",
                        " has",
                        " dev",
                        "olved",
                        " into",
                        " an",
                        " inaccurate",
                        " check"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        57.88204574584961,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "ESPN, UniMas, UDN) before facing the 2014 FIFA World Cup<|endoftext|> can",
                    "tokens": [
                        "ESPN",
                        ",",
                        " Uni",
                        "Mas",
                        ",",
                        " U",
                        "DN",
                        ")",
                        " before",
                        " facing",
                        " the",
                        " 2014",
                        " FIFA",
                        " World",
                        " Cup",
                        "<|endoftext|>",
                        " can"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " vaccine industry, with the assistance of a sold-out government, is forcing vaccinations on",
                    "tokens": [
                        " vaccine",
                        " industry",
                        ",",
                        " with",
                        " the",
                        " assistance",
                        " of",
                        " a",
                        " sold",
                        "-",
                        "out",
                        " government",
                        ",",
                        " is",
                        " forcing",
                        " vaccinations",
                        " on"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": ", build and maintain effective student groups, advertise and rebrand conservative values, engage in",
                    "tokens": [
                        ",",
                        " build",
                        " and",
                        " maintain",
                        " effective",
                        " student",
                        " groups",
                        ",",
                        " advertise",
                        " and",
                        " re",
                        "brand",
                        " conservative",
                        " values",
                        ",",
                        " engage",
                        " in"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " political protest\u010a\u010aUpdated\u010a\u010aIt was one of the great art heists of",
                    "tokens": [
                        " political",
                        " protest",
                        "\u010a",
                        "\u010a",
                        "Updated",
                        "\u010a",
                        "\u010a",
                        "It",
                        " was",
                        " one",
                        " of",
                        " the",
                        " great",
                        " art",
                        " he",
                        "ists",
                        " of"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " that kid who felt unwanted, or the grown up who remembers how hard it was to",
                    "tokens": [
                        " that",
                        " kid",
                        " who",
                        " felt",
                        " unwanted",
                        ",",
                        " or",
                        " the",
                        " grown",
                        " up",
                        " who",
                        " remembers",
                        " how",
                        " hard",
                        " it",
                        " was",
                        " to"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 68.2912826538086
        },
        {
            "feature_index": 604,
            "gpt_predictions": [
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "words related to Serbian culture or history",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " voce della Luna.\u010a\u010aSerafini\u00e2\u0122\u013bs amazing studio<|endoftext|>",
                    "max_token": "Ser",
                    "tokens": [
                        " vo",
                        "ce",
                        " de",
                        "lla",
                        " Luna",
                        ".",
                        "\u010a",
                        "\u010a",
                        "Ser",
                        "af",
                        "ini",
                        "\u00e2\u0122",
                        "\u013b",
                        "s",
                        " amazing",
                        " studio",
                        "<|endoftext|>"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        62.23730087280273,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": ". \u00e2\u0122\u013eBecause,\u00e2\u0122\u013f replied Serpico, if I had just walked",
                    "max_token": " Ser",
                    "tokens": [
                        ".",
                        " \u00e2\u0122",
                        "\u013e",
                        "Because",
                        ",",
                        "\u00e2\u0122",
                        "\u013f",
                        " replied",
                        " Ser",
                        "p",
                        "ico",
                        ",",
                        " if",
                        " I",
                        " had",
                        " just",
                        " walked"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        67.66822814941406,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " it \u00e2\u0122\u0136 the US has been bringing in Serbs and Georgians experienced in non-",
                    "max_token": " Ser",
                    "tokens": [
                        " it",
                        " \u00e2\u0122\u0136",
                        " the",
                        " US",
                        " has",
                        " been",
                        " bringing",
                        " in",
                        " Ser",
                        "bs",
                        " and",
                        " Georg",
                        "ians",
                        " experienced",
                        " in",
                        " non",
                        "-"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        74.2169189453125,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " filmmaker Federico Fellini, to whom Serafini offered a series of drawings for",
                    "max_token": " Ser",
                    "tokens": [
                        " filmmaker",
                        " Feder",
                        "ico",
                        " Fell",
                        "ini",
                        ",",
                        " to",
                        " whom",
                        " Ser",
                        "af",
                        "ini",
                        " offered",
                        " a",
                        " series",
                        " of",
                        " drawings",
                        " for"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        75.33587646484375,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " ]\u010a\u010aBulgarian unification and Serbo-Bulgarian War\u010a\u010a",
                    "max_token": " Ser",
                    "tokens": [
                        " ]",
                        "\u010a",
                        "\u010a",
                        "Bul",
                        "g",
                        "arian",
                        " unification",
                        " and",
                        " Ser",
                        "bo",
                        "-",
                        "Bul",
                        "g",
                        "arian",
                        " War",
                        "\u010a",
                        "\u010a"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        77.26522827148438,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " quick lead only to have to bow out before the end of the game because they went",
                    "tokens": [
                        " quick",
                        " lead",
                        " only",
                        " to",
                        " have",
                        " to",
                        " bow",
                        " out",
                        " before",
                        " the",
                        " end",
                        " of",
                        " the",
                        " game",
                        " because",
                        " they",
                        " went"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "\u010aAldrin was a drug user before, Castillo admitted, but he had",
                    "tokens": [
                        "\u010a",
                        "A",
                        "ld",
                        "rin",
                        " was",
                        " a",
                        " drug",
                        " user",
                        " before",
                        ",",
                        " Cast",
                        "illo",
                        " admitted",
                        ",",
                        " but",
                        " he",
                        " had"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " height at some international summit.\u010a\u010aOr is it because of these deeply controversial statements",
                    "tokens": [
                        " height",
                        " at",
                        " some",
                        " international",
                        " summit",
                        ".",
                        "\u010a",
                        "\u010a",
                        "Or",
                        " is",
                        " it",
                        " because",
                        " of",
                        " these",
                        " deeply",
                        " controversial",
                        " statements"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " years after he was jailed for espionage, before his release in 1969.\u010a\u010aHe",
                    "tokens": [
                        " years",
                        " after",
                        " he",
                        " was",
                        " jailed",
                        " for",
                        " espionage",
                        ",",
                        " before",
                        " his",
                        " release",
                        " in",
                        " 1969",
                        ".",
                        "\u010a",
                        "\u010a",
                        "He"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " faces with respect to women.\u010a\u010aA CNN poll released Monday showed Obama moving into",
                    "tokens": [
                        " faces",
                        " with",
                        " respect",
                        " to",
                        " women",
                        ".",
                        "\u010a",
                        "\u010a",
                        "A",
                        " CNN",
                        " poll",
                        " released",
                        " Monday",
                        " showed",
                        " Obama",
                        " moving",
                        " into"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 79.00758361816406
        },
        {
            "feature_index": 692,
            "gpt_predictions": [
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "phrases indicating comparison or evaluation, focusing on the outcome or result",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " lets the story be told in a way that is naturally occurring.\u00e2\u0122\u013f\u010a\u010a",
                    "max_token": " that",
                    "tokens": [
                        " lets",
                        " the",
                        " story",
                        " be",
                        " told",
                        " in",
                        " a",
                        " way",
                        " that",
                        " is",
                        " naturally",
                        " occurring",
                        ".",
                        "\u00e2\u0122",
                        "\u013f",
                        "\u010a",
                        "\u010a"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        25.76190948486328,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " the Arizona Cardinals on Sunday and did something that many would consider very disrespectful.\u010a\u010a",
                    "max_token": " that",
                    "tokens": [
                        " the",
                        " Arizona",
                        " Cardinals",
                        " on",
                        " Sunday",
                        " and",
                        " did",
                        " something",
                        " that",
                        " many",
                        " would",
                        " consider",
                        " very",
                        " disrespectful",
                        ".",
                        "\u010a",
                        "\u010a"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        31.86639404296875,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " ushering in an era of divided government that has led to repeated standoffs over taxes",
                    "max_token": " that",
                    "tokens": [
                        " usher",
                        "ing",
                        " in",
                        " an",
                        " era",
                        " of",
                        " divided",
                        " government",
                        " that",
                        " has",
                        " led",
                        " to",
                        " repeated",
                        " stand",
                        "offs",
                        " over",
                        " taxes"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        9.12602710723877,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " ideas, thoughts, and emotions in ways that serve them. Therefore, unless we make",
                    "max_token": " that",
                    "tokens": [
                        " ideas",
                        ",",
                        " thoughts",
                        ",",
                        " and",
                        " emotions",
                        " in",
                        " ways",
                        " that",
                        " serve",
                        " them",
                        ".",
                        " Therefore",
                        ",",
                        " unless",
                        " we",
                        " make"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        6.709101676940918,
                        35.54756546020508,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " pay and were reprimanded in a way that doesn\u00e2\u0122\u013bt limit their future activities",
                    "max_token": " that",
                    "tokens": [
                        " pay",
                        " and",
                        " were",
                        " reprim",
                        "anded",
                        " in",
                        " a",
                        " way",
                        " that",
                        " doesn",
                        "\u00e2\u0122",
                        "\u013b",
                        "t",
                        " limit",
                        " their",
                        " future",
                        " activities"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        34.43329620361328,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " quick lead only to have to bow out before the end of the game because they went",
                    "tokens": [
                        " quick",
                        " lead",
                        " only",
                        " to",
                        " have",
                        " to",
                        " bow",
                        " out",
                        " before",
                        " the",
                        " end",
                        " of",
                        " the",
                        " game",
                        " because",
                        " they",
                        " went"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "\u010aAldrin was a drug user before, Castillo admitted, but he had",
                    "tokens": [
                        "\u010a",
                        "A",
                        "ld",
                        "rin",
                        " was",
                        " a",
                        " drug",
                        " user",
                        " before",
                        ",",
                        " Cast",
                        "illo",
                        " admitted",
                        ",",
                        " but",
                        " he",
                        " had"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " height at some international summit.\u010a\u010aOr is it because of these deeply controversial statements",
                    "tokens": [
                        " height",
                        " at",
                        " some",
                        " international",
                        " summit",
                        ".",
                        "\u010a",
                        "\u010a",
                        "Or",
                        " is",
                        " it",
                        " because",
                        " of",
                        " these",
                        " deeply",
                        " controversial",
                        " statements"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " years after he was jailed for espionage, before his release in 1969.\u010a\u010aHe",
                    "tokens": [
                        " years",
                        " after",
                        " he",
                        " was",
                        " jailed",
                        " for",
                        " espionage",
                        ",",
                        " before",
                        " his",
                        " release",
                        " in",
                        " 1969",
                        ".",
                        "\u010a",
                        "\u010a",
                        "He"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " faces with respect to women.\u010a\u010aA CNN poll released Monday showed Obama moving into",
                    "tokens": [
                        " faces",
                        " with",
                        " respect",
                        " to",
                        " women",
                        ".",
                        "\u010a",
                        "\u010a",
                        "A",
                        " CNN",
                        " poll",
                        " released",
                        " Monday",
                        " showed",
                        " Obama",
                        " moving",
                        " into"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 43.74796295166016
        },
        {
            "feature_index": 662,
            "gpt_predictions": [
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "numerical values related to quantities or limits",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " limit of medical marijuana dispensaries from four to six. The Apothecarium and Berkeley",
                    "max_token": " six",
                    "tokens": [
                        " limit",
                        " of",
                        " medical",
                        " marijuana",
                        " dispensaries",
                        " from",
                        " four",
                        " to",
                        " six",
                        ".",
                        " The",
                        " Ap",
                        "othe",
                        "car",
                        "ium",
                        " and",
                        " Berkeley"
                    ],
                    "values": [
                        0,
                        2.735258340835571,
                        0,
                        0,
                        0,
                        0,
                        12.3233699798584,
                        0,
                        13.6997537612915,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " $1,000 and as much as six months in jail. Going past homeowners and",
                    "max_token": " six",
                    "tokens": [
                        " $",
                        "1",
                        ",",
                        "000",
                        " and",
                        " as",
                        " much",
                        " as",
                        " six",
                        " months",
                        " in",
                        " jail",
                        ".",
                        " Going",
                        " past",
                        " homeowners",
                        " and"
                    ],
                    "values": [
                        2.746324062347412,
                        1.655964970588684,
                        0.3694043457508087,
                        0.5436257123947144,
                        0,
                        0,
                        2.733747959136963,
                        0,
                        14.18374824523926,
                        1.152942180633545,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " to be off the wheel for up to three minutes while following a car at highway speeds",
                    "max_token": " three",
                    "tokens": [
                        " to",
                        " be",
                        " off",
                        " the",
                        " wheel",
                        " for",
                        " up",
                        " to",
                        " three",
                        " minutes",
                        " while",
                        " following",
                        " a",
                        " car",
                        " at",
                        " highway",
                        " speeds"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        4.839183330535889,
                        16.72072792053223,
                        4.209568977355957,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " rechargeable batteries that last for up to four hours. It can be used with or",
                    "max_token": " four",
                    "tokens": [
                        " recharge",
                        "able",
                        " batteries",
                        " that",
                        " last",
                        " for",
                        " up",
                        " to",
                        " four",
                        " hours",
                        ".",
                        " It",
                        " can",
                        " be",
                        " used",
                        " with",
                        " or"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        4.919373512268066,
                        18.21491050720215,
                        4.503236770629883,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 1,
                    "sentence_string": " to six plants and possess up to one ounce of usable marijuana for his own consumption,",
                    "max_token": " six",
                    "tokens": [
                        " to",
                        " six",
                        " plants",
                        " and",
                        " possess",
                        " up",
                        " to",
                        " one",
                        " ounce",
                        " of",
                        " usable",
                        " marijuana",
                        " for",
                        " his",
                        " own",
                        " consumption",
                        ","
                    ],
                    "values": [
                        2.316314697265625,
                        20.60229873657227,
                        3.887819766998291,
                        0,
                        0,
                        4.305776119232178,
                        9.592511177062988,
                        16.17242240905762,
                        11.0258731842041,
                        5.412356376647949,
                        0.6582893133163452,
                        3.234410762786865,
                        1.138221621513367,
                        2.428598403930664,
                        2.912848711013794,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " quick lead only to have to bow out before the end of the game because they went",
                    "tokens": [
                        " quick",
                        " lead",
                        " only",
                        " to",
                        " have",
                        " to",
                        " bow",
                        " out",
                        " before",
                        " the",
                        " end",
                        " of",
                        " the",
                        " game",
                        " because",
                        " they",
                        " went"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "\u010aAldrin was a drug user before, Castillo admitted, but he had",
                    "tokens": [
                        "\u010a",
                        "A",
                        "ld",
                        "rin",
                        " was",
                        " a",
                        " drug",
                        " user",
                        " before",
                        ",",
                        " Cast",
                        "illo",
                        " admitted",
                        ",",
                        " but",
                        " he",
                        " had"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " height at some international summit.\u010a\u010aOr is it because of these deeply controversial statements",
                    "tokens": [
                        " height",
                        " at",
                        " some",
                        " international",
                        " summit",
                        ".",
                        "\u010a",
                        "\u010a",
                        "Or",
                        " is",
                        " it",
                        " because",
                        " of",
                        " these",
                        " deeply",
                        " controversial",
                        " statements"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " years after he was jailed for espionage, before his release in 1969.\u010a\u010aHe",
                    "tokens": [
                        " years",
                        " after",
                        " he",
                        " was",
                        " jailed",
                        " for",
                        " espionage",
                        ",",
                        " before",
                        " his",
                        " release",
                        " in",
                        " 1969",
                        ".",
                        "\u010a",
                        "\u010a",
                        "He"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " faces with respect to women.\u010a\u010aA CNN poll released Monday showed Obama moving into",
                    "tokens": [
                        " faces",
                        " with",
                        " respect",
                        " to",
                        " women",
                        ".",
                        "\u010a",
                        "\u010a",
                        "A",
                        " CNN",
                        " poll",
                        " released",
                        " Monday",
                        " showed",
                        " Obama",
                        " moving",
                        " into"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 23.23741912841797
        },
        {
            "feature_index": 866,
            "gpt_predictions": [
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    0,
                    1.0
                ],
                [
                    0,
                    1.0
                ],
                [
                    0,
                    1.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    1.0
                ]
            ],
            "description": "phrases related to news or events",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " comment.<|endoftext|>For the past few months, my team have been working on a<|endoftext|>",
                    "max_token": ",",
                    "tokens": [
                        " comment",
                        ".",
                        "<|endoftext|>",
                        "For",
                        " the",
                        " past",
                        " few",
                        " months",
                        ",",
                        " my",
                        " team",
                        " have",
                        " been",
                        " working",
                        " on",
                        " a",
                        "<|endoftext|>"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        13.93795013427734,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " pilot?<|endoftext|>Since his election in 2008, many on the right have believed that the",
                    "max_token": ",",
                    "tokens": [
                        " pilot",
                        "?",
                        "<|endoftext|>",
                        "Since",
                        " his",
                        " election",
                        " in",
                        " 2008",
                        ",",
                        " many",
                        " on",
                        " the",
                        " right",
                        " have",
                        " believed",
                        " that",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        16.42732810974121,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0.04584553092718124,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "1:00pm EDT) on Wednesday, April 23rd, 2014 will be eligible",
                    "max_token": ",",
                    "tokens": [
                        "1",
                        ":",
                        "00",
                        "pm",
                        " EDT",
                        ")",
                        " on",
                        " Wednesday",
                        ",",
                        " April",
                        " 23",
                        "rd",
                        ",",
                        " 2014",
                        " will",
                        " be",
                        " eligible"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        4.669582366943359,
                        0,
                        0,
                        0,
                        0.2655119895935059,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " directorial debut The Cabin in the Woods, which he also co-wrote with J",
                    "max_token": ",",
                    "tokens": [
                        " director",
                        "ial",
                        " debut",
                        " The",
                        " Cabin",
                        " in",
                        " the",
                        " Woods",
                        ",",
                        " which",
                        " he",
                        " also",
                        " co",
                        "-",
                        "wrote",
                        " with",
                        " J"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        19.34963226318359,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " gonorrhea outbreak in the Pacific Northwest, public health officials have taken on the messy",
                    "max_token": ",",
                    "tokens": [
                        " gon",
                        "orr",
                        "hea",
                        " outbreak",
                        " in",
                        " the",
                        " Pacific",
                        " Northwest",
                        ",",
                        " public",
                        " health",
                        " officials",
                        " have",
                        " taken",
                        " on",
                        " the",
                        " messy"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        18.79365539550781,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " coat.\u010a\u010aDetective Inspector Steve Christian, from Liverpool CID, said:",
                    "tokens": [
                        " coat",
                        ".",
                        "\u010a",
                        "\u010a",
                        "Det",
                        "ective",
                        " Inspector",
                        " Steve",
                        " Christian",
                        ",",
                        " from",
                        " Liverpool",
                        " C",
                        "ID",
                        ",",
                        " said",
                        ":"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " drop out of the presidential race, according to NBC News ' Andrea Mitchell.\u010a\u010a",
                    "tokens": [
                        " drop",
                        " out",
                        " of",
                        " the",
                        " presidential",
                        " race",
                        ",",
                        " according",
                        " to",
                        " NBC",
                        " News",
                        " '",
                        " Andrea",
                        " Mitchell",
                        ".",
                        "\u010a",
                        "\u010a"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " Rumors that Amazon will soon begin accepting Bitcoin are persistent. If the retail giant does",
                    "tokens": [
                        " Rum",
                        "ors",
                        " that",
                        " Amazon",
                        " will",
                        " soon",
                        " begin",
                        " accepting",
                        " Bitcoin",
                        " are",
                        " persistent",
                        ".",
                        " If",
                        " the",
                        " retail",
                        " giant",
                        " does"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": ", and banking flows was a monthly net TIC (Treasury International Capital)<|endoftext|>",
                    "tokens": [
                        ",",
                        " and",
                        " banking",
                        " flows",
                        " was",
                        " a",
                        " monthly",
                        " net",
                        " T",
                        "IC",
                        " (",
                        "Tre",
                        "asury",
                        " International",
                        " Capital",
                        ")",
                        "<|endoftext|>"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " faces with respect to women.\u010a\u010aA CNN poll released Monday showed Obama moving into",
                    "tokens": [
                        " faces",
                        " with",
                        " respect",
                        " to",
                        " women",
                        ".",
                        "\u010a",
                        "\u010a",
                        "A",
                        " CNN",
                        " poll",
                        " released",
                        " Monday",
                        " showed",
                        " Obama",
                        " moving",
                        " into"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 23.25111389160156
        },
        {
            "feature_index": 70,
            "gpt_predictions": [
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "numerical information or technological terms",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "\u00e2\u0122\u0136-4th Place, Player C: Demise True DracoCard Strike, Local Tournament",
                    "max_token": " Dem",
                    "tokens": [
                        "\u00e2\u0122\u0136-",
                        "4",
                        "th",
                        " Place",
                        ",",
                        " Player",
                        " C",
                        ":",
                        " Dem",
                        "ise",
                        " True",
                        " Draco",
                        "Card",
                        " Strike",
                        ",",
                        " Local",
                        " Tournament"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        19.90665435791016,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "\u010aThe phonological system of the North Frisian dialects is strongly being influenced",
                    "max_token": " Fr",
                    "tokens": [
                        "\u010a",
                        "The",
                        " phon",
                        "ological",
                        " system",
                        " of",
                        " the",
                        " North",
                        " Fr",
                        "is",
                        "ian",
                        " dialect",
                        "s",
                        " is",
                        " strongly",
                        " being",
                        " influenced"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        20.38755226135254,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " helped everyone at the club,\" said Lloris. \"He has a proper philosophy",
                    "max_token": "lor",
                    "tokens": [
                        " helped",
                        " everyone",
                        " at",
                        " the",
                        " club",
                        ",\"",
                        " said",
                        " L",
                        "lor",
                        "is",
                        ".",
                        " \"",
                        "He",
                        " has",
                        " a",
                        " proper",
                        " philosophy"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        22.84058952331543,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " Dargis (@ManohlaDargis) July 13, 2017\u010a\u010a",
                    "max_token": "arg",
                    "tokens": [
                        " D",
                        "arg",
                        "is",
                        " (@",
                        "Man",
                        "oh",
                        "la",
                        "D",
                        "arg",
                        "is",
                        ")",
                        " July",
                        " 13",
                        ",",
                        " 2017",
                        "\u010a",
                        "\u010a"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        24.46297073364258,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " more data to check Facebook and read CILISOS? Who dowan? \u00f0\u0141\u013a",
                    "max_token": "IL",
                    "tokens": [
                        " more",
                        " data",
                        " to",
                        " check",
                        " Facebook",
                        " and",
                        " read",
                        " C",
                        "IL",
                        "IS",
                        "OS",
                        "?",
                        " Who",
                        " d",
                        "owan",
                        "?",
                        " \u00f0\u0141\u013a"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        28.06285285949707,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "ESPN, UniMas, UDN) before facing the 2014 FIFA World Cup<|endoftext|> can",
                    "tokens": [
                        "ESPN",
                        ",",
                        " Uni",
                        "Mas",
                        ",",
                        " U",
                        "DN",
                        ")",
                        " before",
                        " facing",
                        " the",
                        " 2014",
                        " FIFA",
                        " World",
                        " Cup",
                        "<|endoftext|>",
                        " can"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " vaccine industry, with the assistance of a sold-out government, is forcing vaccinations on",
                    "tokens": [
                        " vaccine",
                        " industry",
                        ",",
                        " with",
                        " the",
                        " assistance",
                        " of",
                        " a",
                        " sold",
                        "-",
                        "out",
                        " government",
                        ",",
                        " is",
                        " forcing",
                        " vaccinations",
                        " on"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": ", build and maintain effective student groups, advertise and rebrand conservative values, engage in",
                    "tokens": [
                        ",",
                        " build",
                        " and",
                        " maintain",
                        " effective",
                        " student",
                        " groups",
                        ",",
                        " advertise",
                        " and",
                        " re",
                        "brand",
                        " conservative",
                        " values",
                        ",",
                        " engage",
                        " in"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " political protest\u010a\u010aUpdated\u010a\u010aIt was one of the great art heists of",
                    "tokens": [
                        " political",
                        " protest",
                        "\u010a",
                        "\u010a",
                        "Updated",
                        "\u010a",
                        "\u010a",
                        "It",
                        " was",
                        " one",
                        " of",
                        " the",
                        " great",
                        " art",
                        " he",
                        "ists",
                        " of"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " that kid who felt unwanted, or the grown up who remembers how hard it was to",
                    "tokens": [
                        " that",
                        " kid",
                        " who",
                        " felt",
                        " unwanted",
                        ",",
                        " or",
                        " the",
                        " grown",
                        " up",
                        " who",
                        " remembers",
                        " how",
                        " hard",
                        " it",
                        " was",
                        " to"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 31.83901214599609
        },
        {
            "feature_index": 543,
            "gpt_predictions": [
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "specific numerical patterns",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": ".\u010a\u010aUpper Arlington, OH 43221\u010a\u010aMonday, October 31\u010a",
                    "max_token": " 43",
                    "tokens": [
                        ".",
                        "\u010a",
                        "\u010a",
                        "U",
                        "pper",
                        " Arlington",
                        ",",
                        " OH",
                        " 43",
                        "221",
                        "\u010a",
                        "\u010a",
                        "Monday",
                        ",",
                        " October",
                        " 31",
                        "\u010a"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        17.66351699829102,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "350 votes) Tae Takemi (343 votes) Sojiro Sakura (279",
                    "max_token": "343",
                    "tokens": [
                        "350",
                        " votes",
                        ")",
                        " T",
                        "ae",
                        " Take",
                        "mi",
                        " (",
                        "343",
                        " votes",
                        ")",
                        " So",
                        "j",
                        "iro",
                        " Sakura",
                        " (",
                        "279"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        13.61698246002197,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " us at 9:47 on Saturday the 23rd.\u010a\u010aAMY GOODMAN: So",
                    "max_token": " 23",
                    "tokens": [
                        " us",
                        " at",
                        " 9",
                        ":",
                        "47",
                        " on",
                        " Saturday",
                        " the",
                        " 23",
                        "rd",
                        ".",
                        "\u010a",
                        "\u010a",
                        "AMY",
                        " GOODMAN",
                        ":",
                        " So"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        13.21758460998535,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " Challenger classUSS Chekov NCC-53702 Springfield classUSS Firebrand NCC",
                    "max_token": "53",
                    "tokens": [
                        " Challenger",
                        " class",
                        "USS",
                        " Che",
                        "kov",
                        " N",
                        "CC",
                        "-",
                        "53",
                        "702",
                        " Springfield",
                        " class",
                        "USS",
                        " Fire",
                        "brand",
                        " N",
                        "CC"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        19.6700439453125,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " that issue BEFORE Amazing Spider-Man #53 (which came out the same month as",
                    "max_token": "53",
                    "tokens": [
                        " that",
                        " issue",
                        " BEFORE",
                        " Amazing",
                        " Spider",
                        "-",
                        "Man",
                        " #",
                        "53",
                        " (",
                        "which",
                        " came",
                        " out",
                        " the",
                        " same",
                        " month",
                        " as"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        15.81826972961426,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " quick lead only to have to bow out before the end of the game because they went",
                    "tokens": [
                        " quick",
                        " lead",
                        " only",
                        " to",
                        " have",
                        " to",
                        " bow",
                        " out",
                        " before",
                        " the",
                        " end",
                        " of",
                        " the",
                        " game",
                        " because",
                        " they",
                        " went"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "\u010aAldrin was a drug user before, Castillo admitted, but he had",
                    "tokens": [
                        "\u010a",
                        "A",
                        "ld",
                        "rin",
                        " was",
                        " a",
                        " drug",
                        " user",
                        " before",
                        ",",
                        " Cast",
                        "illo",
                        " admitted",
                        ",",
                        " but",
                        " he",
                        " had"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " height at some international summit.\u010a\u010aOr is it because of these deeply controversial statements",
                    "tokens": [
                        " height",
                        " at",
                        " some",
                        " international",
                        " summit",
                        ".",
                        "\u010a",
                        "\u010a",
                        "Or",
                        " is",
                        " it",
                        " because",
                        " of",
                        " these",
                        " deeply",
                        " controversial",
                        " statements"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " years after he was jailed for espionage, before his release in 1969.\u010a\u010aHe",
                    "tokens": [
                        " years",
                        " after",
                        " he",
                        " was",
                        " jailed",
                        " for",
                        " espionage",
                        ",",
                        " before",
                        " his",
                        " release",
                        " in",
                        " 1969",
                        ".",
                        "\u010a",
                        "\u010a",
                        "He"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " faces with respect to women.\u010a\u010aA CNN poll released Monday showed Obama moving into",
                    "tokens": [
                        " faces",
                        " with",
                        " respect",
                        " to",
                        " women",
                        ".",
                        "\u010a",
                        "\u010a",
                        "A",
                        " CNN",
                        " poll",
                        " released",
                        " Monday",
                        " showed",
                        " Obama",
                        " moving",
                        " into"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 21.24277305603027
        },
        {
            "feature_index": 107,
            "gpt_predictions": [
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "mentions instructing or receiving something in a document, potentially related to terms of use or agreements",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " and promotions from The New York Times. You may opt-out at any time.",
                    "max_token": " You",
                    "tokens": [
                        " and",
                        " promotions",
                        " from",
                        " The",
                        " New",
                        " York",
                        " Times",
                        ".",
                        " You",
                        " may",
                        " opt",
                        "-",
                        "out",
                        " at",
                        " any",
                        " time",
                        "."
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        51.32375717163086,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " a news editor for theScore esports. You can follow him on Twitter.<|endoftext|>The",
                    "max_token": " You",
                    "tokens": [
                        " a",
                        " news",
                        " editor",
                        " for",
                        " the",
                        "Score",
                        " esports",
                        ".",
                        " You",
                        " can",
                        " follow",
                        " him",
                        " on",
                        " Twitter",
                        ".",
                        "<|endoftext|>",
                        "The"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        23.64199066162109,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " Start Download Corporate E-mail Address: You forgot to provide an Email Address. This",
                    "max_token": " You",
                    "tokens": [
                        " Start",
                        " Download",
                        " Corporate",
                        " E",
                        "-",
                        "mail",
                        " Address",
                        ":",
                        " You",
                        " forgot",
                        " to",
                        " provide",
                        " an",
                        " Email",
                        " Address",
                        ".",
                        " This"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        34.19683456420898,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        3.685574293136597
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " a newsletter to subscribe to. Sign Up You will receive emails containing news content , updates",
                    "max_token": " You",
                    "tokens": [
                        " a",
                        " newsletter",
                        " to",
                        " subscribe",
                        " to",
                        ".",
                        " Sign",
                        " Up",
                        " You",
                        " will",
                        " receive",
                        " emails",
                        " containing",
                        " news",
                        " content",
                        " ,",
                        " updates"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        39.2236213684082,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " inbox. Email Sign Up By signing up you agree to receive email newsletters or alerts from",
                    "max_token": " you",
                    "tokens": [
                        " inbox",
                        ".",
                        " Email",
                        " Sign",
                        " Up",
                        " By",
                        " signing",
                        " up",
                        " you",
                        " agree",
                        " to",
                        " receive",
                        " email",
                        " newsletters",
                        " or",
                        " alerts",
                        " from"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        21.24243354797363,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "ESPN, UniMas, UDN) before facing the 2014 FIFA World Cup<|endoftext|> can",
                    "tokens": [
                        "ESPN",
                        ",",
                        " Uni",
                        "Mas",
                        ",",
                        " U",
                        "DN",
                        ")",
                        " before",
                        " facing",
                        " the",
                        " 2014",
                        " FIFA",
                        " World",
                        " Cup",
                        "<|endoftext|>",
                        " can"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " vaccine industry, with the assistance of a sold-out government, is forcing vaccinations on",
                    "tokens": [
                        " vaccine",
                        " industry",
                        ",",
                        " with",
                        " the",
                        " assistance",
                        " of",
                        " a",
                        " sold",
                        "-",
                        "out",
                        " government",
                        ",",
                        " is",
                        " forcing",
                        " vaccinations",
                        " on"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": ", build and maintain effective student groups, advertise and rebrand conservative values, engage in",
                    "tokens": [
                        ",",
                        " build",
                        " and",
                        " maintain",
                        " effective",
                        " student",
                        " groups",
                        ",",
                        " advertise",
                        " and",
                        " re",
                        "brand",
                        " conservative",
                        " values",
                        ",",
                        " engage",
                        " in"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " political protest\u010a\u010aUpdated\u010a\u010aIt was one of the great art heists of",
                    "tokens": [
                        " political",
                        " protest",
                        "\u010a",
                        "\u010a",
                        "Updated",
                        "\u010a",
                        "\u010a",
                        "It",
                        " was",
                        " one",
                        " of",
                        " the",
                        " great",
                        " art",
                        " he",
                        "ists",
                        " of"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " that kid who felt unwanted, or the grown up who remembers how hard it was to",
                    "tokens": [
                        " that",
                        " kid",
                        " who",
                        " felt",
                        " unwanted",
                        ",",
                        " or",
                        " the",
                        " grown",
                        " up",
                        " who",
                        " remembers",
                        " how",
                        " hard",
                        " it",
                        " was",
                        " to"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 57.76604080200195
        },
        {
            "feature_index": 493,
            "gpt_predictions": [
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "references to political entities or government positions",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "ents and purposes, commandeered U.S. industry to win the war in",
                    "max_token": ".",
                    "tokens": [
                        "ents",
                        " and",
                        " purposes",
                        ",",
                        " command",
                        "e",
                        "ered",
                        " U",
                        ".",
                        "S",
                        ".",
                        " industry",
                        " to",
                        " win",
                        " the",
                        " war",
                        " in"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        18.47649383544922,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "China is second largest importer of U.S. agricultural products behind Canada, with",
                    "max_token": ".",
                    "tokens": [
                        "China",
                        " is",
                        " second",
                        " largest",
                        " imp",
                        "orter",
                        " of",
                        " U",
                        ".",
                        "S",
                        ".",
                        " agricultural",
                        " products",
                        " behind",
                        " Canada",
                        ",",
                        " with"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        2.99110221862793,
                        24.34479904174805,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " military action in South Sudan to protect U.S. citizens, personnel and property.",
                    "max_token": ".",
                    "tokens": [
                        " military",
                        " action",
                        " in",
                        " South",
                        " Sudan",
                        " to",
                        " protect",
                        " U",
                        ".",
                        "S",
                        ".",
                        " citizens",
                        ",",
                        " personnel",
                        " and",
                        " property",
                        "."
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        4.190378189086914,
                        23.11874198913574,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": ". Another plus for cities in the U.S. and Europe, thanks to their",
                    "max_token": ".",
                    "tokens": [
                        ".",
                        " Another",
                        " plus",
                        " for",
                        " cities",
                        " in",
                        " the",
                        " U",
                        ".",
                        "S",
                        ".",
                        " and",
                        " Europe",
                        ",",
                        " thanks",
                        " to",
                        " their"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0.236900806427002,
                        30.22654342651367,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "85 and lasted until 28 November [O.S. 16 November] 1885.",
                    "max_token": ".",
                    "tokens": [
                        "85",
                        " and",
                        " lasted",
                        " until",
                        " 28",
                        " November",
                        " [",
                        "O",
                        ".",
                        "S",
                        ".",
                        " 16",
                        " November",
                        "]",
                        " 18",
                        "85",
                        "."
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        26.13498115539551,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " quick lead only to have to bow out before the end of the game because they went",
                    "tokens": [
                        " quick",
                        " lead",
                        " only",
                        " to",
                        " have",
                        " to",
                        " bow",
                        " out",
                        " before",
                        " the",
                        " end",
                        " of",
                        " the",
                        " game",
                        " because",
                        " they",
                        " went"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "\u010aAldrin was a drug user before, Castillo admitted, but he had",
                    "tokens": [
                        "\u010a",
                        "A",
                        "ld",
                        "rin",
                        " was",
                        " a",
                        " drug",
                        " user",
                        " before",
                        ",",
                        " Cast",
                        "illo",
                        " admitted",
                        ",",
                        " but",
                        " he",
                        " had"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " height at some international summit.\u010a\u010aOr is it because of these deeply controversial statements",
                    "tokens": [
                        " height",
                        " at",
                        " some",
                        " international",
                        " summit",
                        ".",
                        "\u010a",
                        "\u010a",
                        "Or",
                        " is",
                        " it",
                        " because",
                        " of",
                        " these",
                        " deeply",
                        " controversial",
                        " statements"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " years after he was jailed for espionage, before his release in 1969.\u010a\u010aHe",
                    "tokens": [
                        " years",
                        " after",
                        " he",
                        " was",
                        " jailed",
                        " for",
                        " espionage",
                        ",",
                        " before",
                        " his",
                        " release",
                        " in",
                        " 1969",
                        ".",
                        "\u010a",
                        "\u010a",
                        "He"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " faces with respect to women.\u010a\u010aA CNN poll released Monday showed Obama moving into",
                    "tokens": [
                        " faces",
                        " with",
                        " respect",
                        " to",
                        " women",
                        ".",
                        "\u010a",
                        "\u010a",
                        "A",
                        " CNN",
                        " poll",
                        " released",
                        " Monday",
                        " showed",
                        " Obama",
                        " moving",
                        " into"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 39.82904434204102
        },
        {
            "feature_index": 590,
            "gpt_predictions": [
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "image and HTML code related phrases",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 15,
                    "sentence_string": " transform-style: preserve-3d; transform-origin: center top; transform",
                    "max_token": ";",
                    "tokens": [
                        " transform",
                        "-",
                        "style",
                        ":",
                        " preserve",
                        "-",
                        "3",
                        "d",
                        ";",
                        " transform",
                        "-",
                        "origin",
                        ":",
                        " center",
                        " top",
                        ";",
                        " transform"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        20.61129379272461,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        24.0617733001709,
                        4.262228488922119
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "\",\"code\":null,\"level\":\"error\",\"spans\":[],\"children\":[],\"rendered",
                    "max_token": "\",\"",
                    "tokens": [
                        "\",\"",
                        "code",
                        "\":",
                        "null",
                        ",\"",
                        "level",
                        "\":\"",
                        "error",
                        "\",\"",
                        "sp",
                        "ans",
                        "\":[",
                        "],\"",
                        "children",
                        "\":[",
                        "],\"",
                        "rendered"
                    ],
                    "values": [
                        15.26379203796387,
                        0,
                        0,
                        5.379652500152588,
                        20.63217926025391,
                        0,
                        0,
                        0,
                        27.5273323059082,
                        0,
                        0,
                        0,
                        18.82923698425293,
                        0,
                        0,
                        16.5643310546875,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 11,
                    "sentence_string": "65\\x78\\x4F\\x66\u00e2\u0122\u00b3,\u00e2\u0122\u013f\\x",
                    "max_token": "\u00e2\u0122\u00b3",
                    "tokens": [
                        "65",
                        "\\",
                        "x",
                        "78",
                        "\\",
                        "x",
                        "4",
                        "F",
                        "\\",
                        "x",
                        "66",
                        "\u00e2\u0122\u00b3",
                        ",",
                        "\u00e2\u0122",
                        "\u013f",
                        "\\",
                        "x"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        2.467353105545044,
                        0,
                        0,
                        0,
                        4.901997089385986,
                        0,
                        0,
                        7.794970512390137,
                        4.861971855163574,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " frameborder=\"0\" width=\"100%\" height=\"240px\"></iframe>\u010a",
                    "max_token": "%\"",
                    "tokens": [
                        " frame",
                        "border",
                        "=\"",
                        "0",
                        "\"",
                        " width",
                        "=\"",
                        "100",
                        "%\"",
                        " height",
                        "=\"",
                        "240",
                        "px",
                        "\"></",
                        "iframe",
                        ">",
                        "\u010a"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        23.09663772583008,
                        0,
                        0,
                        2.870465517044067,
                        30.61655235290527,
                        0,
                        0,
                        5.722907066345215,
                        10.92804431915283,
                        0,
                        3.962636947631836,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "=FFFFFF&target=myspace\" /><param name=\"movie\" value",
                    "max_token": "\"",
                    "tokens": [
                        "=",
                        "FFFF",
                        "FF",
                        "&",
                        "target",
                        "=",
                        "mys",
                        "pace",
                        "\"",
                        " /",
                        "><",
                        "param",
                        " name",
                        "=\"",
                        "movie",
                        "\"",
                        " value"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        14.97086906433105,
                        0,
                        0,
                        0,
                        0,
                        29.23829460144043,
                        14.1094274520874,
                        0.08554305881261826,
                        0,
                        0,
                        0,
                        0,
                        7.531324863433838,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " quick lead only to have to bow out before the end of the game because they went",
                    "tokens": [
                        " quick",
                        " lead",
                        " only",
                        " to",
                        " have",
                        " to",
                        " bow",
                        " out",
                        " before",
                        " the",
                        " end",
                        " of",
                        " the",
                        " game",
                        " because",
                        " they",
                        " went"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "\u010aAldrin was a drug user before, Castillo admitted, but he had",
                    "tokens": [
                        "\u010a",
                        "A",
                        "ld",
                        "rin",
                        " was",
                        " a",
                        " drug",
                        " user",
                        " before",
                        ",",
                        " Cast",
                        "illo",
                        " admitted",
                        ",",
                        " but",
                        " he",
                        " had"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " height at some international summit.\u010a\u010aOr is it because of these deeply controversial statements",
                    "tokens": [
                        " height",
                        " at",
                        " some",
                        " international",
                        " summit",
                        ".",
                        "\u010a",
                        "\u010a",
                        "Or",
                        " is",
                        " it",
                        " because",
                        " of",
                        " these",
                        " deeply",
                        " controversial",
                        " statements"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " years after he was jailed for espionage, before his release in 1969.\u010a\u010aHe",
                    "tokens": [
                        " years",
                        " after",
                        " he",
                        " was",
                        " jailed",
                        " for",
                        " espionage",
                        ",",
                        " before",
                        " his",
                        " release",
                        " in",
                        " 1969",
                        ".",
                        "\u010a",
                        "\u010a",
                        "He"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " faces with respect to women.\u010a\u010aA CNN poll released Monday showed Obama moving into",
                    "tokens": [
                        " faces",
                        " with",
                        " respect",
                        " to",
                        " women",
                        ".",
                        "\u010a",
                        "\u010a",
                        "A",
                        " CNN",
                        " poll",
                        " released",
                        " Monday",
                        " showed",
                        " Obama",
                        " moving",
                        " into"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 34.29157638549805
        },
        {
            "feature_index": 741,
            "gpt_predictions": [
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "instances of the word \"before\" followed by an action",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " connection with the burn injury the boy suffered before his death, WTAE reported.",
                    "max_token": " before",
                    "tokens": [
                        " connection",
                        " with",
                        " the",
                        " burn",
                        " injury",
                        " the",
                        " boy",
                        " suffered",
                        " before",
                        " his",
                        " death",
                        ",",
                        " W",
                        "TA",
                        "E",
                        " reported",
                        "."
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        21.57716751098633,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " years after he was jailed for espionage, before his release in 1969.\u010a\u010aHe",
                    "max_token": " before",
                    "tokens": [
                        " years",
                        " after",
                        " he",
                        " was",
                        " jailed",
                        " for",
                        " espionage",
                        ",",
                        " before",
                        " his",
                        " release",
                        " in",
                        " 1969",
                        ".",
                        "\u010a",
                        "\u010a",
                        "He"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        49.31813812255859,
                        1.860223650932312,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " now, and were close to a deal before HomeStory Cup VI. Winning HSC",
                    "max_token": " before",
                    "tokens": [
                        " now",
                        ",",
                        " and",
                        " were",
                        " close",
                        " to",
                        " a",
                        " deal",
                        " before",
                        " Home",
                        "Story",
                        " Cup",
                        " VI",
                        ".",
                        " Winning",
                        " H",
                        "SC"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        12.9178466796875,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " attacked students who went to her aid, before being subdued.\u010a\u010aPolice said it",
                    "max_token": " before",
                    "tokens": [
                        " attacked",
                        " students",
                        " who",
                        " went",
                        " to",
                        " her",
                        " aid",
                        ",",
                        " before",
                        " being",
                        " subdued",
                        ".",
                        "\u010a",
                        "\u010a",
                        "Police",
                        " said",
                        " it"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        55.42427444458008,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " the West Coast near Salem, Oregon, before traveling cross-country to Charleston, South",
                    "max_token": " before",
                    "tokens": [
                        " the",
                        " West",
                        " Coast",
                        " near",
                        " Salem",
                        ",",
                        " Oregon",
                        ",",
                        " before",
                        " traveling",
                        " cross",
                        "-",
                        "country",
                        " to",
                        " Charleston",
                        ",",
                        " South"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        53.80146789550781,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " coat.\u010a\u010aDetective Inspector Steve Christian, from Liverpool CID, said:",
                    "tokens": [
                        " coat",
                        ".",
                        "\u010a",
                        "\u010a",
                        "Det",
                        "ective",
                        " Inspector",
                        " Steve",
                        " Christian",
                        ",",
                        " from",
                        " Liverpool",
                        " C",
                        "ID",
                        ",",
                        " said",
                        ":"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " drop out of the presidential race, according to NBC News ' Andrea Mitchell.\u010a\u010a",
                    "tokens": [
                        " drop",
                        " out",
                        " of",
                        " the",
                        " presidential",
                        " race",
                        ",",
                        " according",
                        " to",
                        " NBC",
                        " News",
                        " '",
                        " Andrea",
                        " Mitchell",
                        ".",
                        "\u010a",
                        "\u010a"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " documenting the protective effect of male circumcision on HIV infection in young adults pose significant challenges to",
                    "tokens": [
                        " documenting",
                        " the",
                        " protective",
                        " effect",
                        " of",
                        " male",
                        " circumcision",
                        " on",
                        " HIV",
                        " infection",
                        " in",
                        " young",
                        " adults",
                        " pose",
                        " significant",
                        " challenges",
                        " to"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " the virus is no laughing matter, a viral video features a Chicago pup named Herbert who",
                    "tokens": [
                        " the",
                        " virus",
                        " is",
                        " no",
                        " laughing",
                        " matter",
                        ",",
                        " a",
                        " viral",
                        " video",
                        " features",
                        " a",
                        " Chicago",
                        " pup",
                        " named",
                        " Herbert",
                        " who"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " faces with respect to women.\u010a\u010aA CNN poll released Monday showed Obama moving into",
                    "tokens": [
                        " faces",
                        " with",
                        " respect",
                        " to",
                        " women",
                        ".",
                        "\u010a",
                        "\u010a",
                        "A",
                        " CNN",
                        " poll",
                        " released",
                        " Monday",
                        " showed",
                        " Obama",
                        " moving",
                        " into"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 62.19127655029297
        },
        {
            "feature_index": 292,
            "gpt_predictions": [
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "words related to locations or places",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "tracks [ edit ]<|endoftext|>SINGAPORE - Love cheats on the Internet fooled",
                    "max_token": "ORE",
                    "tokens": [
                        "tracks",
                        " [",
                        " edit",
                        " ]",
                        "<|endoftext|>",
                        "S",
                        "ING",
                        "AP",
                        "ORE",
                        " -",
                        " Love",
                        " che",
                        "ats",
                        " on",
                        " the",
                        " Internet",
                        " fooled"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        23.29506301879883,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " women were at their home in Bijnore.Sangeeta is seen mercilessly",
                    "max_token": "ore",
                    "tokens": [
                        " women",
                        " were",
                        " at",
                        " their",
                        " home",
                        " in",
                        " B",
                        "ijn",
                        "ore",
                        ".",
                        "S",
                        "ange",
                        "eta",
                        " is",
                        " seen",
                        " merciless",
                        "ly"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        34.75870132446289,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "2 percentage points.\u010a\u010a2000, Gore vs. Bush, 48.3%",
                    "max_token": " Gore",
                    "tokens": [
                        "2",
                        " percentage",
                        " points",
                        ".",
                        "\u010a",
                        "\u010a",
                        "2000",
                        ",",
                        " Gore",
                        " vs",
                        ".",
                        " Bush",
                        ",",
                        " 48",
                        ".",
                        "3",
                        "%"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        10.31690406799316,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " Camille Claudel, and Tintoretto.[6] The most valuable work",
                    "max_token": "ore",
                    "tokens": [
                        " Cam",
                        "ille",
                        " Claud",
                        "el",
                        ",",
                        " and",
                        " T",
                        "int",
                        "ore",
                        "tto",
                        ".[",
                        "6",
                        "]",
                        " The",
                        " most",
                        " valuable",
                        " work"
                    ],
                    "values": [
                        0,
                        0.8354509472846985,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        39.34261322021484,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " effects be? \u00e2\u0122\u013eA fantastic anorectic if you want to lose weight,",
                    "max_token": "ore",
                    "tokens": [
                        " effects",
                        " be",
                        "?",
                        " \u00e2\u0122",
                        "\u013e",
                        "A",
                        " fantastic",
                        " an",
                        "ore",
                        "ctic",
                        " if",
                        " you",
                        " want",
                        " to",
                        " lose",
                        " weight",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        38.18931579589844,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " Trump and Hillary in the first debate, before either said a word, Trump made what",
                    "tokens": [
                        " Trump",
                        " and",
                        " Hillary",
                        " in",
                        " the",
                        " first",
                        " debate",
                        ",",
                        " before",
                        " either",
                        " said",
                        " a",
                        " word",
                        ",",
                        " Trump",
                        " made",
                        " what"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " that kid who felt unwanted, or the grown up who remembers how hard it was to",
                    "tokens": [
                        " that",
                        " kid",
                        " who",
                        " felt",
                        " unwanted",
                        ",",
                        " or",
                        " the",
                        " grown",
                        " up",
                        " who",
                        " remembers",
                        " how",
                        " hard",
                        " it",
                        " was",
                        " to"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "\u010aAldrin was a drug user before, Castillo admitted, but he had",
                    "tokens": [
                        "\u010a",
                        "A",
                        "ld",
                        "rin",
                        " was",
                        " a",
                        " drug",
                        " user",
                        " before",
                        ",",
                        " Cast",
                        "illo",
                        " admitted",
                        ",",
                        " but",
                        " he",
                        " had"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " who genuinely seem not to believe reality. Or Democrats, cowed into silence on issues",
                    "tokens": [
                        " who",
                        " genuinely",
                        " seem",
                        " not",
                        " to",
                        " believe",
                        " reality",
                        ".",
                        " Or",
                        " Democrats",
                        ",",
                        " c",
                        "owed",
                        " into",
                        " silence",
                        " on",
                        " issues"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " the incident, saying the man threatened him before leaving.\u010a\u010a\u00e2\u0122\u013eI think",
                    "tokens": [
                        " the",
                        " incident",
                        ",",
                        " saying",
                        " the",
                        " man",
                        " threatened",
                        " him",
                        " before",
                        " leaving",
                        ".",
                        "\u010a",
                        "\u010a",
                        "\u00e2\u0122",
                        "\u013e",
                        "I",
                        " think"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 45.79512405395508
        },
        {
            "feature_index": 289,
            "gpt_predictions": [
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    0,
                    1.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    1.0
                ]
            ],
            "description": "verbs related to influencing or persuading others",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": ". A<|endoftext|> arguments will be just as convincing when presented in person remains to be seen",
                    "max_token": " convincing",
                    "tokens": [
                        ".",
                        " A",
                        "<|endoftext|>",
                        " arguments",
                        " will",
                        " be",
                        " just",
                        " as",
                        " convincing",
                        " when",
                        " presented",
                        " in",
                        " person",
                        " remains",
                        " to",
                        " be",
                        " seen"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        25.43436622619629,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " Lanier said could undermine her efforts to persuade women to talk to officers in what is",
                    "max_token": " persuade",
                    "tokens": [
                        " Lan",
                        "ier",
                        " said",
                        " could",
                        " undermine",
                        " her",
                        " efforts",
                        " to",
                        " persuade",
                        " women",
                        " to",
                        " talk",
                        " to",
                        " officers",
                        " in",
                        " what",
                        " is"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        35.98454666137695,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " civilization in our past. Others are less convinced, however.\u010a\u010aBoston University ge",
                    "max_token": " convinced",
                    "tokens": [
                        " civilization",
                        " in",
                        " our",
                        " past",
                        ".",
                        " Others",
                        " are",
                        " less",
                        " convinced",
                        ",",
                        " however",
                        ".",
                        "\u010a",
                        "\u010a",
                        "Boston",
                        " University",
                        " ge"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        10.24174308776855,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " the medical establishment's brainwashing efforts to convince us that we need constant medical care -",
                    "max_token": " convince",
                    "tokens": [
                        " the",
                        " medical",
                        " establishment",
                        "'s",
                        " brain",
                        "washing",
                        " efforts",
                        " to",
                        " convince",
                        " us",
                        " that",
                        " we",
                        " need",
                        " constant",
                        " medical",
                        " care",
                        " -"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0.1629216074943542,
                        0,
                        0,
                        38.8958854675293,
                        8.426271438598633,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " some damning tidbit of information that will convince the doubters that beneath that sophisticated manner",
                    "max_token": " convince",
                    "tokens": [
                        " some",
                        " damning",
                        " tid",
                        "bit",
                        " of",
                        " information",
                        " that",
                        " will",
                        " convince",
                        " the",
                        " doub",
                        "ters",
                        " that",
                        " beneath",
                        " that",
                        " sophisticated",
                        " manner"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        38.17436981201172,
                        8.909947395324707,
                        0,
                        3.172218322753906,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " Trump and Hillary in the first debate, before either said a word, Trump made what",
                    "tokens": [
                        " Trump",
                        " and",
                        " Hillary",
                        " in",
                        " the",
                        " first",
                        " debate",
                        ",",
                        " before",
                        " either",
                        " said",
                        " a",
                        " word",
                        ",",
                        " Trump",
                        " made",
                        " what"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " that kid who felt unwanted, or the grown up who remembers how hard it was to",
                    "tokens": [
                        " that",
                        " kid",
                        " who",
                        " felt",
                        " unwanted",
                        ",",
                        " or",
                        " the",
                        " grown",
                        " up",
                        " who",
                        " remembers",
                        " how",
                        " hard",
                        " it",
                        " was",
                        " to"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "\u010aAldrin was a drug user before, Castillo admitted, but he had",
                    "tokens": [
                        "\u010a",
                        "A",
                        "ld",
                        "rin",
                        " was",
                        " a",
                        " drug",
                        " user",
                        " before",
                        ",",
                        " Cast",
                        "illo",
                        " admitted",
                        ",",
                        " but",
                        " he",
                        " had"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " who genuinely seem not to believe reality. Or Democrats, cowed into silence on issues",
                    "tokens": [
                        " who",
                        " genuinely",
                        " seem",
                        " not",
                        " to",
                        " believe",
                        " reality",
                        ".",
                        " Or",
                        " Democrats",
                        ",",
                        " c",
                        "owed",
                        " into",
                        " silence",
                        " on",
                        " issues"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " the incident, saying the man threatened him before leaving.\u010a\u010a\u00e2\u0122\u013eI think",
                    "tokens": [
                        " the",
                        " incident",
                        ",",
                        " saying",
                        " the",
                        " man",
                        " threatened",
                        " him",
                        " before",
                        " leaving",
                        ".",
                        "\u010a",
                        "\u010a",
                        "\u00e2\u0122",
                        "\u013e",
                        "I",
                        " think"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 41.89437866210938
        },
        {
            "feature_index": 652,
            "gpt_predictions": [
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "phrases related to teaching, training, or mentoring",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " the Episcopal Church of the United States marching not simply out of step with but completely out",
                    "max_token": " not",
                    "tokens": [
                        " the",
                        " Episcopal",
                        " Church",
                        " of",
                        " the",
                        " United",
                        " States",
                        " marching",
                        " not",
                        " simply",
                        " out",
                        " of",
                        " step",
                        " with",
                        " but",
                        " completely",
                        " out"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        2.557648658752441,
                        0.3025627732276917,
                        0,
                        0.5607777833938599,
                        3.931344270706177,
                        0.7631038427352905,
                        0,
                        0,
                        0,
                        0,
                        0.1651615649461746,
                        0.09867841005325317,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " Court that will carry forward the ideas of the Reagan Revolution\u00e2\u0122\u0135into the 21st century",
                    "max_token": " the",
                    "tokens": [
                        " Court",
                        " that",
                        " will",
                        " carry",
                        " forward",
                        " the",
                        " ideas",
                        " of",
                        " the",
                        " Reagan",
                        " Revolution",
                        "\u00e2\u0122\u0135",
                        "into",
                        " the",
                        " 21",
                        "st",
                        " century"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0.9232298135757446,
                        0,
                        0.7532885074615479,
                        0,
                        3.246544122695923,
                        4.327158451080322,
                        0,
                        0,
                        0,
                        1.012260913848877,
                        1.875493407249451,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " beautiful way to connect Texas' past and its present, something that Brown obviously values.",
                    "max_token": " its",
                    "tokens": [
                        " beautiful",
                        " way",
                        " to",
                        " connect",
                        " Texas",
                        "'",
                        " past",
                        " and",
                        " its",
                        " present",
                        ",",
                        " something",
                        " that",
                        " Brown",
                        " obviously",
                        " values",
                        "."
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        1.465764760971069,
                        4.89705753326416,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " release. \"I expect our officers to serve this community in a professional manner at all",
                    "max_token": " serve",
                    "tokens": [
                        " release",
                        ".",
                        " \"",
                        "I",
                        " expect",
                        " our",
                        " officers",
                        " to",
                        " serve",
                        " this",
                        " community",
                        " in",
                        " a",
                        " professional",
                        " manner",
                        " at",
                        " all"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        1.091094017028809,
                        2.125871419906616,
                        2.934513330459595,
                        4.538989067077637,
                        5.092665195465088,
                        2.559452295303345,
                        0.1888419687747955,
                        2.866093635559082,
                        2.789141178131104,
                        0,
                        5.029372215270996,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": ". The teams behind revolutionary products succeed because they make strategic bets about which things to reinvent",
                    "max_token": " they",
                    "tokens": [
                        ".",
                        " The",
                        " teams",
                        " behind",
                        " revolutionary",
                        " products",
                        " succeed",
                        " because",
                        " they",
                        " make",
                        " strategic",
                        " bets",
                        " about",
                        " which",
                        " things",
                        " to",
                        " reinvent"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0.6191402077674866,
                        5.608463764190674,
                        5.785977363586426,
                        3.039328575134277,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " quick lead only to have to bow out before the end of the game because they went",
                    "tokens": [
                        " quick",
                        " lead",
                        " only",
                        " to",
                        " have",
                        " to",
                        " bow",
                        " out",
                        " before",
                        " the",
                        " end",
                        " of",
                        " the",
                        " game",
                        " because",
                        " they",
                        " went"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "\u010aAldrin was a drug user before, Castillo admitted, but he had",
                    "tokens": [
                        "\u010a",
                        "A",
                        "ld",
                        "rin",
                        " was",
                        " a",
                        " drug",
                        " user",
                        " before",
                        ",",
                        " Cast",
                        "illo",
                        " admitted",
                        ",",
                        " but",
                        " he",
                        " had"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " height at some international summit.\u010a\u010aOr is it because of these deeply controversial statements",
                    "tokens": [
                        " height",
                        " at",
                        " some",
                        " international",
                        " summit",
                        ".",
                        "\u010a",
                        "\u010a",
                        "Or",
                        " is",
                        " it",
                        " because",
                        " of",
                        " these",
                        " deeply",
                        " controversial",
                        " statements"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " years after he was jailed for espionage, before his release in 1969.\u010a\u010aHe",
                    "tokens": [
                        " years",
                        " after",
                        " he",
                        " was",
                        " jailed",
                        " for",
                        " espionage",
                        ",",
                        " before",
                        " his",
                        " release",
                        " in",
                        " 1969",
                        ".",
                        "\u010a",
                        "\u010a",
                        "He"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " faces with respect to women.\u010a\u010aA CNN poll released Monday showed Obama moving into",
                    "tokens": [
                        " faces",
                        " with",
                        " respect",
                        " to",
                        " women",
                        ".",
                        "\u010a",
                        "\u010a",
                        "A",
                        " CNN",
                        " poll",
                        " released",
                        " Monday",
                        " showed",
                        " Obama",
                        " moving",
                        " into"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 6.915557861328125
        },
        {
            "feature_index": 39,
            "gpt_predictions": [
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "phrases or descriptions related to someone's character or impact",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " Windows 7.\u010a\u010aWindows 8 is a dramatic shift from earlier versions of Windows.",
                    "max_token": " a",
                    "tokens": [
                        " Windows",
                        " 7",
                        ".",
                        "\u010a",
                        "\u010a",
                        "Windows",
                        " 8",
                        " is",
                        " a",
                        " dramatic",
                        " shift",
                        " from",
                        " earlier",
                        " versions",
                        " of",
                        " Windows",
                        "."
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        12.52579689025879,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "down told 5 Live: \"Tim's a very able guy but at the moment judgement",
                    "max_token": " a",
                    "tokens": [
                        "down",
                        " told",
                        " 5",
                        " Live",
                        ":",
                        " \"",
                        "Tim",
                        "'s",
                        " a",
                        " very",
                        " able",
                        " guy",
                        " but",
                        " at",
                        " the",
                        " moment",
                        " judgement"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        25.95602607727051,
                        4.073008060455322,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " on the character.\"<|endoftext|>James Bass is a Texas transportation veteran, but in his new",
                    "max_token": " a",
                    "tokens": [
                        " on",
                        " the",
                        " character",
                        ".\"",
                        "<|endoftext|>",
                        "James",
                        " Bass",
                        " is",
                        " a",
                        " Texas",
                        " transportation",
                        " veteran",
                        ",",
                        " but",
                        " in",
                        " his",
                        " new"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        7.121735572814941,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": ".\u010a\u010a\"He\u00e2\u0122\u013bs a solid two-way player and throughout his",
                    "max_token": " a",
                    "tokens": [
                        ".",
                        "\u010a",
                        "\u010a",
                        "\"",
                        "He",
                        "\u00e2\u0122",
                        "\u013b",
                        "s",
                        " a",
                        " solid",
                        " two",
                        "-",
                        "way",
                        " player",
                        " and",
                        " throughout",
                        " his"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        28.03397178649902,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " the best pitchers in baseball. He's a difference-maker for somebody.\"<|endoftext|>Scientists",
                    "max_token": " a",
                    "tokens": [
                        " the",
                        " best",
                        " pitchers",
                        " in",
                        " baseball",
                        ".",
                        " He",
                        "'s",
                        " a",
                        " difference",
                        "-",
                        "maker",
                        " for",
                        " somebody",
                        ".\"",
                        "<|endoftext|>",
                        "Scientists"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        27.36321449279785,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "ESPN, UniMas, UDN) before facing the 2014 FIFA World Cup<|endoftext|> can",
                    "tokens": [
                        "ESPN",
                        ",",
                        " Uni",
                        "Mas",
                        ",",
                        " U",
                        "DN",
                        ")",
                        " before",
                        " facing",
                        " the",
                        " 2014",
                        " FIFA",
                        " World",
                        " Cup",
                        "<|endoftext|>",
                        " can"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " vaccine industry, with the assistance of a sold-out government, is forcing vaccinations on",
                    "tokens": [
                        " vaccine",
                        " industry",
                        ",",
                        " with",
                        " the",
                        " assistance",
                        " of",
                        " a",
                        " sold",
                        "-",
                        "out",
                        " government",
                        ",",
                        " is",
                        " forcing",
                        " vaccinations",
                        " on"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": ", build and maintain effective student groups, advertise and rebrand conservative values, engage in",
                    "tokens": [
                        ",",
                        " build",
                        " and",
                        " maintain",
                        " effective",
                        " student",
                        " groups",
                        ",",
                        " advertise",
                        " and",
                        " re",
                        "brand",
                        " conservative",
                        " values",
                        ",",
                        " engage",
                        " in"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " political protest\u010a\u010aUpdated\u010a\u010aIt was one of the great art heists of",
                    "tokens": [
                        " political",
                        " protest",
                        "\u010a",
                        "\u010a",
                        "Updated",
                        "\u010a",
                        "\u010a",
                        "It",
                        " was",
                        " one",
                        " of",
                        " the",
                        " great",
                        " art",
                        " he",
                        "ists",
                        " of"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " that kid who felt unwanted, or the grown up who remembers how hard it was to",
                    "tokens": [
                        " that",
                        " kid",
                        " who",
                        " felt",
                        " unwanted",
                        ",",
                        " or",
                        " the",
                        " grown",
                        " up",
                        " who",
                        " remembers",
                        " how",
                        " hard",
                        " it",
                        " was",
                        " to"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 32.44255065917969
        },
        {
            "feature_index": 589,
            "gpt_predictions": [
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "references to the term \"Ho\"",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " (<\u00e2\u0122\u0136 updated as well)\u010a\u010aHOLE FAQ\u010a\u010aI developed an<|endoftext|>",
                    "max_token": "HO",
                    "tokens": [
                        " (<",
                        "\u00e2\u0122\u0136",
                        " updated",
                        " as",
                        " well",
                        ")",
                        "\u010a",
                        "\u010a",
                        "HO",
                        "LE",
                        " FAQ",
                        "\u010a",
                        "\u010a",
                        "I",
                        " developed",
                        " an",
                        "<|endoftext|>"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        31.94223022460938,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " singer Matt Thiessen and guitarist Matt Hoopes chat on a wide array of topics",
                    "max_token": " Ho",
                    "tokens": [
                        " singer",
                        " Matt",
                        " Th",
                        "i",
                        "essen",
                        " and",
                        " guitarist",
                        " Matt",
                        " Ho",
                        "opes",
                        " chat",
                        " on",
                        " a",
                        " wide",
                        " array",
                        " of",
                        " topics"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        65.43685150146484,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": ".\u010a\u010aCNN's Greg Botelho contributed to this report.<|endoftext|>people with",
                    "max_token": "ho",
                    "tokens": [
                        ".",
                        "\u010a",
                        "\u010a",
                        "CNN",
                        "'s",
                        " Greg",
                        " Bot",
                        "el",
                        "ho",
                        " contributed",
                        " to",
                        " this",
                        " report",
                        ".",
                        "<|endoftext|>",
                        "people",
                        " with"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        18.28084564208984,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " not played SC2 since July, and HoN since August of 2010. I played",
                    "max_token": " Ho",
                    "tokens": [
                        " not",
                        " played",
                        " SC",
                        "2",
                        " since",
                        " July",
                        ",",
                        " and",
                        " Ho",
                        "N",
                        " since",
                        " August",
                        " of",
                        " 2010",
                        ".",
                        " I",
                        " played"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        72.99404907226562,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " neighbour, not to be confused with the Hohenstaufen Emperor) robbed and",
                    "max_token": " Ho",
                    "tokens": [
                        " neighbour",
                        ",",
                        " not",
                        " to",
                        " be",
                        " confused",
                        " with",
                        " the",
                        " Ho",
                        "hen",
                        "st",
                        "au",
                        "fen",
                        " Emperor",
                        ")",
                        " robbed",
                        " and"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        71.20158386230469,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " quick lead only to have to bow out before the end of the game because they went",
                    "tokens": [
                        " quick",
                        " lead",
                        " only",
                        " to",
                        " have",
                        " to",
                        " bow",
                        " out",
                        " before",
                        " the",
                        " end",
                        " of",
                        " the",
                        " game",
                        " because",
                        " they",
                        " went"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "\u010aAldrin was a drug user before, Castillo admitted, but he had",
                    "tokens": [
                        "\u010a",
                        "A",
                        "ld",
                        "rin",
                        " was",
                        " a",
                        " drug",
                        " user",
                        " before",
                        ",",
                        " Cast",
                        "illo",
                        " admitted",
                        ",",
                        " but",
                        " he",
                        " had"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " height at some international summit.\u010a\u010aOr is it because of these deeply controversial statements",
                    "tokens": [
                        " height",
                        " at",
                        " some",
                        " international",
                        " summit",
                        ".",
                        "\u010a",
                        "\u010a",
                        "Or",
                        " is",
                        " it",
                        " because",
                        " of",
                        " these",
                        " deeply",
                        " controversial",
                        " statements"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " years after he was jailed for espionage, before his release in 1969.\u010a\u010aHe",
                    "tokens": [
                        " years",
                        " after",
                        " he",
                        " was",
                        " jailed",
                        " for",
                        " espionage",
                        ",",
                        " before",
                        " his",
                        " release",
                        " in",
                        " 1969",
                        ".",
                        "\u010a",
                        "\u010a",
                        "He"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " faces with respect to women.\u010a\u010aA CNN poll released Monday showed Obama moving into",
                    "tokens": [
                        " faces",
                        " with",
                        " respect",
                        " to",
                        " women",
                        ".",
                        "\u010a",
                        "\u010a",
                        "A",
                        " CNN",
                        " poll",
                        " released",
                        " Monday",
                        " showed",
                        " Obama",
                        " moving",
                        " into"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 79.0394515991211
        },
        {
            "feature_index": 307,
            "gpt_predictions": [
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    1.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    1.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "words related to problems or challenges",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "<|endoftext|>Code:\u010a\u010a- Fixed a bug preventing some people from building new trade posts",
                    "max_token": " bug",
                    "tokens": [
                        "<|endoftext|>",
                        "Code",
                        ":",
                        "\u010a",
                        "\u010a",
                        "-",
                        " Fixed",
                        " a",
                        " bug",
                        " preventing",
                        " some",
                        " people",
                        " from",
                        " building",
                        " new",
                        " trade",
                        " posts"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        18.27318954467773,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " moved off of HC due to the other issues and now this? Yeah. Safe to",
                    "max_token": " issues",
                    "tokens": [
                        " moved",
                        " off",
                        " of",
                        " HC",
                        " due",
                        " to",
                        " the",
                        " other",
                        " issues",
                        " and",
                        " now",
                        " this",
                        "?",
                        " Yeah",
                        ".",
                        " Safe",
                        " to"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        22.76357650756836,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "Advertisement\u010a\u010aWhen asked about the design challenges for the first Tesla car, von Hol",
                    "max_token": " challenges",
                    "tokens": [
                        "Advertisement",
                        "\u010a",
                        "\u010a",
                        "When",
                        " asked",
                        " about",
                        " the",
                        " design",
                        " challenges",
                        " for",
                        " the",
                        " first",
                        " Tesla",
                        " car",
                        ",",
                        " von",
                        " Hol"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        6.630831718444824,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": ". First the horrible payouts and connection issues, and now they allow everyone to be",
                    "max_token": " issues",
                    "tokens": [
                        ".",
                        " First",
                        " the",
                        " horrible",
                        " pay",
                        "outs",
                        " and",
                        " connection",
                        " issues",
                        ",",
                        " and",
                        " now",
                        " they",
                        " allow",
                        " everyone",
                        " to",
                        " be"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0.9576850533485413,
                        27.17841911315918,
                        1.403214812278748,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " Susan Rider said wait times were down but glitches remained Monday, including challenges signing up consumers",
                    "max_token": " glitches",
                    "tokens": [
                        " Susan",
                        " Rider",
                        " said",
                        " wait",
                        " times",
                        " were",
                        " down",
                        " but",
                        " glitches",
                        " remained",
                        " Monday",
                        ",",
                        " including",
                        " challenges",
                        " signing",
                        " up",
                        " consumers"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        26.34375190734863,
                        7.171938419342041,
                        0,
                        0,
                        0,
                        13.82070159912109,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " Trump and Hillary in the first debate, before either said a word, Trump made what",
                    "tokens": [
                        " Trump",
                        " and",
                        " Hillary",
                        " in",
                        " the",
                        " first",
                        " debate",
                        ",",
                        " before",
                        " either",
                        " said",
                        " a",
                        " word",
                        ",",
                        " Trump",
                        " made",
                        " what"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " that kid who felt unwanted, or the grown up who remembers how hard it was to",
                    "tokens": [
                        " that",
                        " kid",
                        " who",
                        " felt",
                        " unwanted",
                        ",",
                        " or",
                        " the",
                        " grown",
                        " up",
                        " who",
                        " remembers",
                        " how",
                        " hard",
                        " it",
                        " was",
                        " to"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "\u010aAldrin was a drug user before, Castillo admitted, but he had",
                    "tokens": [
                        "\u010a",
                        "A",
                        "ld",
                        "rin",
                        " was",
                        " a",
                        " drug",
                        " user",
                        " before",
                        ",",
                        " Cast",
                        "illo",
                        " admitted",
                        ",",
                        " but",
                        " he",
                        " had"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " who genuinely seem not to believe reality. Or Democrats, cowed into silence on issues",
                    "tokens": [
                        " who",
                        " genuinely",
                        " seem",
                        " not",
                        " to",
                        " believe",
                        " reality",
                        ".",
                        " Or",
                        " Democrats",
                        ",",
                        " c",
                        "owed",
                        " into",
                        " silence",
                        " on",
                        " issues"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " the incident, saying the man threatened him before leaving.\u010a\u010a\u00e2\u0122\u013eI think",
                    "tokens": [
                        " the",
                        " incident",
                        ",",
                        " saying",
                        " the",
                        " man",
                        " threatened",
                        " him",
                        " before",
                        " leaving",
                        ".",
                        "\u010a",
                        "\u010a",
                        "\u00e2\u0122",
                        "\u013e",
                        "I",
                        " think"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 32.01578903198242
        },
        {
            "feature_index": 679,
            "gpt_predictions": [
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "terms related to various topics such as mutations, users, medals, school, submarines, players, trees, sellers, referees, test flights, exceptions, songs, wins, fossils, arguments, hostages, students, distributions, goals, monsters, registers, candidates",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " talk about their bullshit errors. Ask the refs about that. Ask Daniel Anderson about",
                    "max_token": " ref",
                    "tokens": [
                        " talk",
                        " about",
                        " their",
                        " bullshit",
                        " errors",
                        ".",
                        " Ask",
                        " the",
                        " ref",
                        "s",
                        " about",
                        " that",
                        ".",
                        " Ask",
                        " Daniel",
                        " Anderson",
                        " about"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        13.59624004364014,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " a very nice job of that. The shot of him perched like an actual vulture",
                    "max_token": " shot",
                    "tokens": [
                        " a",
                        " very",
                        " nice",
                        " job",
                        " of",
                        " that",
                        ".",
                        " The",
                        " shot",
                        " of",
                        " him",
                        " perched",
                        " like",
                        " an",
                        " actual",
                        " v",
                        "ulture"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        11.41331195831299,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " funding over five years to develop its SMR design, with industrial partners expected at least",
                    "max_token": "R",
                    "tokens": [
                        " funding",
                        " over",
                        " five",
                        " years",
                        " to",
                        " develop",
                        " its",
                        " SM",
                        "R",
                        " design",
                        ",",
                        " with",
                        " industrial",
                        " partners",
                        " expected",
                        " at",
                        " least"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        11.27774906158447,
                        1.085780382156372,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " told the Sunday Times. \u00e2\u0122\u013eThe crater area is likely to represent one of the",
                    "max_token": " crater",
                    "tokens": [
                        " told",
                        " the",
                        " Sunday",
                        " Times",
                        ".",
                        " \u00e2\u0122",
                        "\u013e",
                        "The",
                        " crater",
                        " area",
                        " is",
                        " likely",
                        " to",
                        " represent",
                        " one",
                        " of",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        17.04731750488281,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " and the Season deck. The three different card types have different card backs making this process",
                    "max_token": " card",
                    "tokens": [
                        " and",
                        " the",
                        " Season",
                        " deck",
                        ".",
                        " The",
                        " three",
                        " different",
                        " card",
                        " types",
                        " have",
                        " different",
                        " card",
                        " backs",
                        " making",
                        " this",
                        " process"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        12.75621032714844,
                        0,
                        0,
                        0,
                        6.262800216674805,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " quick lead only to have to bow out before the end of the game because they went",
                    "tokens": [
                        " quick",
                        " lead",
                        " only",
                        " to",
                        " have",
                        " to",
                        " bow",
                        " out",
                        " before",
                        " the",
                        " end",
                        " of",
                        " the",
                        " game",
                        " because",
                        " they",
                        " went"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "\u010aAldrin was a drug user before, Castillo admitted, but he had",
                    "tokens": [
                        "\u010a",
                        "A",
                        "ld",
                        "rin",
                        " was",
                        " a",
                        " drug",
                        " user",
                        " before",
                        ",",
                        " Cast",
                        "illo",
                        " admitted",
                        ",",
                        " but",
                        " he",
                        " had"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " height at some international summit.\u010a\u010aOr is it because of these deeply controversial statements",
                    "tokens": [
                        " height",
                        " at",
                        " some",
                        " international",
                        " summit",
                        ".",
                        "\u010a",
                        "\u010a",
                        "Or",
                        " is",
                        " it",
                        " because",
                        " of",
                        " these",
                        " deeply",
                        " controversial",
                        " statements"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " years after he was jailed for espionage, before his release in 1969.\u010a\u010aHe",
                    "tokens": [
                        " years",
                        " after",
                        " he",
                        " was",
                        " jailed",
                        " for",
                        " espionage",
                        ",",
                        " before",
                        " his",
                        " release",
                        " in",
                        " 1969",
                        ".",
                        "\u010a",
                        "\u010a",
                        "He"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " faces with respect to women.\u010a\u010aA CNN poll released Monday showed Obama moving into",
                    "tokens": [
                        " faces",
                        " with",
                        " respect",
                        " to",
                        " women",
                        ".",
                        "\u010a",
                        "\u010a",
                        "A",
                        " CNN",
                        " poll",
                        " released",
                        " Monday",
                        " showed",
                        " Obama",
                        " moving",
                        " into"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 18.05411911010742
        },
        {
            "feature_index": 66,
            "gpt_predictions": [
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    1.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "phrases related to legal issues and law enforcement",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " stand in the way of our great agenda. Didn't have the guts<|endoftext|> itself.",
                    "max_token": ".",
                    "tokens": [
                        " stand",
                        " in",
                        " the",
                        " way",
                        " of",
                        " our",
                        " great",
                        " agenda",
                        ".",
                        " Didn",
                        "'t",
                        " have",
                        " the",
                        " guts",
                        "<|endoftext|>",
                        " itself",
                        "."
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        3.551554679870605,
                        23.0441837310791,
                        6.152920722961426,
                        0.02410018444061279,
                        0,
                        0,
                        1.000539302825928,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": ". Early voting will continue through October 31. Photo taken Friday, October 24, 2014",
                    "max_token": ".",
                    "tokens": [
                        ".",
                        " Early",
                        " voting",
                        " will",
                        " continue",
                        " through",
                        " October",
                        " 31",
                        ".",
                        " Photo",
                        " taken",
                        " Friday",
                        ",",
                        " October",
                        " 24",
                        ",",
                        " 2014"
                    ],
                    "values": [
                        14.68631172180176,
                        0,
                        0,
                        0,
                        0,
                        4.842650413513184,
                        0,
                        0,
                        23.72054100036621,
                        10.20935535430908,
                        1.188944578170776,
                        0,
                        0,
                        0,
                        0,
                        0.03497521579265594,
                        5.414639949798584
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " REP was instrumental in a suicide intervention. Knowing that we were involved in preventing another",
                    "max_token": ".",
                    "tokens": [
                        " RE",
                        "P",
                        " was",
                        " instrumental",
                        " in",
                        " a",
                        " suicide",
                        " intervention",
                        ".",
                        " Knowing",
                        " that",
                        " we",
                        " were",
                        " involved",
                        " in",
                        " preventing",
                        " another"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        1.942146897315979,
                        26.5811595916748,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " colored paper and frilly fabrics to it. My advice is to wear a<|endoftext|> physician",
                    "max_token": ".",
                    "tokens": [
                        " colored",
                        " paper",
                        " and",
                        " fr",
                        "illy",
                        " fabrics",
                        " to",
                        " it",
                        ".",
                        " My",
                        " advice",
                        " is",
                        " to",
                        " wear",
                        " a",
                        "<|endoftext|>",
                        " physician"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        1.976321816444397,
                        0,
                        0,
                        0,
                        29.14490509033203,
                        0,
                        0,
                        0.1810193806886673,
                        1.004808783531189,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 6,
                    "sentence_string": " when they have been disarmed.\u00e2\u0122\u013f\u010a\u010aThe Reform movement\u00e2\u0122\u013bs",
                    "max_token": ".",
                    "tokens": [
                        " when",
                        " they",
                        " have",
                        " been",
                        " dis",
                        "armed",
                        ".",
                        "\u00e2\u0122",
                        "\u013f",
                        "\u010a",
                        "\u010a",
                        "The",
                        " Reform",
                        " movement",
                        "\u00e2\u0122",
                        "\u013b",
                        "s"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        5.606927394866943,
                        30.75358200073242,
                        18.49423408508301,
                        26.87698745727539,
                        4.489589691162109,
                        0.7723684310913086,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "ESPN, UniMas, UDN) before facing the 2014 FIFA World Cup<|endoftext|> can",
                    "tokens": [
                        "ESPN",
                        ",",
                        " Uni",
                        "Mas",
                        ",",
                        " U",
                        "DN",
                        ")",
                        " before",
                        " facing",
                        " the",
                        " 2014",
                        " FIFA",
                        " World",
                        " Cup",
                        "<|endoftext|>",
                        " can"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " vaccine industry, with the assistance of a sold-out government, is forcing vaccinations on",
                    "tokens": [
                        " vaccine",
                        " industry",
                        ",",
                        " with",
                        " the",
                        " assistance",
                        " of",
                        " a",
                        " sold",
                        "-",
                        "out",
                        " government",
                        ",",
                        " is",
                        " forcing",
                        " vaccinations",
                        " on"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": ", build and maintain effective student groups, advertise and rebrand conservative values, engage in",
                    "tokens": [
                        ",",
                        " build",
                        " and",
                        " maintain",
                        " effective",
                        " student",
                        " groups",
                        ",",
                        " advertise",
                        " and",
                        " re",
                        "brand",
                        " conservative",
                        " values",
                        ",",
                        " engage",
                        " in"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " political protest\u010a\u010aUpdated\u010a\u010aIt was one of the great art heists of",
                    "tokens": [
                        " political",
                        " protest",
                        "\u010a",
                        "\u010a",
                        "Updated",
                        "\u010a",
                        "\u010a",
                        "It",
                        " was",
                        " one",
                        " of",
                        " the",
                        " great",
                        " art",
                        " he",
                        "ists",
                        " of"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " that kid who felt unwanted, or the grown up who remembers how hard it was to",
                    "tokens": [
                        " that",
                        " kid",
                        " who",
                        " felt",
                        " unwanted",
                        ",",
                        " or",
                        " the",
                        " grown",
                        " up",
                        " who",
                        " remembers",
                        " how",
                        " hard",
                        " it",
                        " was",
                        " to"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 37.43001937866211
        },
        {
            "feature_index": 275,
            "gpt_predictions": [
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "contact information in documents",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " The Road To Delhi: Elections 2015 The Transition: 2015-2016 Uncategorized Viz",
                    "max_token": " Transition",
                    "tokens": [
                        " The",
                        " Road",
                        " To",
                        " Delhi",
                        ":",
                        " Elections",
                        " 2015",
                        " The",
                        " Transition",
                        ":",
                        " 2015",
                        "-",
                        "2016",
                        " Unc",
                        "ategor",
                        "ized",
                        " Viz"
                    ],
                    "values": [
                        0,
                        12.30779647827148,
                        0,
                        9.62592887878418,
                        0,
                        12.28922653198242,
                        11.99136066436768,
                        0,
                        15.4575138092041,
                        0,
                        8.064179420471191,
                        0,
                        11.13016986846924,
                        6.742582321166992,
                        4.502679347991943,
                        10.5805082321167,
                        6.418104648590088
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 15,
                    "sentence_string": " Cover Story Currency Chaos Development Education Elections 2014 Employment Fact Check Governance Newsletter Health homepage video",
                    "max_token": " homepage",
                    "tokens": [
                        " Cover",
                        " Story",
                        " Currency",
                        " Chaos",
                        " Development",
                        " Education",
                        " Elections",
                        " 2014",
                        " Employment",
                        " Fact",
                        " Check",
                        " Govern",
                        "ance",
                        " Newsletter",
                        " Health",
                        " homepage",
                        " video"
                    ],
                    "values": [
                        8.464239120483398,
                        5.889698028564453,
                        7.771647453308105,
                        12.47336196899414,
                        11.77865600585938,
                        12.93316650390625,
                        10.59136390686035,
                        4.459943771362305,
                        13.12704944610596,
                        5.859076976776123,
                        12.44729518890381,
                        2.833735942840576,
                        6.338600635528564,
                        6.642756462097168,
                        10.11195182800293,
                        13.79227447509766,
                        10.49182415008545
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": ".1.119 Choose Format: BibTeX EndNote Refer/BibIX RIS",
                    "max_token": "TeX",
                    "tokens": [
                        ".",
                        "1",
                        ".",
                        "119",
                        " Choose",
                        " Format",
                        ":",
                        " Bib",
                        "TeX",
                        " End",
                        "Note",
                        " Refer",
                        "/",
                        "B",
                        "ib",
                        "IX",
                        " RIS"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        5.475963115692139,
                        0,
                        5.321590900421143,
                        13.29014778137207,
                        0,
                        1.834476113319397,
                        2.568617582321167,
                        3.72820258140564,
                        0,
                        0,
                        4.508585929870605,
                        2.712122917175293
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " 1/8 European Union's chief Brexit negotiator, Michel Barnier Getty 2/8",
                    "max_token": " negotiator",
                    "tokens": [
                        " 1",
                        "/",
                        "8",
                        " European",
                        " Union",
                        "'s",
                        " chief",
                        " Brexit",
                        " negotiator",
                        ",",
                        " Michel",
                        " Barn",
                        "ier",
                        " Getty",
                        " 2",
                        "/",
                        "8"
                    ],
                    "values": [
                        0,
                        0,
                        1.134572625160217,
                        9.662359237670898,
                        13.32206344604492,
                        0,
                        3.82986044883728,
                        13.10018444061279,
                        17.00823593139648,
                        0,
                        0.5632930397987366,
                        4.830995559692383,
                        6.84812593460083,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "-Smoking: Yes! Wheelchair Accessible: Yes! Contact Owner: Silicon",
                    "max_token": " Access",
                    "tokens": [
                        "-",
                        "Sm",
                        "oking",
                        ":",
                        " Yes",
                        "!",
                        " Wheel",
                        "chair",
                        " Access",
                        "ible",
                        ":",
                        " Yes",
                        "!",
                        " Contact",
                        " Owner",
                        ":",
                        " Silicon"
                    ],
                    "values": [
                        0,
                        0,
                        7.721466541290283,
                        0,
                        0,
                        0,
                        5.527773380279541,
                        8.116951942443848,
                        14.50457859039307,
                        7.552039623260498,
                        0,
                        0,
                        0,
                        3.817064762115479,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " Trump and Hillary in the first debate, before either said a word, Trump made what",
                    "tokens": [
                        " Trump",
                        " and",
                        " Hillary",
                        " in",
                        " the",
                        " first",
                        " debate",
                        ",",
                        " before",
                        " either",
                        " said",
                        " a",
                        " word",
                        ",",
                        " Trump",
                        " made",
                        " what"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " that kid who felt unwanted, or the grown up who remembers how hard it was to",
                    "tokens": [
                        " that",
                        " kid",
                        " who",
                        " felt",
                        " unwanted",
                        ",",
                        " or",
                        " the",
                        " grown",
                        " up",
                        " who",
                        " remembers",
                        " how",
                        " hard",
                        " it",
                        " was",
                        " to"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "\u010aAldrin was a drug user before, Castillo admitted, but he had",
                    "tokens": [
                        "\u010a",
                        "A",
                        "ld",
                        "rin",
                        " was",
                        " a",
                        " drug",
                        " user",
                        " before",
                        ",",
                        " Cast",
                        "illo",
                        " admitted",
                        ",",
                        " but",
                        " he",
                        " had"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " who genuinely seem not to believe reality. Or Democrats, cowed into silence on issues",
                    "tokens": [
                        " who",
                        " genuinely",
                        " seem",
                        " not",
                        " to",
                        " believe",
                        " reality",
                        ".",
                        " Or",
                        " Democrats",
                        ",",
                        " c",
                        "owed",
                        " into",
                        " silence",
                        " on",
                        " issues"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " the incident, saying the man threatened him before leaving.\u010a\u010a\u00e2\u0122\u013eI think",
                    "tokens": [
                        " the",
                        " incident",
                        ",",
                        " saying",
                        " the",
                        " man",
                        " threatened",
                        " him",
                        " before",
                        " leaving",
                        ".",
                        "\u010a",
                        "\u010a",
                        "\u00e2\u0122",
                        "\u013e",
                        "I",
                        " think"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 18.21903610229492
        },
        {
            "feature_index": 67,
            "gpt_predictions": [
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "words related to hosting or being a host",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " during a radio interview with conservative talk show host Hugh Hewitt.\u010a\u010aAll of",
                    "max_token": " host",
                    "tokens": [
                        " during",
                        " a",
                        " radio",
                        " interview",
                        " with",
                        " conservative",
                        " talk",
                        " show",
                        " host",
                        " Hugh",
                        " Hew",
                        "itt",
                        ".",
                        "\u010a",
                        "\u010a",
                        "All",
                        " of"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        25.04773330688477,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " In Java Project maintained by Flipkart Hosted on GitHub Pages \u00e2\u0122\u0136 Theme by matt",
                    "max_token": " Host",
                    "tokens": [
                        " In",
                        " Java",
                        " Project",
                        " maintained",
                        " by",
                        " Flip",
                        "k",
                        "art",
                        " Host",
                        "ed",
                        " on",
                        " GitHub",
                        " Pages",
                        " \u00e2\u0122\u0136",
                        " Theme",
                        " by",
                        " matt"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        50.62713623046875,
                        2.228137016296387,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 3,
                    "sentence_string": " out which web hosting company a site is hosted with and the organisation that filed the FO",
                    "max_token": " hosting",
                    "tokens": [
                        " out",
                        " which",
                        " web",
                        " hosting",
                        " company",
                        " a",
                        " site",
                        " is",
                        " hosted",
                        " with",
                        " and",
                        " the",
                        " organisation",
                        " that",
                        " filed",
                        " the",
                        " FO"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        13.86390686035156,
                        0,
                        0,
                        0,
                        0,
                        9.784906387329102,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " the standoff, was killed by the FBI Hostage Rescue Team\u010a\u010aThe five-",
                    "max_token": " Host",
                    "tokens": [
                        " the",
                        " standoff",
                        ",",
                        " was",
                        " killed",
                        " by",
                        " the",
                        " FBI",
                        " Host",
                        "age",
                        " Rescue",
                        " Team",
                        "\u010a",
                        "\u010a",
                        "The",
                        " five",
                        "-"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        56.62308502197266,
                        5.765644073486328,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "oon's Military Says It Has Freed 900 Hostages From Boko Haram\u010a\u010aEnlarge this",
                    "max_token": " Host",
                    "tokens": [
                        "oon",
                        "'s",
                        " Military",
                        " Says",
                        " It",
                        " Has",
                        " Freed",
                        " 900",
                        " Host",
                        "ages",
                        " From",
                        " Boko",
                        " Haram",
                        "\u010a",
                        "\u010a",
                        "Enlarge",
                        " this"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        55.12849426269531,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "ESPN, UniMas, UDN) before facing the 2014 FIFA World Cup<|endoftext|> can",
                    "tokens": [
                        "ESPN",
                        ",",
                        " Uni",
                        "Mas",
                        ",",
                        " U",
                        "DN",
                        ")",
                        " before",
                        " facing",
                        " the",
                        " 2014",
                        " FIFA",
                        " World",
                        " Cup",
                        "<|endoftext|>",
                        " can"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " vaccine industry, with the assistance of a sold-out government, is forcing vaccinations on",
                    "tokens": [
                        " vaccine",
                        " industry",
                        ",",
                        " with",
                        " the",
                        " assistance",
                        " of",
                        " a",
                        " sold",
                        "-",
                        "out",
                        " government",
                        ",",
                        " is",
                        " forcing",
                        " vaccinations",
                        " on"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": ", build and maintain effective student groups, advertise and rebrand conservative values, engage in",
                    "tokens": [
                        ",",
                        " build",
                        " and",
                        " maintain",
                        " effective",
                        " student",
                        " groups",
                        ",",
                        " advertise",
                        " and",
                        " re",
                        "brand",
                        " conservative",
                        " values",
                        ",",
                        " engage",
                        " in"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " political protest\u010a\u010aUpdated\u010a\u010aIt was one of the great art heists of",
                    "tokens": [
                        " political",
                        " protest",
                        "\u010a",
                        "\u010a",
                        "Updated",
                        "\u010a",
                        "\u010a",
                        "It",
                        " was",
                        " one",
                        " of",
                        " the",
                        " great",
                        " art",
                        " he",
                        "ists",
                        " of"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " that kid who felt unwanted, or the grown up who remembers how hard it was to",
                    "tokens": [
                        " that",
                        " kid",
                        " who",
                        " felt",
                        " unwanted",
                        ",",
                        " or",
                        " the",
                        " grown",
                        " up",
                        " who",
                        " remembers",
                        " how",
                        " hard",
                        " it",
                        " was",
                        " to"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 62.39588928222656
        },
        {
            "feature_index": 318,
            "gpt_predictions": [
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    1.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "words related to negative actions or emotions",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " producers, who rely on a perception of luxury to sell at higher prices aboard, also",
                    "max_token": " luxury",
                    "tokens": [
                        " producers",
                        ",",
                        " who",
                        " rely",
                        " on",
                        " a",
                        " perception",
                        " of",
                        " luxury",
                        " to",
                        " sell",
                        " at",
                        " higher",
                        " prices",
                        " aboard",
                        ",",
                        " also"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        7.112383842468262,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "ence, the corrupt and feminine arts of pleasure and ornament that Sprat sees spread throughout",
                    "max_token": " pleasure",
                    "tokens": [
                        "ence",
                        ",",
                        " the",
                        " corrupt",
                        " and",
                        " feminine",
                        " arts",
                        " of",
                        " pleasure",
                        " and",
                        " ornament",
                        " that",
                        " Spr",
                        "at",
                        " sees",
                        " spread",
                        " throughout"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        13.55226898193359,
                        0,
                        12.60378932952881,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " examples of liberal \u00e2\u0122\u013eschadenfreude,\u00e2\u0122\u013f as he\u00e2\u0122\u013bs",
                    "max_token": "ude",
                    "tokens": [
                        " examples",
                        " of",
                        " liberal",
                        " \u00e2\u0122",
                        "\u013e",
                        "sch",
                        "aden",
                        "fre",
                        "ude",
                        ",",
                        "\u00e2\u0122",
                        "\u013f",
                        " as",
                        " he",
                        "\u00e2\u0122",
                        "\u013b",
                        "s"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        5.542647361755371,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " state represents a new and pervasive form of rule, and a perversion of constitutional self",
                    "max_token": " rule",
                    "tokens": [
                        " state",
                        " represents",
                        " a",
                        " new",
                        " and",
                        " pervasive",
                        " form",
                        " of",
                        " rule",
                        ",",
                        " and",
                        " a",
                        " per",
                        "version",
                        " of",
                        " constitutional",
                        " self"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        10.82342338562012,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " those great motivators and justifiers of malice and stupidity: idealism, dogmat",
                    "max_token": " malice",
                    "tokens": [
                        " those",
                        " great",
                        " motiv",
                        "ators",
                        " and",
                        " just",
                        "ifiers",
                        " of",
                        " malice",
                        " and",
                        " stupidity",
                        ":",
                        " ideal",
                        "ism",
                        ",",
                        " dog",
                        "mat"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        14.62522792816162,
                        0,
                        6.043818950653076,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "\u00e2\u0122\u013bThe<|endoftext|> potential impact surveys and mitigation and restoration plans\u00e2\u0122\u013b\u00e2\u0122\u013b are",
                    "tokens": [
                        "\u00e2\u0122",
                        "\u013b",
                        "The",
                        "<|endoftext|>",
                        " potential",
                        " impact",
                        " surveys",
                        " and",
                        " mitigation",
                        " and",
                        " restoration",
                        " plans",
                        "\u00e2\u0122",
                        "\u013b",
                        "\u00e2\u0122",
                        "\u013b",
                        " are"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " that kid who felt unwanted, or the grown up who remembers how hard it was to",
                    "tokens": [
                        " that",
                        " kid",
                        " who",
                        " felt",
                        " unwanted",
                        ",",
                        " or",
                        " the",
                        " grown",
                        " up",
                        " who",
                        " remembers",
                        " how",
                        " hard",
                        " it",
                        " was",
                        " to"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " Trump and Hillary in the first debate, before either said a word, Trump made what",
                    "tokens": [
                        " Trump",
                        " and",
                        " Hillary",
                        " in",
                        " the",
                        " first",
                        " debate",
                        ",",
                        " before",
                        " either",
                        " said",
                        " a",
                        " word",
                        ",",
                        " Trump",
                        " made",
                        " what"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "3 billion) installing everything from traffic-management technologies to smart electricity grids.\u010a\u010a",
                    "tokens": [
                        "3",
                        " billion",
                        ")",
                        " installing",
                        " everything",
                        " from",
                        " traffic",
                        "-",
                        "management",
                        " technologies",
                        " to",
                        " smart",
                        " electricity",
                        " grids",
                        ".",
                        "\u010a",
                        "\u010a"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " city staff (Fire, Police, Crime Prevention, and more)\u010a\u010aOption to",
                    "tokens": [
                        " city",
                        " staff",
                        " (",
                        "Fire",
                        ",",
                        " Police",
                        ",",
                        " Crime",
                        " Prevention",
                        ",",
                        " and",
                        " more",
                        ")",
                        "\u010a",
                        "\u010a",
                        "Option",
                        " to"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 18.65837478637695
        },
        {
            "feature_index": 548,
            "gpt_predictions": [
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "colors",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " harnesses complement what appears to be a blue stained burl wood dash, and condition",
                    "max_token": " blue",
                    "tokens": [
                        " harness",
                        "es",
                        " complement",
                        " what",
                        " appears",
                        " to",
                        " be",
                        " a",
                        " blue",
                        " stained",
                        " bur",
                        "l",
                        " wood",
                        " dash",
                        ",",
                        " and",
                        " condition"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        25.14552116394043,
                        3.20676589012146,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " but survived.\u010a\u010aWearing an orange prison jumpsuit and wire-rimmed",
                    "max_token": " orange",
                    "tokens": [
                        " but",
                        " survived",
                        ".",
                        "\u010a",
                        "\u010a",
                        "W",
                        "earing",
                        " an",
                        " orange",
                        " prison",
                        " jumps",
                        "uit",
                        " and",
                        " wire",
                        "-",
                        "rim",
                        "med"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        32.0811882019043,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0.3912909030914307
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " over from battle to battle; going from green to elite! Not only do units gain",
                    "max_token": " green",
                    "tokens": [
                        " over",
                        " from",
                        " battle",
                        " to",
                        " battle",
                        ";",
                        " going",
                        " from",
                        " green",
                        " to",
                        " elite",
                        "!",
                        " Not",
                        " only",
                        " do",
                        " units",
                        " gain"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        9.720568656921387,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " clean look \u00e2\u0122\u0136 straight lines and a cool blue palette.\u010a\u010aThat look has carried",
                    "max_token": " blue",
                    "tokens": [
                        " clean",
                        " look",
                        " \u00e2\u0122\u0136",
                        " straight",
                        " lines",
                        " and",
                        " a",
                        " cool",
                        " blue",
                        " palette",
                        ".",
                        "\u010a",
                        "\u010a",
                        "That",
                        " look",
                        " has",
                        " carried"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        34.90236282348633,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "leeved crew-neck sweater, navy blue flats and a pearl necklace. And then",
                    "max_token": " blue",
                    "tokens": [
                        "lee",
                        "ved",
                        " crew",
                        "-",
                        "neck",
                        " sweater",
                        ",",
                        " navy",
                        " blue",
                        " flats",
                        " and",
                        " a",
                        " pearl",
                        " necklace",
                        ".",
                        " And",
                        " then"
                    ],
                    "values": [
                        0,
                        0.7687832713127136,
                        0,
                        0,
                        0,
                        0,
                        0,
                        20.7056941986084,
                        34.07372283935547,
                        0,
                        0,
                        0,
                        5.94032621383667,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " quick lead only to have to bow out before the end of the game because they went",
                    "tokens": [
                        " quick",
                        " lead",
                        " only",
                        " to",
                        " have",
                        " to",
                        " bow",
                        " out",
                        " before",
                        " the",
                        " end",
                        " of",
                        " the",
                        " game",
                        " because",
                        " they",
                        " went"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "\u010aAldrin was a drug user before, Castillo admitted, but he had",
                    "tokens": [
                        "\u010a",
                        "A",
                        "ld",
                        "rin",
                        " was",
                        " a",
                        " drug",
                        " user",
                        " before",
                        ",",
                        " Cast",
                        "illo",
                        " admitted",
                        ",",
                        " but",
                        " he",
                        " had"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " height at some international summit.\u010a\u010aOr is it because of these deeply controversial statements",
                    "tokens": [
                        " height",
                        " at",
                        " some",
                        " international",
                        " summit",
                        ".",
                        "\u010a",
                        "\u010a",
                        "Or",
                        " is",
                        " it",
                        " because",
                        " of",
                        " these",
                        " deeply",
                        " controversial",
                        " statements"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " years after he was jailed for espionage, before his release in 1969.\u010a\u010aHe",
                    "tokens": [
                        " years",
                        " after",
                        " he",
                        " was",
                        " jailed",
                        " for",
                        " espionage",
                        ",",
                        " before",
                        " his",
                        " release",
                        " in",
                        " 1969",
                        ".",
                        "\u010a",
                        "\u010a",
                        "He"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " faces with respect to women.\u010a\u010aA CNN poll released Monday showed Obama moving into",
                    "tokens": [
                        " faces",
                        " with",
                        " respect",
                        " to",
                        " women",
                        ".",
                        "\u010a",
                        "\u010a",
                        "A",
                        " CNN",
                        " poll",
                        " released",
                        " Monday",
                        " showed",
                        " Obama",
                        " moving",
                        " into"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 42.18172836303711
        },
        {
            "feature_index": 998,
            "gpt_predictions": [
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "phrases related to social interactions involving people's behaviors and relationships",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " parents taught me to love people across the board and always be open to people,\" said",
                    "max_token": " board",
                    "tokens": [
                        " parents",
                        " taught",
                        " me",
                        " to",
                        " love",
                        " people",
                        " across",
                        " the",
                        " board",
                        " and",
                        " always",
                        " be",
                        " open",
                        " to",
                        " people",
                        ",\"",
                        " said"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0.2243621349334717,
                        2.061154365539551,
                        1.122049450874329,
                        6.271233081817627,
                        1.273785948753357,
                        0,
                        0.31805419921875,
                        0.2833294868469238,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " many Irish people in Australia to have an enjoyable celebration of St Patrick's Day and St",
                    "max_token": " enjoyable",
                    "tokens": [
                        " many",
                        " Irish",
                        " people",
                        " in",
                        " Australia",
                        " to",
                        " have",
                        " an",
                        " enjoyable",
                        " celebration",
                        " of",
                        " St",
                        " Patrick",
                        "'s",
                        " Day",
                        " and",
                        " St"
                    ],
                    "values": [
                        0,
                        0,
                        0.4998364448547363,
                        3.619210958480835,
                        2.236884832382202,
                        2.020378351211548,
                        3.043856859207153,
                        4.730143547058105,
                        5.373110294342041,
                        4.272918224334717,
                        1.764533996582031,
                        0,
                        0,
                        0,
                        0.03012063354253769,
                        0.08921656012535095,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " quite clearly people can track other people not just by devices but by software . . .",
                    "max_token": " just",
                    "tokens": [
                        " quite",
                        " clearly",
                        " people",
                        " can",
                        " track",
                        " other",
                        " people",
                        " not",
                        " just",
                        " by",
                        " devices",
                        " but",
                        " by",
                        " software",
                        " .",
                        " .",
                        " ."
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        2.651607275009155,
                        3.161376237869263,
                        0.357204407453537,
                        2.116690635681152,
                        3.559005737304688,
                        4.983542442321777,
                        1.75547182559967,
                        0,
                        2.866504669189453,
                        0.9187122583389282,
                        0.2272437810897827,
                        0.965202271938324,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "000 people who all are members of communities summed up in the distribution list general-list",
                    "max_token": " summed",
                    "tokens": [
                        "000",
                        " people",
                        " who",
                        " all",
                        " are",
                        " members",
                        " of",
                        " communities",
                        " summed",
                        " up",
                        " in",
                        " the",
                        " distribution",
                        " list",
                        " general",
                        "-",
                        "list"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0.4312761425971985,
                        0,
                        1.980087637901306,
                        6.757612705230713,
                        5.590889453887939,
                        2.592499732971191,
                        1.69597852230072,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "in-law, despite very busy lives, contribute time and energy, as well as",
                    "max_token": ",",
                    "tokens": [
                        "in",
                        "-",
                        "law",
                        ",",
                        " despite",
                        " very",
                        " busy",
                        " lives",
                        ",",
                        " contribute",
                        " time",
                        " and",
                        " energy",
                        ",",
                        " as",
                        " well",
                        " as"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        1.831048965454102,
                        0.3358391225337982,
                        3.490878105163574,
                        2.28536057472229,
                        5.656495094299316,
                        2.657811641693115,
                        0,
                        0,
                        0.5680046677589417,
                        1.322160005569458,
                        0,
                        1.817387938499451,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " coat.\u010a\u010aDetective Inspector Steve Christian, from Liverpool CID, said:",
                    "tokens": [
                        " coat",
                        ".",
                        "\u010a",
                        "\u010a",
                        "Det",
                        "ective",
                        " Inspector",
                        " Steve",
                        " Christian",
                        ",",
                        " from",
                        " Liverpool",
                        " C",
                        "ID",
                        ",",
                        " said",
                        ":"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " drop out of the presidential race, according to NBC News ' Andrea Mitchell.\u010a\u010a",
                    "tokens": [
                        " drop",
                        " out",
                        " of",
                        " the",
                        " presidential",
                        " race",
                        ",",
                        " according",
                        " to",
                        " NBC",
                        " News",
                        " '",
                        " Andrea",
                        " Mitchell",
                        ".",
                        "\u010a",
                        "\u010a"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " Rumors that Amazon will soon begin accepting Bitcoin are persistent. If the retail giant does",
                    "tokens": [
                        " Rum",
                        "ors",
                        " that",
                        " Amazon",
                        " will",
                        " soon",
                        " begin",
                        " accepting",
                        " Bitcoin",
                        " are",
                        " persistent",
                        ".",
                        " If",
                        " the",
                        " retail",
                        " giant",
                        " does"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": ", and banking flows was a monthly net TIC (Treasury International Capital)<|endoftext|>",
                    "tokens": [
                        ",",
                        " and",
                        " banking",
                        " flows",
                        " was",
                        " a",
                        " monthly",
                        " net",
                        " T",
                        "IC",
                        " (",
                        "Tre",
                        "asury",
                        " International",
                        " Capital",
                        ")",
                        "<|endoftext|>"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " faces with respect to women.\u010a\u010aA CNN poll released Monday showed Obama moving into",
                    "tokens": [
                        " faces",
                        " with",
                        " respect",
                        " to",
                        " women",
                        ".",
                        "\u010a",
                        "\u010a",
                        "A",
                        " CNN",
                        " poll",
                        " released",
                        " Monday",
                        " showed",
                        " Obama",
                        " moving",
                        " into"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 7.675356388092041
        },
        {
            "feature_index": 714,
            "gpt_predictions": [
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    1.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "dates or months mentioned within texts",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "\u010aDated: JUL 09 2009<|endoftext|>The defensive end, who broke his leg after",
                    "max_token": "The",
                    "tokens": [
                        "\u010a",
                        "D",
                        "ated",
                        ":",
                        " JUL",
                        " 09",
                        " 2009",
                        "<|endoftext|>",
                        "The",
                        " defensive",
                        " end",
                        ",",
                        " who",
                        " broke",
                        " his",
                        " leg",
                        " after"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0.4790827035903931,
                        12.19299507141113,
                        0,
                        0,
                        1.079825639724731,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " campaigns.\u010a\u010aPCB Quality<|endoftext|>You'll see lots of tips and tricks this",
                    "max_token": "You",
                    "tokens": [
                        " campaigns",
                        ".",
                        "\u010a",
                        "\u010a",
                        "PC",
                        "B",
                        " Quality",
                        "<|endoftext|>",
                        "You",
                        "'ll",
                        " see",
                        " lots",
                        " of",
                        " tips",
                        " and",
                        " tricks",
                        " this"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        16.75138664245605,
                        4.694691181182861,
                        1.193324685096741,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "<|endoftext|> frequency compared to his predecessors.<|endoftext|>There\u00e2\u0122\u013bs no assurance a new CEO",
                    "max_token": "There",
                    "tokens": [
                        "<|endoftext|>",
                        " frequency",
                        " compared",
                        " to",
                        " his",
                        " predecessors",
                        ".",
                        "<|endoftext|>",
                        "There",
                        "\u00e2\u0122",
                        "\u013b",
                        "s",
                        " no",
                        " assurance",
                        " a",
                        " new",
                        " CEO"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        11.15617942810059,
                        3.920501708984375,
                        0,
                        1.380158543586731,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " NBA efficiency ratings, after finishing eighth<|endoftext|>On Wednesday, Special Rapporteur on freedom",
                    "max_token": "On",
                    "tokens": [
                        " NBA",
                        " efficiency",
                        " ratings",
                        ",",
                        " after",
                        " finishing",
                        " eighth",
                        "<|endoftext|>",
                        "On",
                        " Wednesday",
                        ",",
                        " Special",
                        " Rapp",
                        "ort",
                        "eur",
                        " on",
                        " freedom"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        12.94377136230469,
                        7.001643657684326,
                        4.593774318695068,
                        0.8761634826660156,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " USC to request a DMCA exemption for<|endoftext|>Howdy Shadowverse players! Man! It",
                    "max_token": "How",
                    "tokens": [
                        " USC",
                        " to",
                        " request",
                        " a",
                        " DMCA",
                        " exemption",
                        " for",
                        "<|endoftext|>",
                        "How",
                        "dy",
                        " Shadow",
                        "verse",
                        " players",
                        "!",
                        " Man",
                        "!",
                        " It"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0.4271267056465149,
                        16.89662170410156,
                        3.114123106002808,
                        5.577659130096436,
                        0.7655596733093262,
                        1.761181592941284,
                        3.56092357635498,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " quick lead only to have to bow out before the end of the game because they went",
                    "tokens": [
                        " quick",
                        " lead",
                        " only",
                        " to",
                        " have",
                        " to",
                        " bow",
                        " out",
                        " before",
                        " the",
                        " end",
                        " of",
                        " the",
                        " game",
                        " because",
                        " they",
                        " went"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "\u010aAldrin was a drug user before, Castillo admitted, but he had",
                    "tokens": [
                        "\u010a",
                        "A",
                        "ld",
                        "rin",
                        " was",
                        " a",
                        " drug",
                        " user",
                        " before",
                        ",",
                        " Cast",
                        "illo",
                        " admitted",
                        ",",
                        " but",
                        " he",
                        " had"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " height at some international summit.\u010a\u010aOr is it because of these deeply controversial statements",
                    "tokens": [
                        " height",
                        " at",
                        " some",
                        " international",
                        " summit",
                        ".",
                        "\u010a",
                        "\u010a",
                        "Or",
                        " is",
                        " it",
                        " because",
                        " of",
                        " these",
                        " deeply",
                        " controversial",
                        " statements"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " years after he was jailed for espionage, before his release in 1969.\u010a\u010aHe",
                    "tokens": [
                        " years",
                        " after",
                        " he",
                        " was",
                        " jailed",
                        " for",
                        " espionage",
                        ",",
                        " before",
                        " his",
                        " release",
                        " in",
                        " 1969",
                        ".",
                        "\u010a",
                        "\u010a",
                        "He"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " faces with respect to women.\u010a\u010aA CNN poll released Monday showed Obama moving into",
                    "tokens": [
                        " faces",
                        " with",
                        " respect",
                        " to",
                        " women",
                        ".",
                        "\u010a",
                        "\u010a",
                        "A",
                        " CNN",
                        " poll",
                        " released",
                        " Monday",
                        " showed",
                        " Obama",
                        " moving",
                        " into"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 17.89118957519531
        },
        {
            "feature_index": 753,
            "gpt_predictions": [
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "names of individuals",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "-old mother told relatives that her son Matthew had shot himself and three other family members",
                    "max_token": " Matthew",
                    "tokens": [
                        "-",
                        "old",
                        " mother",
                        " told",
                        " relatives",
                        " that",
                        " her",
                        " son",
                        " Matthew",
                        " had",
                        " shot",
                        " himself",
                        " and",
                        " three",
                        " other",
                        " family",
                        " members"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        17.4900016784668,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " over $5,000. His son Andy wrote this note to supporters last night:",
                    "max_token": " Andy",
                    "tokens": [
                        " over",
                        " $",
                        "5",
                        ",",
                        "000",
                        ".",
                        " His",
                        " son",
                        " Andy",
                        " wrote",
                        " this",
                        " note",
                        " to",
                        " supporters",
                        " last",
                        " night",
                        ":"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        29.35645866394043,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "\u010aHayley eventually married her childhood friend Sam, with whom she shared her infertility struggles",
                    "max_token": " Sam",
                    "tokens": [
                        "\u010a",
                        "Hay",
                        "ley",
                        " eventually",
                        " married",
                        " her",
                        " childhood",
                        " friend",
                        " Sam",
                        ",",
                        " with",
                        " whom",
                        " she",
                        " shared",
                        " her",
                        " infertility",
                        " struggles"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        12.87574481964111,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 2,
                    "sentence_string": " new wife Jennifer and their unborn daughter Cecilia\u010a\u010aIt was also revealed today that",
                    "max_token": " Jennifer",
                    "tokens": [
                        " new",
                        " wife",
                        " Jennifer",
                        " and",
                        " their",
                        " unborn",
                        " daughter",
                        " Cec",
                        "ilia",
                        "\u010a",
                        "\u010a",
                        "It",
                        " was",
                        " also",
                        " revealed",
                        " today",
                        " that"
                    ],
                    "values": [
                        0,
                        0,
                        24.3549747467041,
                        0,
                        0,
                        0.7181652784347534,
                        0,
                        18.6578254699707,
                        19.55332374572754,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "laws' residence where she and husband, Josh, were staying.\u010a\u010a\"While",
                    "max_token": " Josh",
                    "tokens": [
                        "laws",
                        "'",
                        " residence",
                        " where",
                        " she",
                        " and",
                        " husband",
                        ",",
                        " Josh",
                        ",",
                        " were",
                        " staying",
                        ".",
                        "\u010a",
                        "\u010a",
                        "\"",
                        "While"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        29.5202693939209,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " coat.\u010a\u010aDetective Inspector Steve Christian, from Liverpool CID, said:",
                    "tokens": [
                        " coat",
                        ".",
                        "\u010a",
                        "\u010a",
                        "Det",
                        "ective",
                        " Inspector",
                        " Steve",
                        " Christian",
                        ",",
                        " from",
                        " Liverpool",
                        " C",
                        "ID",
                        ",",
                        " said",
                        ":"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " drop out of the presidential race, according to NBC News ' Andrea Mitchell.\u010a\u010a",
                    "tokens": [
                        " drop",
                        " out",
                        " of",
                        " the",
                        " presidential",
                        " race",
                        ",",
                        " according",
                        " to",
                        " NBC",
                        " News",
                        " '",
                        " Andrea",
                        " Mitchell",
                        ".",
                        "\u010a",
                        "\u010a"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " documenting the protective effect of male circumcision on HIV infection in young adults pose significant challenges to",
                    "tokens": [
                        " documenting",
                        " the",
                        " protective",
                        " effect",
                        " of",
                        " male",
                        " circumcision",
                        " on",
                        " HIV",
                        " infection",
                        " in",
                        " young",
                        " adults",
                        " pose",
                        " significant",
                        " challenges",
                        " to"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " the virus is no laughing matter, a viral video features a Chicago pup named Herbert who",
                    "tokens": [
                        " the",
                        " virus",
                        " is",
                        " no",
                        " laughing",
                        " matter",
                        ",",
                        " a",
                        " viral",
                        " video",
                        " features",
                        " a",
                        " Chicago",
                        " pup",
                        " named",
                        " Herbert",
                        " who"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " faces with respect to women.\u010a\u010aA CNN poll released Monday showed Obama moving into",
                    "tokens": [
                        " faces",
                        " with",
                        " respect",
                        " to",
                        " women",
                        ".",
                        "\u010a",
                        "\u010a",
                        "A",
                        " CNN",
                        " poll",
                        " released",
                        " Monday",
                        " showed",
                        " Obama",
                        " moving",
                        " into"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 37.5763053894043
        },
        {
            "feature_index": 327,
            "gpt_predictions": [
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    0,
                    1.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "phrases that are commonly used in research or formal reports",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " the banks have done.\u010a\u010a\u00e2\u0122\u013eI know that we have got to play",
                    "max_token": "\u013e",
                    "tokens": [
                        " the",
                        " banks",
                        " have",
                        " done",
                        ".",
                        "\u010a",
                        "\u010a",
                        "\u00e2\u0122",
                        "\u013e",
                        "I",
                        " know",
                        " that",
                        " we",
                        " have",
                        " got",
                        " to",
                        " play"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        10.92638969421387,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " Byron, Liberty Institute senior counsel. \u00e2\u0122\u013eConsidering the new memorial\u00e2\u0122\u013bs history",
                    "max_token": "\u013e",
                    "tokens": [
                        " Byron",
                        ",",
                        " Liberty",
                        " Institute",
                        " senior",
                        " counsel",
                        ".",
                        " \u00e2\u0122",
                        "\u013e",
                        "Considering",
                        " the",
                        " new",
                        " memorial",
                        "\u00e2\u0122",
                        "\u013b",
                        "s",
                        " history"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        4.29404878616333,
                        22.92844390869141,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " of 25 years to decompose.(7)\u010a<|endoftext|>Target\u00e2\u0122\u013bs anti-",
                    "max_token": ")",
                    "tokens": [
                        " of",
                        " 25",
                        " years",
                        " to",
                        " decom",
                        "pose",
                        ".(",
                        "7",
                        ")",
                        "\u010a",
                        "<|endoftext|>",
                        "Target",
                        "\u00e2\u0122",
                        "\u013b",
                        "s",
                        " anti",
                        "-"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        5.982387065887451,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " their voices.\u00e2\u0122\u013f\u010a\u010a\u00e2\u0122\u013eThis indicates that the voice control<|endoftext|>ts",
                    "max_token": "\u013e",
                    "tokens": [
                        " their",
                        " voices",
                        ".",
                        "\u00e2\u0122",
                        "\u013f",
                        "\u010a",
                        "\u010a",
                        "\u00e2\u0122",
                        "\u013e",
                        "This",
                        " indicates",
                        " that",
                        " the",
                        " voice",
                        " control",
                        "<|endoftext|>",
                        "ts"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        5.651240348815918,
                        0,
                        0,
                        0.7460529208183289,
                        27.58264541625977,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "\u00e2\u0122\u013f Abbeel says. \u00e2\u0122\u013eThe challenges posed by robotic towel-fold",
                    "max_token": "\u013e",
                    "tokens": [
                        "\u00e2\u0122",
                        "\u013f",
                        " Ab",
                        "be",
                        "el",
                        " says",
                        ".",
                        " \u00e2\u0122",
                        "\u013e",
                        "The",
                        " challenges",
                        " posed",
                        " by",
                        " robotic",
                        " towel",
                        "-",
                        "fold"
                    ],
                    "values": [
                        0,
                        2.151858568191528,
                        0,
                        0,
                        0,
                        0,
                        0.04854229092597961,
                        8.75921630859375,
                        26.52619171142578,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "\u00e2\u0122\u013bThe<|endoftext|> potential impact surveys and mitigation and restoration plans\u00e2\u0122\u013b\u00e2\u0122\u013b are",
                    "tokens": [
                        "\u00e2\u0122",
                        "\u013b",
                        "The",
                        "<|endoftext|>",
                        " potential",
                        " impact",
                        " surveys",
                        " and",
                        " mitigation",
                        " and",
                        " restoration",
                        " plans",
                        "\u00e2\u0122",
                        "\u013b",
                        "\u00e2\u0122",
                        "\u013b",
                        " are"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " that kid who felt unwanted, or the grown up who remembers how hard it was to",
                    "tokens": [
                        " that",
                        " kid",
                        " who",
                        " felt",
                        " unwanted",
                        ",",
                        " or",
                        " the",
                        " grown",
                        " up",
                        " who",
                        " remembers",
                        " how",
                        " hard",
                        " it",
                        " was",
                        " to"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " Trump and Hillary in the first debate, before either said a word, Trump made what",
                    "tokens": [
                        " Trump",
                        " and",
                        " Hillary",
                        " in",
                        " the",
                        " first",
                        " debate",
                        ",",
                        " before",
                        " either",
                        " said",
                        " a",
                        " word",
                        ",",
                        " Trump",
                        " made",
                        " what"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "3 billion) installing everything from traffic-management technologies to smart electricity grids.\u010a\u010a",
                    "tokens": [
                        "3",
                        " billion",
                        ")",
                        " installing",
                        " everything",
                        " from",
                        " traffic",
                        "-",
                        "management",
                        " technologies",
                        " to",
                        " smart",
                        " electricity",
                        " grids",
                        ".",
                        "\u010a",
                        "\u010a"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " city staff (Fire, Police, Crime Prevention, and more)\u010a\u010aOption to",
                    "tokens": [
                        " city",
                        " staff",
                        " (",
                        "Fire",
                        ",",
                        " Police",
                        ",",
                        " Crime",
                        " Prevention",
                        ",",
                        " and",
                        " more",
                        ")",
                        "\u010a",
                        "\u010a",
                        "Option",
                        " to"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 30.16028785705566
        },
        {
            "feature_index": 382,
            "gpt_predictions": [
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "phrases involving step-by-step instructions",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "related matters\" on a case-by-case basis and appropriate action will be taken",
                    "max_token": "-",
                    "tokens": [
                        "related",
                        " matters",
                        "\"",
                        " on",
                        " a",
                        " case",
                        "-",
                        "by",
                        "-",
                        "case",
                        " basis",
                        " and",
                        " appropriate",
                        " action",
                        " will",
                        " be",
                        " taken"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        11.62279605865479,
                        65.35600280761719,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " preview, where we go position-by-position until we have looked at all 90",
                    "max_token": "-",
                    "tokens": [
                        " preview",
                        ",",
                        " where",
                        " we",
                        " go",
                        " position",
                        "-",
                        "by",
                        "-",
                        "position",
                        " until",
                        " we",
                        " have",
                        " looked",
                        " at",
                        " all",
                        " 90"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        15.99447727203369,
                        55.08119201660156,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " new study looking at the state-by-state value of welfare for a mother with",
                    "max_token": "-",
                    "tokens": [
                        " new",
                        " study",
                        " looking",
                        " at",
                        " the",
                        " state",
                        "-",
                        "by",
                        "-",
                        "state",
                        " value",
                        " of",
                        " welfare",
                        " for",
                        " a",
                        " mother",
                        " with"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        4.080380439758301,
                        51.49861526489258,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " running time must consist of frame-by-frame animation. And like all Oscar-",
                    "max_token": "-",
                    "tokens": [
                        " running",
                        " time",
                        " must",
                        " consist",
                        " of",
                        " frame",
                        "-",
                        "by",
                        "-",
                        "frame",
                        " animation",
                        ".",
                        " And",
                        " like",
                        " all",
                        " Oscar",
                        "-"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        16.94542503356934,
                        70.236328125,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " \u00e2\u0122\u0135 which is why, one-by-one, our musical tastes, political views",
                    "max_token": "-",
                    "tokens": [
                        " \u00e2\u0122\u0135",
                        " which",
                        " is",
                        " why",
                        ",",
                        " one",
                        "-",
                        "by",
                        "-",
                        "one",
                        ",",
                        " our",
                        " musical",
                        " tastes",
                        ",",
                        " political",
                        " views"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        8.615488052368164,
                        55.98789978027344,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "\u00e2\u0122\u013bThe<|endoftext|> potential impact surveys and mitigation and restoration plans\u00e2\u0122\u013b\u00e2\u0122\u013b are",
                    "tokens": [
                        "\u00e2\u0122",
                        "\u013b",
                        "The",
                        "<|endoftext|>",
                        " potential",
                        " impact",
                        " surveys",
                        " and",
                        " mitigation",
                        " and",
                        " restoration",
                        " plans",
                        "\u00e2\u0122",
                        "\u013b",
                        "\u00e2\u0122",
                        "\u013b",
                        " are"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " that kid who felt unwanted, or the grown up who remembers how hard it was to",
                    "tokens": [
                        " that",
                        " kid",
                        " who",
                        " felt",
                        " unwanted",
                        ",",
                        " or",
                        " the",
                        " grown",
                        " up",
                        " who",
                        " remembers",
                        " how",
                        " hard",
                        " it",
                        " was",
                        " to"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " Trump and Hillary in the first debate, before either said a word, Trump made what",
                    "tokens": [
                        " Trump",
                        " and",
                        " Hillary",
                        " in",
                        " the",
                        " first",
                        " debate",
                        ",",
                        " before",
                        " either",
                        " said",
                        " a",
                        " word",
                        ",",
                        " Trump",
                        " made",
                        " what"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "3 billion) installing everything from traffic-management technologies to smart electricity grids.\u010a\u010a",
                    "tokens": [
                        "3",
                        " billion",
                        ")",
                        " installing",
                        " everything",
                        " from",
                        " traffic",
                        "-",
                        "management",
                        " technologies",
                        " to",
                        " smart",
                        " electricity",
                        " grids",
                        ".",
                        "\u010a",
                        "\u010a"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " city staff (Fire, Police, Crime Prevention, and more)\u010a\u010aOption to",
                    "tokens": [
                        " city",
                        " staff",
                        " (",
                        "Fire",
                        ",",
                        " Police",
                        ",",
                        " Crime",
                        " Prevention",
                        ",",
                        " and",
                        " more",
                        ")",
                        "\u010a",
                        "\u010a",
                        "Option",
                        " to"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 79.48687744140625
        },
        {
            "feature_index": 451,
            "gpt_predictions": [
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    1.0
                ],
                [
                    0,
                    1.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    1.0
                ]
            ],
            "description": "specific phrases or proper nouns within longer strings of text",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " page or so to architecture and urbanism, but the point is that he is scathing",
                    "max_token": ",",
                    "tokens": [
                        " page",
                        " or",
                        " so",
                        " to",
                        " architecture",
                        " and",
                        " urban",
                        "ism",
                        ",",
                        " but",
                        " the",
                        " point",
                        " is",
                        " that",
                        " he",
                        " is",
                        " scathing"
                    ],
                    "values": [
                        6.619438648223877,
                        0,
                        9.189448356628418,
                        5.874995231628418,
                        0,
                        0.8481632471084595,
                        0,
                        0,
                        9.55058479309082,
                        9.492595672607422,
                        4.626617431640625,
                        1.216718673706055,
                        0.8691782355308533,
                        0,
                        3.511276721954346,
                        2.099506855010986,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 7,
                    "sentence_string": " the video; the awkwardness begins at around 3:51:\u010a\u010aYouTube\u010a",
                    "max_token": " at",
                    "tokens": [
                        " the",
                        " video",
                        ";",
                        " the",
                        " awkward",
                        "ness",
                        " begins",
                        " at",
                        " around",
                        " 3",
                        ":",
                        "51",
                        ":",
                        "\u010a",
                        "\u010a",
                        "YouTube",
                        "\u010a"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0.6240056157112122,
                        2.436100006103516,
                        2.989883661270142,
                        4.539024829864502,
                        9.737674713134766,
                        7.748996734619141,
                        5.97736644744873,
                        4.058069229125977,
                        9.252525329589844,
                        6.239969730377197,
                        0,
                        0.9179049730300903,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " by the early Church Fathers. They are alluded to in the Bible in the pastorals",
                    "max_token": " alluded",
                    "tokens": [
                        " by",
                        " the",
                        " early",
                        " Church",
                        " Fathers",
                        ".",
                        " They",
                        " are",
                        " alluded",
                        " to",
                        " in",
                        " the",
                        " Bible",
                        " in",
                        " the",
                        " pastor",
                        "als"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        3.390215158462524,
                        0,
                        1.538754820823669,
                        10.45574569702148,
                        7.581354141235352,
                        8.394956588745117,
                        6.57274341583252,
                        4.001978874206543,
                        7.962050437927246,
                        6.738758563995361,
                        3.682420015335083,
                        3.625891208648682
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "T IV's DLC Archangel I M is alluded to be lying about the archangels",
                    "max_token": " alluded",
                    "tokens": [
                        "T",
                        " IV",
                        "'s",
                        " DLC",
                        " Archangel",
                        " I",
                        " M",
                        " is",
                        " alluded",
                        " to",
                        " be",
                        " lying",
                        " about",
                        " the",
                        " arch",
                        "ang",
                        "els"
                    ],
                    "values": [
                        0,
                        3.387712955474854,
                        2.476382732391357,
                        0.4535184800624847,
                        0,
                        0.7800908088684082,
                        0,
                        6.687222480773926,
                        11.170334815979,
                        7.530694961547852,
                        4.122125625610352,
                        1.227904438972473,
                        2.728649139404297,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " made plain at the end of the film, as Verdoux, put on trial,",
                    "max_token": ",",
                    "tokens": [
                        " made",
                        " plain",
                        " at",
                        " the",
                        " end",
                        " of",
                        " the",
                        " film",
                        ",",
                        " as",
                        " Verd",
                        "oux",
                        ",",
                        " put",
                        " on",
                        " trial",
                        ","
                    ],
                    "values": [
                        2.816566228866577,
                        2.351569414138794,
                        6.494925022125244,
                        6.004143238067627,
                        8.234914779663086,
                        8.768470764160156,
                        4.256908416748047,
                        6.792996406555176,
                        12.30229568481445,
                        6.714242935180664,
                        0,
                        5.916377067565918,
                        0,
                        0.6910147070884705,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "\u00e2\u0122\u013bThe<|endoftext|> potential impact surveys and mitigation and restoration plans\u00e2\u0122\u013b\u00e2\u0122\u013b are",
                    "tokens": [
                        "\u00e2\u0122",
                        "\u013b",
                        "The",
                        "<|endoftext|>",
                        " potential",
                        " impact",
                        " surveys",
                        " and",
                        " mitigation",
                        " and",
                        " restoration",
                        " plans",
                        "\u00e2\u0122",
                        "\u013b",
                        "\u00e2\u0122",
                        "\u013b",
                        " are"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " that kid who felt unwanted, or the grown up who remembers how hard it was to",
                    "tokens": [
                        " that",
                        " kid",
                        " who",
                        " felt",
                        " unwanted",
                        ",",
                        " or",
                        " the",
                        " grown",
                        " up",
                        " who",
                        " remembers",
                        " how",
                        " hard",
                        " it",
                        " was",
                        " to"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " Trump and Hillary in the first debate, before either said a word, Trump made what",
                    "tokens": [
                        " Trump",
                        " and",
                        " Hillary",
                        " in",
                        " the",
                        " first",
                        " debate",
                        ",",
                        " before",
                        " either",
                        " said",
                        " a",
                        " word",
                        ",",
                        " Trump",
                        " made",
                        " what"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "3 billion) installing everything from traffic-management technologies to smart electricity grids.\u010a\u010a",
                    "tokens": [
                        "3",
                        " billion",
                        ")",
                        " installing",
                        " everything",
                        " from",
                        " traffic",
                        "-",
                        "management",
                        " technologies",
                        " to",
                        " smart",
                        " electricity",
                        " grids",
                        ".",
                        "\u010a",
                        "\u010a"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " city staff (Fire, Police, Crime Prevention, and more)\u010a\u010aOption to",
                    "tokens": [
                        " city",
                        " staff",
                        " (",
                        "Fire",
                        ",",
                        " Police",
                        ",",
                        " Crime",
                        " Prevention",
                        ",",
                        " and",
                        " more",
                        ")",
                        "\u010a",
                        "\u010a",
                        "Option",
                        " to"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 14.87227535247803
        },
        {
            "feature_index": 522,
            "gpt_predictions": [
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "words related to work, job satisfaction, and daily routines",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 11,
                    "sentence_string": " I am grateful for. My family, my business, my friends, my health,",
                    "max_token": " my",
                    "tokens": [
                        " I",
                        " am",
                        " grateful",
                        " for",
                        ".",
                        " My",
                        " family",
                        ",",
                        " my",
                        " business",
                        ",",
                        " my",
                        " friends",
                        ",",
                        " my",
                        " health",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        3.005555152893066,
                        0,
                        10.97574234008789,
                        12.93363666534424,
                        0.8977535367012024,
                        8.656845092773438,
                        13.16885375976562,
                        0,
                        5.248812198638916,
                        9.10759162902832,
                        0,
                        6.208621025085449
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " this life worth truly committing to: Kids, pets you bought or rescued and the things",
                    "max_token": ",",
                    "tokens": [
                        " this",
                        " life",
                        " worth",
                        " truly",
                        " committing",
                        " to",
                        ":",
                        " Kids",
                        ",",
                        " pets",
                        " you",
                        " bought",
                        " or",
                        " rescued",
                        " and",
                        " the",
                        " things"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        2.881917476654053,
                        0,
                        0,
                        6.048221111297607,
                        0,
                        12.38284969329834,
                        1.806484222412109,
                        0,
                        0,
                        0,
                        0,
                        6.653765201568604,
                        5.088741779327393,
                        3.410775661468506
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " well as bread, places to play in and pray in where nature may heal and cheer",
                    "max_token": " and",
                    "tokens": [
                        " well",
                        " as",
                        " bread",
                        ",",
                        " places",
                        " to",
                        " play",
                        " in",
                        " and",
                        " pray",
                        " in",
                        " where",
                        " nature",
                        " may",
                        " heal",
                        " and",
                        " cheer"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0.9272804260253906,
                        0,
                        1.023860573768616,
                        0,
                        0,
                        9.064908027648926,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": ", shower, commute, work, home, sleep. Each day you drive the same",
                    "max_token": ",",
                    "tokens": [
                        ",",
                        " shower",
                        ",",
                        " commute",
                        ",",
                        " work",
                        ",",
                        " home",
                        ",",
                        " sleep",
                        ".",
                        " Each",
                        " day",
                        " you",
                        " drive",
                        " the",
                        " same"
                    ],
                    "values": [
                        0.9336497187614441,
                        0,
                        2.855461597442627,
                        0,
                        9.092109680175781,
                        0.7627742886543274,
                        13.50180435180664,
                        3.288272619247437,
                        14.3509635925293,
                        0,
                        4.828027725219727,
                        0.7489621639251709,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " community to come together, whether for work or play.\u010a\u010aWired Houl",
                    "max_token": " or",
                    "tokens": [
                        " community",
                        " to",
                        " come",
                        " together",
                        ",",
                        " whether",
                        " for",
                        " work",
                        " or",
                        " play",
                        ".",
                        "\u010a",
                        "\u010a",
                        "W",
                        "ired",
                        " H",
                        "oul"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        11.3726110458374,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " quick lead only to have to bow out before the end of the game because they went",
                    "tokens": [
                        " quick",
                        " lead",
                        " only",
                        " to",
                        " have",
                        " to",
                        " bow",
                        " out",
                        " before",
                        " the",
                        " end",
                        " of",
                        " the",
                        " game",
                        " because",
                        " they",
                        " went"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "\u010aAldrin was a drug user before, Castillo admitted, but he had",
                    "tokens": [
                        "\u010a",
                        "A",
                        "ld",
                        "rin",
                        " was",
                        " a",
                        " drug",
                        " user",
                        " before",
                        ",",
                        " Cast",
                        "illo",
                        " admitted",
                        ",",
                        " but",
                        " he",
                        " had"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " height at some international summit.\u010a\u010aOr is it because of these deeply controversial statements",
                    "tokens": [
                        " height",
                        " at",
                        " some",
                        " international",
                        " summit",
                        ".",
                        "\u010a",
                        "\u010a",
                        "Or",
                        " is",
                        " it",
                        " because",
                        " of",
                        " these",
                        " deeply",
                        " controversial",
                        " statements"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " years after he was jailed for espionage, before his release in 1969.\u010a\u010aHe",
                    "tokens": [
                        " years",
                        " after",
                        " he",
                        " was",
                        " jailed",
                        " for",
                        " espionage",
                        ",",
                        " before",
                        " his",
                        " release",
                        " in",
                        " 1969",
                        ".",
                        "\u010a",
                        "\u010a",
                        "He"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " faces with respect to women.\u010a\u010aA CNN poll released Monday showed Obama moving into",
                    "tokens": [
                        " faces",
                        " with",
                        " respect",
                        " to",
                        " women",
                        ".",
                        "\u010a",
                        "\u010a",
                        "A",
                        " CNN",
                        " poll",
                        " released",
                        " Monday",
                        " showed",
                        " Obama",
                        " moving",
                        " into"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 20.03130722045898
        },
        {
            "feature_index": 218,
            "gpt_predictions": [
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "the word \"House\" followed by another word or phrase",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " where the Evans Scholars Foundation operates a Scholarship House.\u010a\u010aChapter living\u010a\u010aAt",
                    "max_token": " House",
                    "tokens": [
                        " where",
                        " the",
                        " Evans",
                        " Scholars",
                        " Foundation",
                        " operates",
                        " a",
                        " Scholarship",
                        " House",
                        ".",
                        "\u010a",
                        "\u010a",
                        "Chapter",
                        " living",
                        "\u010a",
                        "\u010a",
                        "At"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        28.91912269592285,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "/PA PA/PA Photos\u010a\u010aHouse prices are rising faster here than anywhere else",
                    "max_token": "House",
                    "tokens": [
                        "/",
                        "PA",
                        " PA",
                        "/",
                        "PA",
                        " Photos",
                        "\u010a",
                        "\u010a",
                        "House",
                        " prices",
                        " are",
                        " rising",
                        " faster",
                        " here",
                        " than",
                        " anywhere",
                        " else"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        33.82765579223633,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " Gavilon, owned by Japanese trading house Marubeni Corp, to oversee the",
                    "max_token": " house",
                    "tokens": [
                        " G",
                        "av",
                        "ilon",
                        ",",
                        " owned",
                        " by",
                        " Japanese",
                        " trading",
                        " house",
                        " Mar",
                        "ub",
                        "eni",
                        " Corp",
                        ",",
                        " to",
                        " oversee",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        10.22268676757812,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " with Depeche Mode, performing original House of Leaves music onstage from Poe\u00e2\u0122\u013b",
                    "max_token": " House",
                    "tokens": [
                        " with",
                        " De",
                        "pe",
                        "che",
                        " Mode",
                        ",",
                        " performing",
                        " original",
                        " House",
                        " of",
                        " Leaves",
                        " music",
                        " onstage",
                        " from",
                        " Poe",
                        "\u00e2\u0122",
                        "\u013b"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        41.84548568725586,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " of the Five Kings. This is why House Martell of Dorne is not the",
                    "max_token": " House",
                    "tokens": [
                        " of",
                        " the",
                        " Five",
                        " Kings",
                        ".",
                        " This",
                        " is",
                        " why",
                        " House",
                        " Mart",
                        "ell",
                        " of",
                        " D",
                        "orne",
                        " is",
                        " not",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        39.21643829345703,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " Trump and Hillary in the first debate, before either said a word, Trump made what",
                    "tokens": [
                        " Trump",
                        " and",
                        " Hillary",
                        " in",
                        " the",
                        " first",
                        " debate",
                        ",",
                        " before",
                        " either",
                        " said",
                        " a",
                        " word",
                        ",",
                        " Trump",
                        " made",
                        " what"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " that kid who felt unwanted, or the grown up who remembers how hard it was to",
                    "tokens": [
                        " that",
                        " kid",
                        " who",
                        " felt",
                        " unwanted",
                        ",",
                        " or",
                        " the",
                        " grown",
                        " up",
                        " who",
                        " remembers",
                        " how",
                        " hard",
                        " it",
                        " was",
                        " to"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "\u010aAldrin was a drug user before, Castillo admitted, but he had",
                    "tokens": [
                        "\u010a",
                        "A",
                        "ld",
                        "rin",
                        " was",
                        " a",
                        " drug",
                        " user",
                        " before",
                        ",",
                        " Cast",
                        "illo",
                        " admitted",
                        ",",
                        " but",
                        " he",
                        " had"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " who genuinely seem not to believe reality. Or Democrats, cowed into silence on issues",
                    "tokens": [
                        " who",
                        " genuinely",
                        " seem",
                        " not",
                        " to",
                        " believe",
                        " reality",
                        ".",
                        " Or",
                        " Democrats",
                        ",",
                        " c",
                        "owed",
                        " into",
                        " silence",
                        " on",
                        " issues"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " the incident, saying the man threatened him before leaving.\u010a\u010a\u00e2\u0122\u013eI think",
                    "tokens": [
                        " the",
                        " incident",
                        ",",
                        " saying",
                        " the",
                        " man",
                        " threatened",
                        " him",
                        " before",
                        " leaving",
                        ".",
                        "\u010a",
                        "\u010a",
                        "\u00e2\u0122",
                        "\u013e",
                        "I",
                        " think"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 47.63297653198242
        },
        {
            "feature_index": 787,
            "gpt_predictions": [
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "time-related terms",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " enough to<|endoftext|>It\u00e2\u0122\u013bs that time of year again when nationalists debate<|endoftext|> one",
                    "max_token": " time",
                    "tokens": [
                        " enough",
                        " to",
                        "<|endoftext|>",
                        "It",
                        "\u00e2\u0122",
                        "\u013b",
                        "s",
                        " that",
                        " time",
                        " of",
                        " year",
                        " again",
                        " when",
                        " nationalists",
                        " debate",
                        "<|endoftext|>",
                        " one"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        5.810299873352051,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " was the free<|endoftext|> would have suspected this time last year.\u010a\u010aThe Ohio State",
                    "max_token": " time",
                    "tokens": [
                        " was",
                        " the",
                        " free",
                        "<|endoftext|>",
                        " would",
                        " have",
                        " suspected",
                        " this",
                        " time",
                        " last",
                        " year",
                        ".",
                        "\u010a",
                        "\u010a",
                        "The",
                        " Ohio",
                        " State"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        7.779041290283203,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "m to<|endoftext|> off the world-leading time over 10,000m this year,",
                    "max_token": " time",
                    "tokens": [
                        "m",
                        " to",
                        "<|endoftext|>",
                        " off",
                        " the",
                        " world",
                        "-",
                        "leading",
                        " time",
                        " over",
                        " 10",
                        ",",
                        "000",
                        "m",
                        " this",
                        " year",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        6.499776840209961,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " of equality. I tweeted back:<|endoftext|>Time finally ran out for Host<|endoftext|> whenever I",
                    "max_token": "Time",
                    "tokens": [
                        " of",
                        " equality",
                        ".",
                        " I",
                        " tweeted",
                        " back",
                        ":",
                        "<|endoftext|>",
                        "Time",
                        " finally",
                        " ran",
                        " out",
                        " for",
                        " Host",
                        "<|endoftext|>",
                        " whenever",
                        " I"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        9.10753059387207,
                        2.066244602203369,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "1280x1024<|endoftext|>Happy Monday! Time for another travel post \u00f0\u0141\u013b\u0124 Out of all",
                    "max_token": " Time",
                    "tokens": [
                        "12",
                        "80",
                        "x",
                        "1024",
                        "<|endoftext|>",
                        "Happy",
                        " Monday",
                        "!",
                        " Time",
                        " for",
                        " another",
                        " travel",
                        " post",
                        " \u00f0\u0141\u013b\u0124",
                        " Out",
                        " of",
                        " all"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        7.305400371551514,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " coat.\u010a\u010aDetective Inspector Steve Christian, from Liverpool CID, said:",
                    "tokens": [
                        " coat",
                        ".",
                        "\u010a",
                        "\u010a",
                        "Det",
                        "ective",
                        " Inspector",
                        " Steve",
                        " Christian",
                        ",",
                        " from",
                        " Liverpool",
                        " C",
                        "ID",
                        ",",
                        " said",
                        ":"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " drop out of the presidential race, according to NBC News ' Andrea Mitchell.\u010a\u010a",
                    "tokens": [
                        " drop",
                        " out",
                        " of",
                        " the",
                        " presidential",
                        " race",
                        ",",
                        " according",
                        " to",
                        " NBC",
                        " News",
                        " '",
                        " Andrea",
                        " Mitchell",
                        ".",
                        "\u010a",
                        "\u010a"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " documenting the protective effect of male circumcision on HIV infection in young adults pose significant challenges to",
                    "tokens": [
                        " documenting",
                        " the",
                        " protective",
                        " effect",
                        " of",
                        " male",
                        " circumcision",
                        " on",
                        " HIV",
                        " infection",
                        " in",
                        " young",
                        " adults",
                        " pose",
                        " significant",
                        " challenges",
                        " to"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " the virus is no laughing matter, a viral video features a Chicago pup named Herbert who",
                    "tokens": [
                        " the",
                        " virus",
                        " is",
                        " no",
                        " laughing",
                        " matter",
                        ",",
                        " a",
                        " viral",
                        " video",
                        " features",
                        " a",
                        " Chicago",
                        " pup",
                        " named",
                        " Herbert",
                        " who"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " faces with respect to women.\u010a\u010aA CNN poll released Monday showed Obama moving into",
                    "tokens": [
                        " faces",
                        " with",
                        " respect",
                        " to",
                        " women",
                        ".",
                        "\u010a",
                        "\u010a",
                        "A",
                        " CNN",
                        " poll",
                        " released",
                        " Monday",
                        " showed",
                        " Obama",
                        " moving",
                        " into"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 20.34969520568848
        },
        {
            "feature_index": 436,
            "gpt_predictions": [
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "references to the brand \"Nestl\u00e9.\"",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " first residential care hospice will soon be nestled along a tree-lined street and",
                    "max_token": " nest",
                    "tokens": [
                        " first",
                        " residential",
                        " care",
                        " hosp",
                        "ice",
                        " will",
                        " soon",
                        " be",
                        " nest",
                        "led",
                        " along",
                        " a",
                        " tree",
                        "-",
                        "lined",
                        " street",
                        " and"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        33.15355682373047,
                        5.161136627197266,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " that never really goes well.<|endoftext|>Nestl\u00c3\u00a9 took its immensely popular instant-n",
                    "max_token": "est",
                    "tokens": [
                        " that",
                        " never",
                        " really",
                        " goes",
                        " well",
                        ".",
                        "<|endoftext|>",
                        "N",
                        "est",
                        "l\u00c3\u00a9",
                        " took",
                        " its",
                        " immensely",
                        " popular",
                        " instant",
                        "-",
                        "n"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        31.64861869812012,
                        1.941535353660583,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "ner and her late husband, former president Nestor Kirchner, who had a",
                    "max_token": " Nest",
                    "tokens": [
                        "ner",
                        " and",
                        " her",
                        " late",
                        " husband",
                        ",",
                        " former",
                        " president",
                        " Nest",
                        "or",
                        " Kir",
                        "ch",
                        "ner",
                        ",",
                        " who",
                        " had",
                        " a"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        53.68062973022461,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " was very poor performance for anything that required nested queries, alleviate frustrations of having to worry",
                    "max_token": " nested",
                    "tokens": [
                        " was",
                        " very",
                        " poor",
                        " performance",
                        " for",
                        " anything",
                        " that",
                        " required",
                        " nested",
                        " queries",
                        ",",
                        " alleviate",
                        " frustrations",
                        " of",
                        " having",
                        " to",
                        " worry"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        14.92768669128418,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " chemistry between William Shatner and Leonard Nimoy gave the original series the edge to",
                    "max_token": " Nim",
                    "tokens": [
                        " chemistry",
                        " between",
                        " William",
                        " Sh",
                        "at",
                        "ner",
                        " and",
                        " Leonard",
                        " Nim",
                        "oy",
                        " gave",
                        " the",
                        " original",
                        " series",
                        " the",
                        " edge",
                        " to"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        8.27782917022705,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "\u00e2\u0122\u013bThe<|endoftext|> potential impact surveys and mitigation and restoration plans\u00e2\u0122\u013b\u00e2\u0122\u013b are",
                    "tokens": [
                        "\u00e2\u0122",
                        "\u013b",
                        "The",
                        "<|endoftext|>",
                        " potential",
                        " impact",
                        " surveys",
                        " and",
                        " mitigation",
                        " and",
                        " restoration",
                        " plans",
                        "\u00e2\u0122",
                        "\u013b",
                        "\u00e2\u0122",
                        "\u013b",
                        " are"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " that kid who felt unwanted, or the grown up who remembers how hard it was to",
                    "tokens": [
                        " that",
                        " kid",
                        " who",
                        " felt",
                        " unwanted",
                        ",",
                        " or",
                        " the",
                        " grown",
                        " up",
                        " who",
                        " remembers",
                        " how",
                        " hard",
                        " it",
                        " was",
                        " to"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " Trump and Hillary in the first debate, before either said a word, Trump made what",
                    "tokens": [
                        " Trump",
                        " and",
                        " Hillary",
                        " in",
                        " the",
                        " first",
                        " debate",
                        ",",
                        " before",
                        " either",
                        " said",
                        " a",
                        " word",
                        ",",
                        " Trump",
                        " made",
                        " what"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "3 billion) installing everything from traffic-management technologies to smart electricity grids.\u010a\u010a",
                    "tokens": [
                        "3",
                        " billion",
                        ")",
                        " installing",
                        " everything",
                        " from",
                        " traffic",
                        "-",
                        "management",
                        " technologies",
                        " to",
                        " smart",
                        " electricity",
                        " grids",
                        ".",
                        "\u010a",
                        "\u010a"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " city staff (Fire, Police, Crime Prevention, and more)\u010a\u010aOption to",
                    "tokens": [
                        " city",
                        " staff",
                        " (",
                        "Fire",
                        ",",
                        " Police",
                        ",",
                        " Crime",
                        " Prevention",
                        ",",
                        " and",
                        " more",
                        ")",
                        "\u010a",
                        "\u010a",
                        "Option",
                        " to"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 56.47019577026367
        },
        {
            "feature_index": 764,
            "gpt_predictions": [
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "references related to data storage & technology, particularly mentioning SD cards and software development kits (SDKs)",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 6,
                    "sentence_string": ".\u010a\u010aWhat is this DDasdf thing?\u010a\u010aDDoS stands for",
                    "max_token": " DD",
                    "tokens": [
                        ".",
                        "\u010a",
                        "\u010a",
                        "What",
                        " is",
                        " this",
                        " DD",
                        "as",
                        "df",
                        " thing",
                        "?",
                        "\u010a",
                        "\u010a",
                        "D",
                        "DoS",
                        " stands",
                        " for"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        13.29877758026123,
                        0,
                        11.52545356750488,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": ", the story is similar. The earliest SD available is for the 1978 admin of the",
                    "max_token": " SD",
                    "tokens": [
                        ",",
                        " the",
                        " story",
                        " is",
                        " similar",
                        ".",
                        " The",
                        " earliest",
                        " SD",
                        " available",
                        " is",
                        " for",
                        " the",
                        " 1978",
                        " admin",
                        " of",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        29.35876083374023,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "->w); // Create scaled image surface SDL_Surface *newImage = SDL",
                    "max_token": " SDL",
                    "tokens": [
                        "->",
                        "w",
                        ");",
                        " //",
                        " Create",
                        " scaled",
                        " image",
                        " surface",
                        " SDL",
                        "_",
                        "Sur",
                        "face",
                        " *",
                        "new",
                        "Image",
                        " =",
                        " SDL"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        8.423745155334473,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        6.816952705383301
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "\u010a5. Simple DirectMedia Layer (SDL)\u010a\u010aWe\u00e2\u0122\u013bre",
                    "max_token": "SD",
                    "tokens": [
                        "\u010a",
                        "5",
                        ".",
                        " Simple",
                        " Direct",
                        "Media",
                        " Layer",
                        " (",
                        "SD",
                        "L",
                        ")",
                        "\u010a",
                        "\u010a",
                        "We",
                        "\u00e2\u0122",
                        "\u013b",
                        "re"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        34.81535720825195,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " It has achieved that objective thanks to the SD-10/A active radar-guided",
                    "max_token": " SD",
                    "tokens": [
                        " It",
                        " has",
                        " achieved",
                        " that",
                        " objective",
                        " thanks",
                        " to",
                        " the",
                        " SD",
                        "-",
                        "10",
                        "/",
                        "A",
                        " active",
                        " radar",
                        "-",
                        "guided"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        33.40946960449219,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " coat.\u010a\u010aDetective Inspector Steve Christian, from Liverpool CID, said:",
                    "tokens": [
                        " coat",
                        ".",
                        "\u010a",
                        "\u010a",
                        "Det",
                        "ective",
                        " Inspector",
                        " Steve",
                        " Christian",
                        ",",
                        " from",
                        " Liverpool",
                        " C",
                        "ID",
                        ",",
                        " said",
                        ":"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " drop out of the presidential race, according to NBC News ' Andrea Mitchell.\u010a\u010a",
                    "tokens": [
                        " drop",
                        " out",
                        " of",
                        " the",
                        " presidential",
                        " race",
                        ",",
                        " according",
                        " to",
                        " NBC",
                        " News",
                        " '",
                        " Andrea",
                        " Mitchell",
                        ".",
                        "\u010a",
                        "\u010a"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " documenting the protective effect of male circumcision on HIV infection in young adults pose significant challenges to",
                    "tokens": [
                        " documenting",
                        " the",
                        " protective",
                        " effect",
                        " of",
                        " male",
                        " circumcision",
                        " on",
                        " HIV",
                        " infection",
                        " in",
                        " young",
                        " adults",
                        " pose",
                        " significant",
                        " challenges",
                        " to"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " the virus is no laughing matter, a viral video features a Chicago pup named Herbert who",
                    "tokens": [
                        " the",
                        " virus",
                        " is",
                        " no",
                        " laughing",
                        " matter",
                        ",",
                        " a",
                        " viral",
                        " video",
                        " features",
                        " a",
                        " Chicago",
                        " pup",
                        " named",
                        " Herbert",
                        " who"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " faces with respect to women.\u010a\u010aA CNN poll released Monday showed Obama moving into",
                    "tokens": [
                        " faces",
                        " with",
                        " respect",
                        " to",
                        " women",
                        ".",
                        "\u010a",
                        "\u010a",
                        "A",
                        " CNN",
                        " poll",
                        " released",
                        " Monday",
                        " showed",
                        " Obama",
                        " moving",
                        " into"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 38.16885757446289
        },
        {
            "feature_index": 88,
            "gpt_predictions": [
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "phrases related to education and training programs",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " with measures like ventilation, intubation and surfactant to improve the functioning of",
                    "max_token": " and",
                    "tokens": [
                        " with",
                        " measures",
                        " like",
                        " ventilation",
                        ",",
                        " int",
                        "ub",
                        "ation",
                        " and",
                        " surf",
                        "act",
                        "ant",
                        " to",
                        " improve",
                        " the",
                        " functioning",
                        " of"
                    ],
                    "values": [
                        2.523407936096191,
                        3.337230920791626,
                        5.724314212799072,
                        0,
                        4.465150356292725,
                        0,
                        0,
                        2.70344352722168,
                        6.666768550872803,
                        0,
                        0,
                        0,
                        1.092283248901367,
                        0.5311574935913086,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": ".\u010a\u010aSinging traditional lullabies and nursery rhymes to babies and infants before",
                    "max_token": " and",
                    "tokens": [
                        ".",
                        "\u010a",
                        "\u010a",
                        "S",
                        "inging",
                        " traditional",
                        " lull",
                        "abies",
                        " and",
                        " nursery",
                        " rh",
                        "ymes",
                        " to",
                        " babies",
                        " and",
                        " infants",
                        " before"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        2.740765571594238,
                        1.759171605110168,
                        4.416098117828369,
                        6.061013221740723,
                        7.209429740905762,
                        3.184505224227905,
                        1.645745635032654,
                        5.715893268585205,
                        5.124078750610352,
                        0,
                        0,
                        0,
                        1.028527975082397
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " sexually transmitted diseases. Well-rounded education that not only teaches about straight sex, but",
                    "max_token": " that",
                    "tokens": [
                        " sexually",
                        " transmitted",
                        " diseases",
                        ".",
                        " Well",
                        "-",
                        "rounded",
                        " education",
                        " that",
                        " not",
                        " only",
                        " teaches",
                        " about",
                        " straight",
                        " sex",
                        ",",
                        " but"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0.06451420485973358,
                        1.176946997642517,
                        3.094387531280518,
                        7.584897041320801,
                        3.686207056045532,
                        3.54562520980835,
                        5.80393123626709,
                        2.696685075759888,
                        0,
                        0.570783257484436,
                        3.49423885345459,
                        3.196985721588135
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " be the stress relief that a regular exercise program provides.\u010a\u010aThe elderly, on",
                    "max_token": " program",
                    "tokens": [
                        " be",
                        " the",
                        " stress",
                        " relief",
                        " that",
                        " a",
                        " regular",
                        " exercise",
                        " program",
                        " provides",
                        ".",
                        "\u010a",
                        "\u010a",
                        "The",
                        " elderly",
                        ",",
                        " on"
                    ],
                    "values": [
                        0,
                        0,
                        0.1909778416156769,
                        1.216800093650818,
                        2.37602686882019,
                        0.1726172864437103,
                        0,
                        0.6799822449684143,
                        8.191818237304688,
                        3.113192319869995,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 13,
                    "sentence_string": " podcast includes a 5-minute warm-up walk and cool-down walk to ease",
                    "max_token": "down",
                    "tokens": [
                        " podcast",
                        " includes",
                        " a",
                        " 5",
                        "-",
                        "minute",
                        " warm",
                        "-",
                        "up",
                        " walk",
                        " and",
                        " cool",
                        "-",
                        "down",
                        " walk",
                        " to",
                        " ease"
                    ],
                    "values": [
                        0,
                        1.987376809120178,
                        1.176548957824707,
                        0.4808248281478882,
                        0,
                        4.714185237884521,
                        2.742748260498047,
                        0,
                        6.818882942199707,
                        4.204228401184082,
                        7.08519172668457,
                        4.8522629737854,
                        1.366103649139404,
                        10.46969223022461,
                        5.2100830078125,
                        4.425583362579346,
                        2.442990303039551
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "ESPN, UniMas, UDN) before facing the 2014 FIFA World Cup<|endoftext|> can",
                    "tokens": [
                        "ESPN",
                        ",",
                        " Uni",
                        "Mas",
                        ",",
                        " U",
                        "DN",
                        ")",
                        " before",
                        " facing",
                        " the",
                        " 2014",
                        " FIFA",
                        " World",
                        " Cup",
                        "<|endoftext|>",
                        " can"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " vaccine industry, with the assistance of a sold-out government, is forcing vaccinations on",
                    "tokens": [
                        " vaccine",
                        " industry",
                        ",",
                        " with",
                        " the",
                        " assistance",
                        " of",
                        " a",
                        " sold",
                        "-",
                        "out",
                        " government",
                        ",",
                        " is",
                        " forcing",
                        " vaccinations",
                        " on"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": ", build and maintain effective student groups, advertise and rebrand conservative values, engage in",
                    "tokens": [
                        ",",
                        " build",
                        " and",
                        " maintain",
                        " effective",
                        " student",
                        " groups",
                        ",",
                        " advertise",
                        " and",
                        " re",
                        "brand",
                        " conservative",
                        " values",
                        ",",
                        " engage",
                        " in"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " political protest\u010a\u010aUpdated\u010a\u010aIt was one of the great art heists of",
                    "tokens": [
                        " political",
                        " protest",
                        "\u010a",
                        "\u010a",
                        "Updated",
                        "\u010a",
                        "\u010a",
                        "It",
                        " was",
                        " one",
                        " of",
                        " the",
                        " great",
                        " art",
                        " he",
                        "ists",
                        " of"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " that kid who felt unwanted, or the grown up who remembers how hard it was to",
                    "tokens": [
                        " that",
                        " kid",
                        " who",
                        " felt",
                        " unwanted",
                        ",",
                        " or",
                        " the",
                        " grown",
                        " up",
                        " who",
                        " remembers",
                        " how",
                        " hard",
                        " it",
                        " was",
                        " to"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 10.48395252227783
        },
        {
            "feature_index": 63,
            "gpt_predictions": [
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    1.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "instances of rules and regulations being violated",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " Hildebrandt. Airport security was decidedly lack back then compared to today. Somehow",
                    "max_token": " decidedly",
                    "tokens": [
                        " H",
                        "ilde",
                        "brand",
                        "t",
                        ".",
                        " Airport",
                        " security",
                        " was",
                        " decidedly",
                        " lack",
                        " back",
                        " then",
                        " compared",
                        " to",
                        " today",
                        ".",
                        " Somehow"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        4.912020683288574,
                        9.491907119750977,
                        4.477002143859863,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " shopping, I\u00e2\u0122\u013bm really careful and scared. If I\u00e2\u0122\u013bm standing",
                    "max_token": " and",
                    "tokens": [
                        " shopping",
                        ",",
                        " I",
                        "\u00e2\u0122",
                        "\u013b",
                        "m",
                        " really",
                        " careful",
                        " and",
                        " scared",
                        ".",
                        " If",
                        " I",
                        "\u00e2\u0122",
                        "\u013b",
                        "m",
                        " standing"
                    ],
                    "values": [
                        0,
                        2.383653163909912,
                        0.4240103662014008,
                        0,
                        0,
                        1.556898593902588,
                        1.351753234863281,
                        9.14404582977295,
                        9.90489387512207,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        1.060007929801941
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " a concern. The public is advised to be vigilant and avoid falling victims to scams.",
                    "max_token": " be",
                    "tokens": [
                        " a",
                        " concern",
                        ".",
                        " The",
                        " public",
                        " is",
                        " advised",
                        " to",
                        " be",
                        " vigilant",
                        " and",
                        " avoid",
                        " falling",
                        " victims",
                        " to",
                        " scams",
                        "."
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        2.244056940078735,
                        10.50044727325439,
                        3.018570899963379,
                        2.811033487319946,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 2,
                    "sentence_string": " act polite, follow the traffic laws, and refrain from throttling the a-hole",
                    "max_token": ",",
                    "tokens": [
                        " act",
                        " polite",
                        ",",
                        " follow",
                        " the",
                        " traffic",
                        " laws",
                        ",",
                        " and",
                        " refrain",
                        " from",
                        " thrott",
                        "ling",
                        " the",
                        " a",
                        "-",
                        "hole"
                    ],
                    "values": [
                        4.689865589141846,
                        4.13724422454834,
                        10.94409275054932,
                        8.527921676635742,
                        2.199553728103638,
                        2.78241753578186,
                        9.027130126953125,
                        8.486241340637207,
                        8.654651641845703,
                        2.17919397354126,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " rigorous food safety program, Jack's maintains strict food handling policies and procedures that<|endoftext|>),",
                    "max_token": " strict",
                    "tokens": [
                        " rigorous",
                        " food",
                        " safety",
                        " program",
                        ",",
                        " Jack",
                        "'s",
                        " maintains",
                        " strict",
                        " food",
                        " handling",
                        " policies",
                        " and",
                        " procedures",
                        " that",
                        "<|endoftext|>",
                        "),"
                    ],
                    "values": [
                        5.852888107299805,
                        5.919856071472168,
                        6.776082515716553,
                        1.319103479385376,
                        1.647796154022217,
                        0,
                        0,
                        8.60688591003418,
                        11.68833160400391,
                        7.777316093444824,
                        9.728670120239258,
                        9.351495742797852,
                        8.524646759033203,
                        9.700308799743652,
                        8.368592262268066,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "ESPN, UniMas, UDN) before facing the 2014 FIFA World Cup<|endoftext|> can",
                    "tokens": [
                        "ESPN",
                        ",",
                        " Uni",
                        "Mas",
                        ",",
                        " U",
                        "DN",
                        ")",
                        " before",
                        " facing",
                        " the",
                        " 2014",
                        " FIFA",
                        " World",
                        " Cup",
                        "<|endoftext|>",
                        " can"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " vaccine industry, with the assistance of a sold-out government, is forcing vaccinations on",
                    "tokens": [
                        " vaccine",
                        " industry",
                        ",",
                        " with",
                        " the",
                        " assistance",
                        " of",
                        " a",
                        " sold",
                        "-",
                        "out",
                        " government",
                        ",",
                        " is",
                        " forcing",
                        " vaccinations",
                        " on"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": ", build and maintain effective student groups, advertise and rebrand conservative values, engage in",
                    "tokens": [
                        ",",
                        " build",
                        " and",
                        " maintain",
                        " effective",
                        " student",
                        " groups",
                        ",",
                        " advertise",
                        " and",
                        " re",
                        "brand",
                        " conservative",
                        " values",
                        ",",
                        " engage",
                        " in"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " political protest\u010a\u010aUpdated\u010a\u010aIt was one of the great art heists of",
                    "tokens": [
                        " political",
                        " protest",
                        "\u010a",
                        "\u010a",
                        "Updated",
                        "\u010a",
                        "\u010a",
                        "It",
                        " was",
                        " one",
                        " of",
                        " the",
                        " great",
                        " art",
                        " he",
                        "ists",
                        " of"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " that kid who felt unwanted, or the grown up who remembers how hard it was to",
                    "tokens": [
                        " that",
                        " kid",
                        " who",
                        " felt",
                        " unwanted",
                        ",",
                        " or",
                        " the",
                        " grown",
                        " up",
                        " who",
                        " remembers",
                        " how",
                        " hard",
                        " it",
                        " was",
                        " to"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 15.53856086730957
        },
        {
            "feature_index": 826,
            "gpt_predictions": [
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    1.0
                ],
                [
                    0,
                    1.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "verbs related to actions, decisions, and intentions",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 7,
                    "sentence_string": " in prison MORE said he'd be willing to force a government shutdown to defund Planned Parenthood",
                    "max_token": " willing",
                    "tokens": [
                        " in",
                        " prison",
                        " MORE",
                        " said",
                        " he",
                        "'d",
                        " be",
                        " willing",
                        " to",
                        " force",
                        " a",
                        " government",
                        " shutdown",
                        " to",
                        " defund",
                        " Planned",
                        " Parenthood"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        1.264281153678894,
                        2.835613965988159,
                        7.706480026245117,
                        5.387870788574219,
                        4.608469009399414,
                        0.9978033900260925,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " by the McLaughlin family, said he expected strong interest in the assets.\u010a\u010a",
                    "max_token": " expected",
                    "tokens": [
                        " by",
                        " the",
                        " McL",
                        "aughlin",
                        " family",
                        ",",
                        " said",
                        " he",
                        " expected",
                        " strong",
                        " interest",
                        " in",
                        " the",
                        " assets",
                        ".",
                        "\u010a",
                        "\u010a"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        12.49336814880371,
                        0,
                        0.9032230973243713,
                        0,
                        0.2334824651479721,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " institutions have told me that they are not including rape law in their courses, arguing that",
                    "max_token": " including",
                    "tokens": [
                        " institutions",
                        " have",
                        " told",
                        " me",
                        " that",
                        " they",
                        " are",
                        " not",
                        " including",
                        " rape",
                        " law",
                        " in",
                        " their",
                        " courses",
                        ",",
                        " arguing",
                        " that"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        4.408794403076172,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "\u010a\u010aDe Villiers said he had sought to maintain a French defence force able to",
                    "max_token": " sought",
                    "tokens": [
                        "\u010a",
                        "\u010a",
                        "De",
                        " Vill",
                        "iers",
                        " said",
                        " he",
                        " had",
                        " sought",
                        " to",
                        " maintain",
                        " a",
                        " French",
                        " defence",
                        " force",
                        " able",
                        " to"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        1.535750269889832,
                        4.938364028930664,
                        13.47183609008789,
                        8.825664520263672,
                        6.306025981903076,
                        2.890681743621826,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " Russia. Stoltenberg said he expected a summer NATO summit in Warsaw would be",
                    "max_token": " expected",
                    "tokens": [
                        " Russia",
                        ".",
                        " St",
                        "ol",
                        "ten",
                        "berg",
                        " said",
                        " he",
                        " expected",
                        " a",
                        " summer",
                        " NATO",
                        " summit",
                        " in",
                        " Warsaw",
                        " would",
                        " be"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0.5199124217033386,
                        12.96672439575195,
                        3.29511570930481,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " coat.\u010a\u010aDetective Inspector Steve Christian, from Liverpool CID, said:",
                    "tokens": [
                        " coat",
                        ".",
                        "\u010a",
                        "\u010a",
                        "Det",
                        "ective",
                        " Inspector",
                        " Steve",
                        " Christian",
                        ",",
                        " from",
                        " Liverpool",
                        " C",
                        "ID",
                        ",",
                        " said",
                        ":"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " drop out of the presidential race, according to NBC News ' Andrea Mitchell.\u010a\u010a",
                    "tokens": [
                        " drop",
                        " out",
                        " of",
                        " the",
                        " presidential",
                        " race",
                        ",",
                        " according",
                        " to",
                        " NBC",
                        " News",
                        " '",
                        " Andrea",
                        " Mitchell",
                        ".",
                        "\u010a",
                        "\u010a"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " Rumors that Amazon will soon begin accepting Bitcoin are persistent. If the retail giant does",
                    "tokens": [
                        " Rum",
                        "ors",
                        " that",
                        " Amazon",
                        " will",
                        " soon",
                        " begin",
                        " accepting",
                        " Bitcoin",
                        " are",
                        " persistent",
                        ".",
                        " If",
                        " the",
                        " retail",
                        " giant",
                        " does"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": ", and banking flows was a monthly net TIC (Treasury International Capital)<|endoftext|>",
                    "tokens": [
                        ",",
                        " and",
                        " banking",
                        " flows",
                        " was",
                        " a",
                        " monthly",
                        " net",
                        " T",
                        "IC",
                        " (",
                        "Tre",
                        "asury",
                        " International",
                        " Capital",
                        ")",
                        "<|endoftext|>"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " faces with respect to women.\u010a\u010aA CNN poll released Monday showed Obama moving into",
                    "tokens": [
                        " faces",
                        " with",
                        " respect",
                        " to",
                        " women",
                        ".",
                        "\u010a",
                        "\u010a",
                        "A",
                        " CNN",
                        " poll",
                        " released",
                        " Monday",
                        " showed",
                        " Obama",
                        " moving",
                        " into"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 14.90109157562256
        }
    ],
    "timestamp": 1715383683.376679
}