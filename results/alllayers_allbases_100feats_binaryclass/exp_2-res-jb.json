{
    "hyperparameters": {
        "num_features": 100,
        "layer": 2,
        "basis": "res-jb",
        "test_pos": 5,
        "test_neg": 5,
        "show_pos": 0,
        "show_neg": 0,
        "binary_class": true,
        "neg_type": "others",
        "show_max_token": false,
        "num_completions": 1,
        "debug": false,
        "randomize_pos": true,
        "seed": 42
    },
    "num_features": 100,
    "results": [
        {
            "feature_index": 15309,
            "gpt_predictions": [
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "proper names, particularly the name \"Ernest\", as well as names like \"Bedford\" and \"Buchanan\"",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " fired thousands of workers nationally, said Natasha Buchanan, an attorney in Santa Ana, Calif",
                    "max_token": " Buchanan",
                    "tokens": [
                        " fired",
                        " thousands",
                        " of",
                        " workers",
                        " nationally",
                        ",",
                        " said",
                        " Natasha",
                        " Buchanan",
                        ",",
                        " an",
                        " attorney",
                        " in",
                        " Santa",
                        " Ana",
                        ",",
                        " Calif"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        7.417911052703857,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " Sen. Chuck Grassley Charles (Chuck) Ernest GrassleyOvernight Health Care: Drug exec",
                    "max_token": " Ernest",
                    "tokens": [
                        " Sen",
                        ".",
                        " Chuck",
                        " Grassley",
                        " Charles",
                        " (",
                        "Chuck",
                        ")",
                        " Ernest",
                        " Grassley",
                        "O",
                        "vernight",
                        " Health",
                        " Care",
                        ":",
                        " Drug",
                        " exec"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        42.95276641845703,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " \u00e2\u0122\u013edisappointed and saddened by Ms Bedford\u00e2\u0122\u013bs decision to resign from the",
                    "max_token": " Bedford",
                    "tokens": [
                        " \u00e2\u0122",
                        "\u013e",
                        "dis",
                        "appointed",
                        " and",
                        " saddened",
                        " by",
                        " Ms",
                        " Bedford",
                        "\u00e2\u0122",
                        "\u013b",
                        "s",
                        " decision",
                        " to",
                        " resign",
                        " from",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        7.289312839508057,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " a viral video features a Chicago pup named Herbert who's putting on a brave face while",
                    "max_token": " Herbert",
                    "tokens": [
                        " a",
                        " viral",
                        " video",
                        " features",
                        " a",
                        " Chicago",
                        " pup",
                        " named",
                        " Herbert",
                        " who",
                        "'s",
                        " putting",
                        " on",
                        " a",
                        " brave",
                        " face",
                        " while"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        4.641164779663086,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " next Member for Florey,\u00e2\u0122\u013f Bedford said in a statement sent to InDaily",
                    "max_token": " Bedford",
                    "tokens": [
                        " next",
                        " Member",
                        " for",
                        " Flore",
                        "y",
                        ",",
                        "\u00e2\u0122",
                        "\u013f",
                        " Bedford",
                        " said",
                        " in",
                        " a",
                        " statement",
                        " sent",
                        " to",
                        " In",
                        "Daily"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        7.476082801818848,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " most rich countries. But \u00e2\u0122\u013epremiumisation\u00e2\u0122\u013f is working. Though still",
                    "tokens": [
                        " most",
                        " rich",
                        " countries",
                        ".",
                        " But",
                        " \u00e2\u0122",
                        "\u013e",
                        "prem",
                        "ium",
                        "isation",
                        "\u00e2\u0122",
                        "\u013f",
                        " is",
                        " working",
                        ".",
                        " Though",
                        " still"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " outfit in the top 20. The Denver Nuggets reportedly spend \u00c2\u00a32,939,",
                    "tokens": [
                        " outfit",
                        " in",
                        " the",
                        " top",
                        " 20",
                        ".",
                        " The",
                        " Denver",
                        " Nuggets",
                        " reportedly",
                        " spend",
                        " \u00c2\u00a3",
                        "2",
                        ",",
                        "9",
                        "39",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "if Al-Arab al-Qathafi, where three of Colonel Gaddafi's grandchildren",
                    "tokens": [
                        "if",
                        " Al",
                        "-",
                        "Arab",
                        " al",
                        "-",
                        "Q",
                        "ath",
                        "afi",
                        ",",
                        " where",
                        " three",
                        " of",
                        " Colonel",
                        " Gaddafi",
                        "'s",
                        " grandchildren"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "2 14 Lubbock +4 20 Denver +2 23 Portland +2 7 Birmingham",
                    "tokens": [
                        "2",
                        " 14",
                        " L",
                        "ubb",
                        "ock",
                        " +",
                        "4",
                        " 20",
                        " Denver",
                        " +",
                        "2",
                        " 23",
                        " Portland",
                        " +",
                        "2",
                        " 7",
                        " Birmingham"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " has to be levied, it should be levied on the Central and state governments and the",
                    "tokens": [
                        " has",
                        " to",
                        " be",
                        " levied",
                        ",",
                        " it",
                        " should",
                        " be",
                        " levied",
                        " on",
                        " the",
                        " Central",
                        " and",
                        " state",
                        " governments",
                        " and",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 45.6166877746582
        },
        {
            "feature_index": 17041,
            "gpt_predictions": [
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "phrases related to sales and commercial activities",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "NN:\u010a\u010a'Kittendales': Shirtless men do their part to",
                    "max_token": "ales",
                    "tokens": [
                        "NN",
                        ":",
                        "\u010a",
                        "\u010a",
                        "'",
                        "K",
                        "itt",
                        "end",
                        "ales",
                        "':",
                        " Shirt",
                        "less",
                        " men",
                        " do",
                        " their",
                        " part",
                        " to"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        26.03537368774414,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " Aug 23, 2017\u010a\u010aThe crowdsales of the next four Cofound.",
                    "max_token": "ales",
                    "tokens": [
                        " Aug",
                        " 23",
                        ",",
                        " 2017",
                        "\u010a",
                        "\u010a",
                        "The",
                        " crowds",
                        "ales",
                        " of",
                        " the",
                        " next",
                        " four",
                        " C",
                        "of",
                        "ound",
                        "."
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        27.38091659545898,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " the Dalai Lama.\u010a\u010aThe Nepalese government has said it cannot permit demonstrations",
                    "max_token": "ales",
                    "tokens": [
                        " the",
                        " Dalai",
                        " Lama",
                        ".",
                        "\u010a",
                        "\u010a",
                        "The",
                        " Nep",
                        "ales",
                        "e",
                        " government",
                        " has",
                        " said",
                        " it",
                        " cannot",
                        " permit",
                        " demonstrations"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        28.87565231323242,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "<|endoftext|> andanada de ataques verbales con un coro de gritos.",
                    "max_token": "ales",
                    "tokens": [
                        "<|endoftext|>",
                        " and",
                        "an",
                        "ada",
                        " de",
                        " at",
                        "aques",
                        " verb",
                        "ales",
                        " con",
                        " un",
                        " cor",
                        "o",
                        " de",
                        " grit",
                        "os",
                        "."
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        30.03435134887695,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " Mujica\u010a\u010aHis salary pales in comparison to other world leaders. Ear",
                    "max_token": "ales",
                    "tokens": [
                        " Mu",
                        "j",
                        "ica",
                        "\u010a",
                        "\u010a",
                        "His",
                        " salary",
                        " p",
                        "ales",
                        " in",
                        " comparison",
                        " to",
                        " other",
                        " world",
                        " leaders",
                        ".",
                        " Ear"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        30.77561187744141,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " most rich countries. But \u00e2\u0122\u013epremiumisation\u00e2\u0122\u013f is working. Though still",
                    "tokens": [
                        " most",
                        " rich",
                        " countries",
                        ".",
                        " But",
                        " \u00e2\u0122",
                        "\u013e",
                        "prem",
                        "ium",
                        "isation",
                        "\u00e2\u0122",
                        "\u013f",
                        " is",
                        " working",
                        ".",
                        " Though",
                        " still"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " outfit in the top 20. The Denver Nuggets reportedly spend \u00c2\u00a32,939,",
                    "tokens": [
                        " outfit",
                        " in",
                        " the",
                        " top",
                        " 20",
                        ".",
                        " The",
                        " Denver",
                        " Nuggets",
                        " reportedly",
                        " spend",
                        " \u00c2\u00a3",
                        "2",
                        ",",
                        "9",
                        "39",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "if Al-Arab al-Qathafi, where three of Colonel Gaddafi's grandchildren",
                    "tokens": [
                        "if",
                        " Al",
                        "-",
                        "Arab",
                        " al",
                        "-",
                        "Q",
                        "ath",
                        "afi",
                        ",",
                        " where",
                        " three",
                        " of",
                        " Colonel",
                        " Gaddafi",
                        "'s",
                        " grandchildren"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "2 14 Lubbock +4 20 Denver +2 23 Portland +2 7 Birmingham",
                    "tokens": [
                        "2",
                        " 14",
                        " L",
                        "ubb",
                        "ock",
                        " +",
                        "4",
                        " 20",
                        " Denver",
                        " +",
                        "2",
                        " 23",
                        " Portland",
                        " +",
                        "2",
                        " 7",
                        " Birmingham"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " has to be levied, it should be levied on the Central and state governments and the",
                    "tokens": [
                        " has",
                        " to",
                        " be",
                        " levied",
                        ",",
                        " it",
                        " should",
                        " be",
                        " levied",
                        " on",
                        " the",
                        " Central",
                        " and",
                        " state",
                        " governments",
                        " and",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 33.12109756469727
        },
        {
            "feature_index": 18648,
            "gpt_predictions": [
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "terms related to physical or psychological trauma",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " hundreds of Iraq war veterans suffering from post traumatic stress disorder. Participants in this ongoing study",
                    "max_token": " traumatic",
                    "tokens": [
                        " hundreds",
                        " of",
                        " Iraq",
                        " war",
                        " veterans",
                        " suffering",
                        " from",
                        " post",
                        " traumatic",
                        " stress",
                        " disorder",
                        ".",
                        " Participants",
                        " in",
                        " this",
                        " ongoing",
                        " study"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        9.003937721252441,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " Health concerns e.g. stress, trauma, medications, medical conditions, heavy metals",
                    "max_token": " trauma",
                    "tokens": [
                        " Health",
                        " concerns",
                        " e",
                        ".",
                        "g",
                        ".",
                        " stress",
                        ",",
                        " trauma",
                        ",",
                        " medications",
                        ",",
                        " medical",
                        " conditions",
                        ",",
                        " heavy",
                        " metals"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        37.78424072265625,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " nation's prison population could be redirected into trauma-sensitive training for teachers and administrators,",
                    "max_token": " trauma",
                    "tokens": [
                        " nation",
                        "'s",
                        " prison",
                        " population",
                        " could",
                        " be",
                        " redirected",
                        " into",
                        " trauma",
                        "-",
                        "sensitive",
                        " training",
                        " for",
                        " teachers",
                        " and",
                        " administrators",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        37.17726516723633,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " dementia Center's brain to be donated to trauma study\u010a\u010aFormer 49ers center Forrest",
                    "max_token": " trauma",
                    "tokens": [
                        " dementia",
                        " Center",
                        "'s",
                        " brain",
                        " to",
                        " be",
                        " donated",
                        " to",
                        " trauma",
                        " study",
                        "\u010a",
                        "\u010a",
                        "Former",
                        " 49",
                        "ers",
                        " center",
                        " Forrest"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        36.35952377319336,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " help. And as time passed, the traumatic memories became more and more intrusive.\u010a",
                    "max_token": " traumatic",
                    "tokens": [
                        " help",
                        ".",
                        " And",
                        " as",
                        " time",
                        " passed",
                        ",",
                        " the",
                        " traumatic",
                        " memories",
                        " became",
                        " more",
                        " and",
                        " more",
                        " intrusive",
                        ".",
                        "\u010a"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        9.322181701660156,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " most rich countries. But \u00e2\u0122\u013epremiumisation\u00e2\u0122\u013f is working. Though still",
                    "tokens": [
                        " most",
                        " rich",
                        " countries",
                        ".",
                        " But",
                        " \u00e2\u0122",
                        "\u013e",
                        "prem",
                        "ium",
                        "isation",
                        "\u00e2\u0122",
                        "\u013f",
                        " is",
                        " working",
                        ".",
                        " Though",
                        " still"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " outfit in the top 20. The Denver Nuggets reportedly spend \u00c2\u00a32,939,",
                    "tokens": [
                        " outfit",
                        " in",
                        " the",
                        " top",
                        " 20",
                        ".",
                        " The",
                        " Denver",
                        " Nuggets",
                        " reportedly",
                        " spend",
                        " \u00c2\u00a3",
                        "2",
                        ",",
                        "9",
                        "39",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "if Al-Arab al-Qathafi, where three of Colonel Gaddafi's grandchildren",
                    "tokens": [
                        "if",
                        " Al",
                        "-",
                        "Arab",
                        " al",
                        "-",
                        "Q",
                        "ath",
                        "afi",
                        ",",
                        " where",
                        " three",
                        " of",
                        " Colonel",
                        " Gaddafi",
                        "'s",
                        " grandchildren"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "2 14 Lubbock +4 20 Denver +2 23 Portland +2 7 Birmingham",
                    "tokens": [
                        "2",
                        " 14",
                        " L",
                        "ubb",
                        "ock",
                        " +",
                        "4",
                        " 20",
                        " Denver",
                        " +",
                        "2",
                        " 23",
                        " Portland",
                        " +",
                        "2",
                        " 7",
                        " Birmingham"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " has to be levied, it should be levied on the Central and state governments and the",
                    "tokens": [
                        " has",
                        " to",
                        " be",
                        " levied",
                        ",",
                        " it",
                        " should",
                        " be",
                        " levied",
                        " on",
                        " the",
                        " Central",
                        " and",
                        " state",
                        " governments",
                        " and",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 39.09721374511719
        },
        {
            "feature_index": 13705,
            "gpt_predictions": [
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "adjectives related to rationality and logic",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " without jobs.\u010a\u010aThere are perfectly sane Christians out there and there have been perfectly",
                    "max_token": " sane",
                    "tokens": [
                        " without",
                        " jobs",
                        ".",
                        "\u010a",
                        "\u010a",
                        "There",
                        " are",
                        " perfectly",
                        " sane",
                        " Christians",
                        " out",
                        " there",
                        " and",
                        " there",
                        " have",
                        " been",
                        " perfectly"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        10.50432395935059,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " for a generation.\u010a\u010aIn any sane universe of political discourse, a party that",
                    "max_token": " sane",
                    "tokens": [
                        " for",
                        " a",
                        " generation",
                        ".",
                        "\u010a",
                        "\u010a",
                        "In",
                        " any",
                        " sane",
                        " universe",
                        " of",
                        " political",
                        " discourse",
                        ",",
                        " a",
                        " party",
                        " that"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        12.23275279998779,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " a real phenomenon? Of course. No sensible person would argue otherwise. But is it",
                    "max_token": " sensible",
                    "tokens": [
                        " a",
                        " real",
                        " phenomenon",
                        "?",
                        " Of",
                        " course",
                        ".",
                        " No",
                        " sensible",
                        " person",
                        " would",
                        " argue",
                        " otherwise",
                        ".",
                        " But",
                        " is",
                        " it"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        31.24946022033691,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " said the employees. Some saw it as prudent, while others thought it a sign of",
                    "max_token": " prudent",
                    "tokens": [
                        " said",
                        " the",
                        " employees",
                        ".",
                        " Some",
                        " saw",
                        " it",
                        " as",
                        " prudent",
                        ",",
                        " while",
                        " others",
                        " thought",
                        " it",
                        " a",
                        " sign",
                        " of"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        7.246280193328857,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": ". Those moderate Muslim-Americans wrote something thoughtful, and they are not loudly parading",
                    "max_token": " thoughtful",
                    "tokens": [
                        ".",
                        " Those",
                        " moderate",
                        " Muslim",
                        "-",
                        "Americans",
                        " wrote",
                        " something",
                        " thoughtful",
                        ",",
                        " and",
                        " they",
                        " are",
                        " not",
                        " loudly",
                        " par",
                        "ading"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        7.476162433624268,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " most rich countries. But \u00e2\u0122\u013epremiumisation\u00e2\u0122\u013f is working. Though still",
                    "tokens": [
                        " most",
                        " rich",
                        " countries",
                        ".",
                        " But",
                        " \u00e2\u0122",
                        "\u013e",
                        "prem",
                        "ium",
                        "isation",
                        "\u00e2\u0122",
                        "\u013f",
                        " is",
                        " working",
                        ".",
                        " Though",
                        " still"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " outfit in the top 20. The Denver Nuggets reportedly spend \u00c2\u00a32,939,",
                    "tokens": [
                        " outfit",
                        " in",
                        " the",
                        " top",
                        " 20",
                        ".",
                        " The",
                        " Denver",
                        " Nuggets",
                        " reportedly",
                        " spend",
                        " \u00c2\u00a3",
                        "2",
                        ",",
                        "9",
                        "39",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "if Al-Arab al-Qathafi, where three of Colonel Gaddafi's grandchildren",
                    "tokens": [
                        "if",
                        " Al",
                        "-",
                        "Arab",
                        " al",
                        "-",
                        "Q",
                        "ath",
                        "afi",
                        ",",
                        " where",
                        " three",
                        " of",
                        " Colonel",
                        " Gaddafi",
                        "'s",
                        " grandchildren"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "2 14 Lubbock +4 20 Denver +2 23 Portland +2 7 Birmingham",
                    "tokens": [
                        "2",
                        " 14",
                        " L",
                        "ubb",
                        "ock",
                        " +",
                        "4",
                        " 20",
                        " Denver",
                        " +",
                        "2",
                        " 23",
                        " Portland",
                        " +",
                        "2",
                        " 7",
                        " Birmingham"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " has to be levied, it should be levied on the Central and state governments and the",
                    "tokens": [
                        " has",
                        " to",
                        " be",
                        " levied",
                        ",",
                        " it",
                        " should",
                        " be",
                        " levied",
                        " on",
                        " the",
                        " Central",
                        " and",
                        " state",
                        " governments",
                        " and",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 33.43647384643555
        },
        {
            "feature_index": 2693,
            "gpt_predictions": [
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    1.0
                ],
                [
                    0,
                    1.0
                ],
                [
                    0,
                    1.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "proper nouns, specifically locations or names of places and people",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " untaxed cigarettes; John Crawford in Beavercreek, Ohio on August 5,",
                    "max_token": " Beaver",
                    "tokens": [
                        " unt",
                        "ax",
                        "ed",
                        " cigarettes",
                        ";",
                        " John",
                        " Crawford",
                        " in",
                        " Beaver",
                        "c",
                        "reek",
                        ",",
                        " Ohio",
                        " on",
                        " August",
                        " 5",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        6.373401641845703,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " care of itself.\u00e2\u0122\u013f<|endoftext|>Albert Pike, poet, Freemason, and Confederate",
                    "max_token": " Pike",
                    "tokens": [
                        " care",
                        " of",
                        " itself",
                        ".",
                        "\u00e2\u0122",
                        "\u013f",
                        "<|endoftext|>",
                        "Albert",
                        " Pike",
                        ",",
                        " poet",
                        ",",
                        " Freem",
                        "ason",
                        ",",
                        " and",
                        " Confederate"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        9.080371856689453,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " Health, all in Toronto.<|endoftext|>Christian Petersen/Getty Images\u010a\u010aNew England Patriots",
                    "max_token": " Petersen",
                    "tokens": [
                        " Health",
                        ",",
                        " all",
                        " in",
                        " Toronto",
                        ".",
                        "<|endoftext|>",
                        "Christian",
                        " Petersen",
                        "/",
                        "Getty",
                        " Images",
                        "\u010a",
                        "\u010a",
                        "New",
                        " England",
                        " Patriots"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        6.089240074157715,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " Limbaugh claiming that the \u00e2\u0122\u013eNew Black Panther Party,\u00e2\u0122\u013f with the<|endoftext|>\u010a",
                    "max_token": " Panther",
                    "tokens": [
                        " Limbaugh",
                        " claiming",
                        " that",
                        " the",
                        " \u00e2\u0122",
                        "\u013e",
                        "New",
                        " Black",
                        " Panther",
                        " Party",
                        ",",
                        "\u00e2\u0122",
                        "\u013f",
                        " with",
                        " the",
                        "<|endoftext|>",
                        "\u010a"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        4.228074550628662,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " mechanisms involved in the process, said Michael Kuhar, professor of neuropharmacology",
                    "max_token": " Kuh",
                    "tokens": [
                        " mechanisms",
                        " involved",
                        " in",
                        " the",
                        " process",
                        ",",
                        " said",
                        " Michael",
                        " Kuh",
                        "ar",
                        ",",
                        " professor",
                        " of",
                        " neuro",
                        "ph",
                        "armac",
                        "ology"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        6.585012435913086,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " most rich countries. But \u00e2\u0122\u013epremiumisation\u00e2\u0122\u013f is working. Though still",
                    "tokens": [
                        " most",
                        " rich",
                        " countries",
                        ".",
                        " But",
                        " \u00e2\u0122",
                        "\u013e",
                        "prem",
                        "ium",
                        "isation",
                        "\u00e2\u0122",
                        "\u013f",
                        " is",
                        " working",
                        ".",
                        " Though",
                        " still"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " outfit in the top 20. The Denver Nuggets reportedly spend \u00c2\u00a32,939,",
                    "tokens": [
                        " outfit",
                        " in",
                        " the",
                        " top",
                        " 20",
                        ".",
                        " The",
                        " Denver",
                        " Nuggets",
                        " reportedly",
                        " spend",
                        " \u00c2\u00a3",
                        "2",
                        ",",
                        "9",
                        "39",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "if Al-Arab al-Qathafi, where three of Colonel Gaddafi's grandchildren",
                    "tokens": [
                        "if",
                        " Al",
                        "-",
                        "Arab",
                        " al",
                        "-",
                        "Q",
                        "ath",
                        "afi",
                        ",",
                        " where",
                        " three",
                        " of",
                        " Colonel",
                        " Gaddafi",
                        "'s",
                        " grandchildren"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "2 14 Lubbock +4 20 Denver +2 23 Portland +2 7 Birmingham",
                    "tokens": [
                        "2",
                        " 14",
                        " L",
                        "ubb",
                        "ock",
                        " +",
                        "4",
                        " 20",
                        " Denver",
                        " +",
                        "2",
                        " 23",
                        " Portland",
                        " +",
                        "2",
                        " 7",
                        " Birmingham"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " has to be levied, it should be levied on the Central and state governments and the",
                    "tokens": [
                        " has",
                        " to",
                        " be",
                        " levied",
                        ",",
                        " it",
                        " should",
                        " be",
                        " levied",
                        " on",
                        " the",
                        " Central",
                        " and",
                        " state",
                        " governments",
                        " and",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 41.18330001831055
        },
        {
            "feature_index": 3705,
            "gpt_predictions": [
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "a specific last name \"Grady.\"",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": ", echoing comments made by Mr. Casady on the morning's conference call. Sevent",
                    "max_token": "ady",
                    "tokens": [
                        ",",
                        " echoing",
                        " comments",
                        " made",
                        " by",
                        " Mr",
                        ".",
                        " Cas",
                        "ady",
                        " on",
                        " the",
                        " morning",
                        "'s",
                        " conference",
                        " call",
                        ".",
                        " Sevent"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        28.0715274810791,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " University of Pittsburgh. LeSean \"Shady\" McCoy, the team's star running",
                    "max_token": "ady",
                    "tokens": [
                        " University",
                        " of",
                        " Pittsburgh",
                        ".",
                        " Le",
                        "Sean",
                        " \"",
                        "Sh",
                        "ady",
                        "\"",
                        " McCoy",
                        ",",
                        " the",
                        " team",
                        "'s",
                        " star",
                        " running"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        29.98476982116699,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " I saved a ton.\" -Sean Grady, SMU class of 2016.\u010a",
                    "max_token": "ady",
                    "tokens": [
                        " I",
                        " saved",
                        " a",
                        " ton",
                        ".\"",
                        " -",
                        "Sean",
                        " Gr",
                        "ady",
                        ",",
                        " SM",
                        "U",
                        " class",
                        " of",
                        " 2016",
                        ".",
                        "\u010a"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        31.10109519958496,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " bounds,\" said Baird.\u010a\u010aBrady says he only followed the vehicle to get",
                    "max_token": "ady",
                    "tokens": [
                        " bounds",
                        ",\"",
                        " said",
                        " Baird",
                        ".",
                        "\u010a",
                        "\u010a",
                        "Br",
                        "ady",
                        " says",
                        " he",
                        " only",
                        " followed",
                        " the",
                        " vehicle",
                        " to",
                        " get"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        23.72756385803223,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " Bishop, R-Utah, recently shepherded five bills out of the Natural Resources",
                    "max_token": "pher",
                    "tokens": [
                        " Bishop",
                        ",",
                        " R",
                        "-",
                        "Utah",
                        ",",
                        " recently",
                        " she",
                        "pher",
                        "ded",
                        " five",
                        " bills",
                        " out",
                        " of",
                        " the",
                        " Natural",
                        " Resources"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        4.247149467468262,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " most rich countries. But \u00e2\u0122\u013epremiumisation\u00e2\u0122\u013f is working. Though still",
                    "tokens": [
                        " most",
                        " rich",
                        " countries",
                        ".",
                        " But",
                        " \u00e2\u0122",
                        "\u013e",
                        "prem",
                        "ium",
                        "isation",
                        "\u00e2\u0122",
                        "\u013f",
                        " is",
                        " working",
                        ".",
                        " Though",
                        " still"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " outfit in the top 20. The Denver Nuggets reportedly spend \u00c2\u00a32,939,",
                    "tokens": [
                        " outfit",
                        " in",
                        " the",
                        " top",
                        " 20",
                        ".",
                        " The",
                        " Denver",
                        " Nuggets",
                        " reportedly",
                        " spend",
                        " \u00c2\u00a3",
                        "2",
                        ",",
                        "9",
                        "39",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "if Al-Arab al-Qathafi, where three of Colonel Gaddafi's grandchildren",
                    "tokens": [
                        "if",
                        " Al",
                        "-",
                        "Arab",
                        " al",
                        "-",
                        "Q",
                        "ath",
                        "afi",
                        ",",
                        " where",
                        " three",
                        " of",
                        " Colonel",
                        " Gaddafi",
                        "'s",
                        " grandchildren"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "2 14 Lubbock +4 20 Denver +2 23 Portland +2 7 Birmingham",
                    "tokens": [
                        "2",
                        " 14",
                        " L",
                        "ubb",
                        "ock",
                        " +",
                        "4",
                        " 20",
                        " Denver",
                        " +",
                        "2",
                        " 23",
                        " Portland",
                        " +",
                        "2",
                        " 7",
                        " Birmingham"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " has to be levied, it should be levied on the Central and state governments and the",
                    "tokens": [
                        " has",
                        " to",
                        " be",
                        " levied",
                        ",",
                        " it",
                        " should",
                        " be",
                        " levied",
                        " on",
                        " the",
                        " Central",
                        " and",
                        " state",
                        " governments",
                        " and",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 33.54273223876953
        },
        {
            "feature_index": 2245,
            "gpt_predictions": [
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "phrases introducing general statements or observations",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " with little experience, that's encouraging. Hopefully there's growth potential still untapped.",
                    "max_token": " Hopefully",
                    "tokens": [
                        " with",
                        " little",
                        " experience",
                        ",",
                        " that",
                        "'s",
                        " encouraging",
                        ".",
                        " Hopefully",
                        " there",
                        "'s",
                        " growth",
                        " potential",
                        " still",
                        " unt",
                        "apped",
                        "."
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        5.371774196624756,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " time since programmers are lazy(fact!). Normally I would make a IDA script to",
                    "max_token": " Normally",
                    "tokens": [
                        " time",
                        " since",
                        " programmers",
                        " are",
                        " lazy",
                        "(",
                        "fact",
                        "!).",
                        " Normally",
                        " I",
                        " would",
                        " make",
                        " a",
                        " ID",
                        "A",
                        " script",
                        " to"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        11.94458198547363,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " by the FISC.\u010a\u010aHistorically, the minimization procedures proposed by the",
                    "max_token": "orically",
                    "tokens": [
                        " by",
                        " the",
                        " F",
                        "ISC",
                        ".",
                        "\u010a",
                        "\u010a",
                        "Hist",
                        "orically",
                        ",",
                        " the",
                        " minim",
                        "ization",
                        " procedures",
                        " proposed",
                        " by",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0.1733839213848114,
                        3.640376567840576,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "\u013bs critique suffers from internal inconsistency. Generally he laments the airlines\u00e2\u0122\u013b broken",
                    "max_token": " Generally",
                    "tokens": [
                        "\u013b",
                        "s",
                        " critique",
                        " suffers",
                        " from",
                        " internal",
                        " inconsistency",
                        ".",
                        " Generally",
                        " he",
                        " l",
                        "aments",
                        " the",
                        " airlines",
                        "\u00e2\u0122",
                        "\u013b",
                        " broken"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        14.25124645233154,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " the bounds of natural variations.\u010a\u010aTypically, precipitation change is the afterthought in",
                    "max_token": "Typically",
                    "tokens": [
                        " the",
                        " bounds",
                        " of",
                        " natural",
                        " variations",
                        ".",
                        "\u010a",
                        "\u010a",
                        "Typically",
                        ",",
                        " precipitation",
                        " change",
                        " is",
                        " the",
                        " after",
                        "thought",
                        " in"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        13.81750774383545,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " most rich countries. But \u00e2\u0122\u013epremiumisation\u00e2\u0122\u013f is working. Though still",
                    "tokens": [
                        " most",
                        " rich",
                        " countries",
                        ".",
                        " But",
                        " \u00e2\u0122",
                        "\u013e",
                        "prem",
                        "ium",
                        "isation",
                        "\u00e2\u0122",
                        "\u013f",
                        " is",
                        " working",
                        ".",
                        " Though",
                        " still"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " outfit in the top 20. The Denver Nuggets reportedly spend \u00c2\u00a32,939,",
                    "tokens": [
                        " outfit",
                        " in",
                        " the",
                        " top",
                        " 20",
                        ".",
                        " The",
                        " Denver",
                        " Nuggets",
                        " reportedly",
                        " spend",
                        " \u00c2\u00a3",
                        "2",
                        ",",
                        "9",
                        "39",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "if Al-Arab al-Qathafi, where three of Colonel Gaddafi's grandchildren",
                    "tokens": [
                        "if",
                        " Al",
                        "-",
                        "Arab",
                        " al",
                        "-",
                        "Q",
                        "ath",
                        "afi",
                        ",",
                        " where",
                        " three",
                        " of",
                        " Colonel",
                        " Gaddafi",
                        "'s",
                        " grandchildren"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "2 14 Lubbock +4 20 Denver +2 23 Portland +2 7 Birmingham",
                    "tokens": [
                        "2",
                        " 14",
                        " L",
                        "ubb",
                        "ock",
                        " +",
                        "4",
                        " 20",
                        " Denver",
                        " +",
                        "2",
                        " 23",
                        " Portland",
                        " +",
                        "2",
                        " 7",
                        " Birmingham"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " has to be levied, it should be levied on the Central and state governments and the",
                    "tokens": [
                        " has",
                        " to",
                        " be",
                        " levied",
                        ",",
                        " it",
                        " should",
                        " be",
                        " levied",
                        " on",
                        " the",
                        " Central",
                        " and",
                        " state",
                        " governments",
                        " and",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 15.71995162963867
        },
        {
            "feature_index": 7296,
            "gpt_predictions": [
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "instances of the word \"draft\" in documents",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " the 11th pick of the 2016 NBA draft. He was an opening day starter and",
                    "max_token": " draft",
                    "tokens": [
                        " the",
                        " 11",
                        "th",
                        " pick",
                        " of",
                        " the",
                        " 2016",
                        " NBA",
                        " draft",
                        ".",
                        " He",
                        " was",
                        " an",
                        " opening",
                        " day",
                        " starter",
                        " and"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        21.5376091003418,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " states and MEPs as they debate the draft directives, which are likely to be watered",
                    "max_token": " draft",
                    "tokens": [
                        " states",
                        " and",
                        " MEP",
                        "s",
                        " as",
                        " they",
                        " debate",
                        " the",
                        " draft",
                        " directives",
                        ",",
                        " which",
                        " are",
                        " likely",
                        " to",
                        " be",
                        " watered"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        37.44305801391602,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " shared a few facts about these microbes in Draft Magazine earlier this year:\u010a\u010aB",
                    "max_token": " Draft",
                    "tokens": [
                        " shared",
                        " a",
                        " few",
                        " facts",
                        " about",
                        " these",
                        " microbes",
                        " in",
                        " Draft",
                        " Magazine",
                        " earlier",
                        " this",
                        " year",
                        ":",
                        "\u010a",
                        "\u010a",
                        "B"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        11.24621677398682,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " Chargers have selected just one quarterback in the draft under general manager Tom Telesco --",
                    "max_token": " draft",
                    "tokens": [
                        " Chargers",
                        " have",
                        " selected",
                        " just",
                        " one",
                        " quarterback",
                        " in",
                        " the",
                        " draft",
                        " under",
                        " general",
                        " manager",
                        " Tom",
                        " Tel",
                        "es",
                        "co",
                        " --"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        29.35346031188965,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " clause had been quietly reintroduced into a draft bill on wiretapping and gag laws.",
                    "max_token": " draft",
                    "tokens": [
                        " clause",
                        " had",
                        " been",
                        " quietly",
                        " reintrodu",
                        "ced",
                        " into",
                        " a",
                        " draft",
                        " bill",
                        " on",
                        " wiret",
                        "apping",
                        " and",
                        " gag",
                        " laws",
                        "."
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        38.32394409179688,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " most rich countries. But \u00e2\u0122\u013epremiumisation\u00e2\u0122\u013f is working. Though still",
                    "tokens": [
                        " most",
                        " rich",
                        " countries",
                        ".",
                        " But",
                        " \u00e2\u0122",
                        "\u013e",
                        "prem",
                        "ium",
                        "isation",
                        "\u00e2\u0122",
                        "\u013f",
                        " is",
                        " working",
                        ".",
                        " Though",
                        " still"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " outfit in the top 20. The Denver Nuggets reportedly spend \u00c2\u00a32,939,",
                    "tokens": [
                        " outfit",
                        " in",
                        " the",
                        " top",
                        " 20",
                        ".",
                        " The",
                        " Denver",
                        " Nuggets",
                        " reportedly",
                        " spend",
                        " \u00c2\u00a3",
                        "2",
                        ",",
                        "9",
                        "39",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "if Al-Arab al-Qathafi, where three of Colonel Gaddafi's grandchildren",
                    "tokens": [
                        "if",
                        " Al",
                        "-",
                        "Arab",
                        " al",
                        "-",
                        "Q",
                        "ath",
                        "afi",
                        ",",
                        " where",
                        " three",
                        " of",
                        " Colonel",
                        " Gaddafi",
                        "'s",
                        " grandchildren"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "2 14 Lubbock +4 20 Denver +2 23 Portland +2 7 Birmingham",
                    "tokens": [
                        "2",
                        " 14",
                        " L",
                        "ubb",
                        "ock",
                        " +",
                        "4",
                        " 20",
                        " Denver",
                        " +",
                        "2",
                        " 23",
                        " Portland",
                        " +",
                        "2",
                        " 7",
                        " Birmingham"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " has to be levied, it should be levied on the Central and state governments and the",
                    "tokens": [
                        " has",
                        " to",
                        " be",
                        " levied",
                        ",",
                        " it",
                        " should",
                        " be",
                        " levied",
                        " on",
                        " the",
                        " Central",
                        " and",
                        " state",
                        " governments",
                        " and",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 42.3241081237793
        },
        {
            "feature_index": 6642,
            "gpt_predictions": [
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "references to the physical human head",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " from people's hands as they bowed their heads in a moment of silence for Charlottesville and",
                    "max_token": " heads",
                    "tokens": [
                        " from",
                        " people",
                        "'s",
                        " hands",
                        " as",
                        " they",
                        " bowed",
                        " their",
                        " heads",
                        " in",
                        " a",
                        " moment",
                        " of",
                        " silence",
                        " for",
                        " Charlottesville",
                        " and"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        20.65897750854492,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " divers with knives and pistols attached to their heads, according to RIA Novosti",
                    "max_token": " heads",
                    "tokens": [
                        " divers",
                        " with",
                        " knives",
                        " and",
                        " pistols",
                        " attached",
                        " to",
                        " their",
                        " heads",
                        ",",
                        " according",
                        " to",
                        " R",
                        "IA",
                        " Nov",
                        "ost",
                        "i"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        22.78090286254883,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " out so fast that it sailed over the heads of the studio audience.\u010a\u010a\u00e2\u0122",
                    "max_token": " heads",
                    "tokens": [
                        " out",
                        " so",
                        " fast",
                        " that",
                        " it",
                        " sailed",
                        " over",
                        " the",
                        " heads",
                        " of",
                        " the",
                        " studio",
                        " audience",
                        ".",
                        "\u010a",
                        "\u010a",
                        "\u00e2\u0122"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        22.58793640136719,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " in 1911, it exists only in the heads of progressives in 2014.\u010a\u010aNBC",
                    "max_token": " heads",
                    "tokens": [
                        " in",
                        " 1911",
                        ",",
                        " it",
                        " exists",
                        " only",
                        " in",
                        " the",
                        " heads",
                        " of",
                        " progressives",
                        " in",
                        " 2014",
                        ".",
                        "\u010a",
                        "\u010a",
                        "NBC"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        23.99855995178223,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " You know the saying \u00e2\u0122\u013ebury our heads in the sand\u00e2\u0122\u013f to mean ignoring",
                    "max_token": " heads",
                    "tokens": [
                        " You",
                        " know",
                        " the",
                        " saying",
                        " \u00e2\u0122",
                        "\u013e",
                        "bury",
                        " our",
                        " heads",
                        " in",
                        " the",
                        " sand",
                        "\u00e2\u0122",
                        "\u013f",
                        " to",
                        " mean",
                        " ignoring"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        23.47069931030273,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " most rich countries. But \u00e2\u0122\u013epremiumisation\u00e2\u0122\u013f is working. Though still",
                    "tokens": [
                        " most",
                        " rich",
                        " countries",
                        ".",
                        " But",
                        " \u00e2\u0122",
                        "\u013e",
                        "prem",
                        "ium",
                        "isation",
                        "\u00e2\u0122",
                        "\u013f",
                        " is",
                        " working",
                        ".",
                        " Though",
                        " still"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " outfit in the top 20. The Denver Nuggets reportedly spend \u00c2\u00a32,939,",
                    "tokens": [
                        " outfit",
                        " in",
                        " the",
                        " top",
                        " 20",
                        ".",
                        " The",
                        " Denver",
                        " Nuggets",
                        " reportedly",
                        " spend",
                        " \u00c2\u00a3",
                        "2",
                        ",",
                        "9",
                        "39",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "if Al-Arab al-Qathafi, where three of Colonel Gaddafi's grandchildren",
                    "tokens": [
                        "if",
                        " Al",
                        "-",
                        "Arab",
                        " al",
                        "-",
                        "Q",
                        "ath",
                        "afi",
                        ",",
                        " where",
                        " three",
                        " of",
                        " Colonel",
                        " Gaddafi",
                        "'s",
                        " grandchildren"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "2 14 Lubbock +4 20 Denver +2 23 Portland +2 7 Birmingham",
                    "tokens": [
                        "2",
                        " 14",
                        " L",
                        "ubb",
                        "ock",
                        " +",
                        "4",
                        " 20",
                        " Denver",
                        " +",
                        "2",
                        " 23",
                        " Portland",
                        " +",
                        "2",
                        " 7",
                        " Birmingham"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " has to be levied, it should be levied on the Central and state governments and the",
                    "tokens": [
                        " has",
                        " to",
                        " be",
                        " levied",
                        ",",
                        " it",
                        " should",
                        " be",
                        " levied",
                        " on",
                        " the",
                        " Central",
                        " and",
                        " state",
                        " governments",
                        " and",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 25.97125816345215
        },
        {
            "feature_index": 12509,
            "gpt_predictions": [
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "user profiles or social media user information",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 2,
                    "sentence_string": "%UserProfile%\\Desktop %UserProfile%\\MyMusic %UserProfile%",
                    "max_token": "Profile",
                    "tokens": [
                        "%",
                        "User",
                        "Profile",
                        "%",
                        "\\",
                        "Desktop",
                        " %",
                        "User",
                        "Profile",
                        "%",
                        "\\",
                        "My",
                        "Music",
                        " %",
                        "User",
                        "Profile",
                        "%"
                    ],
                    "values": [
                        0,
                        0,
                        11.95388984680176,
                        0,
                        0,
                        0,
                        0,
                        0,
                        9.96367073059082,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        9.024645805358887,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " files and stored them under the %UserProfile% folder, such as the Downloads folder",
                    "max_token": "Profile",
                    "tokens": [
                        " files",
                        " and",
                        " stored",
                        " them",
                        " under",
                        " the",
                        " %",
                        "User",
                        "Profile",
                        "%",
                        " folder",
                        ",",
                        " such",
                        " as",
                        " the",
                        " Downloads",
                        " folder"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        11.98971557617188,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "<|endoftext|>EternaLEnVy Profile Joined February 2009 Canada 203 Posts Last Edited",
                    "max_token": " Profile",
                    "tokens": [
                        "<|endoftext|>",
                        "E",
                        "tern",
                        "a",
                        "LE",
                        "n",
                        "V",
                        "y",
                        " Profile",
                        " Joined",
                        " February",
                        " 2009",
                        " Canada",
                        " 203",
                        " Posts",
                        " Last",
                        " Edited"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        33.66845321655273,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " fact it was just encrypting %UserProfile% again and thus the Desktop got hit",
                    "max_token": "Profile",
                    "tokens": [
                        " fact",
                        " it",
                        " was",
                        " just",
                        " encrypt",
                        "ing",
                        " %",
                        "User",
                        "Profile",
                        "%",
                        " again",
                        " and",
                        " thus",
                        " the",
                        " Desktop",
                        " got",
                        " hit"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        9.583617210388184,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " against Jadhav:\u010a\u010aConfessional video statement of Kulbushan Jhad",
                    "max_token": "essional",
                    "tokens": [
                        " against",
                        " J",
                        "adh",
                        "av",
                        ":",
                        "\u010a",
                        "\u010a",
                        "Conf",
                        "essional",
                        " video",
                        " statement",
                        " of",
                        " Kul",
                        "bush",
                        "an",
                        " J",
                        "had"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        4.106054306030273,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " most rich countries. But \u00e2\u0122\u013epremiumisation\u00e2\u0122\u013f is working. Though still",
                    "tokens": [
                        " most",
                        " rich",
                        " countries",
                        ".",
                        " But",
                        " \u00e2\u0122",
                        "\u013e",
                        "prem",
                        "ium",
                        "isation",
                        "\u00e2\u0122",
                        "\u013f",
                        " is",
                        " working",
                        ".",
                        " Though",
                        " still"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " outfit in the top 20. The Denver Nuggets reportedly spend \u00c2\u00a32,939,",
                    "tokens": [
                        " outfit",
                        " in",
                        " the",
                        " top",
                        " 20",
                        ".",
                        " The",
                        " Denver",
                        " Nuggets",
                        " reportedly",
                        " spend",
                        " \u00c2\u00a3",
                        "2",
                        ",",
                        "9",
                        "39",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "if Al-Arab al-Qathafi, where three of Colonel Gaddafi's grandchildren",
                    "tokens": [
                        "if",
                        " Al",
                        "-",
                        "Arab",
                        " al",
                        "-",
                        "Q",
                        "ath",
                        "afi",
                        ",",
                        " where",
                        " three",
                        " of",
                        " Colonel",
                        " Gaddafi",
                        "'s",
                        " grandchildren"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "2 14 Lubbock +4 20 Denver +2 23 Portland +2 7 Birmingham",
                    "tokens": [
                        "2",
                        " 14",
                        " L",
                        "ubb",
                        "ock",
                        " +",
                        "4",
                        " 20",
                        " Denver",
                        " +",
                        "2",
                        " 23",
                        " Portland",
                        " +",
                        "2",
                        " 7",
                        " Birmingham"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " has to be levied, it should be levied on the Central and state governments and the",
                    "tokens": [
                        " has",
                        " to",
                        " be",
                        " levied",
                        ",",
                        " it",
                        " should",
                        " be",
                        " levied",
                        " on",
                        " the",
                        " Central",
                        " and",
                        " state",
                        " governments",
                        " and",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 39.07465744018555
        },
        {
            "feature_index": 7110,
            "gpt_predictions": [
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "mentions of the researcher Emmanuel Stamatakis",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " two feet from the sunroom was totally uprooted,\" Theresa Talbert, resident,",
                    "max_token": " upro",
                    "tokens": [
                        " two",
                        " feet",
                        " from",
                        " the",
                        " sun",
                        "room",
                        " was",
                        " totally",
                        " upro",
                        "oted",
                        ",\"",
                        " Theresa",
                        " Tal",
                        "bert",
                        ",",
                        " resident",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        3.837218046188354,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " area near a hospital.\u010a\u010aDespite strenuous denials from Moscow, Turkey has",
                    "max_token": " stren",
                    "tokens": [
                        " area",
                        " near",
                        " a",
                        " hospital",
                        ".",
                        "\u010a",
                        "\u010a",
                        "Despite",
                        " stren",
                        "uous",
                        " den",
                        "ials",
                        " from",
                        " Moscow",
                        ",",
                        " Turkey",
                        " has"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        8.68370246887207,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "(01-18) 22:16 PST \u00e2\u0122\u0136 In their second NFL seasons, 49",
                    "max_token": " PST",
                    "tokens": [
                        "(",
                        "01",
                        "-",
                        "18",
                        ")",
                        " 22",
                        ":",
                        "16",
                        " PST",
                        " \u00e2\u0122\u0136",
                        " In",
                        " their",
                        " second",
                        " NFL",
                        " seasons",
                        ",",
                        " 49"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        7.535638332366943,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " March 7th, 2013 at 10am PST and we\u00e2\u0122\u013bll be on site",
                    "max_token": " PST",
                    "tokens": [
                        " March",
                        " 7",
                        "th",
                        ",",
                        " 2013",
                        " at",
                        " 10",
                        "am",
                        " PST",
                        " and",
                        " we",
                        "\u00e2\u0122",
                        "\u013b",
                        "ll",
                        " be",
                        " on",
                        " site"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        7.166712760925293,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "oshino Library (\u00e6\u013a\u0141\u00e9\u0129\u0130\u00e6\u0138\u0129\u00e5\u00ba\u00ab, Hoshino bunk",
                    "max_token": "\u00e6\u0138",
                    "tokens": [
                        "osh",
                        "ino",
                        " Library",
                        " (",
                        "\u00e6\u013a",
                        "\u0141",
                        "\u00e9\u0129",
                        "\u0130",
                        "\u00e6\u0138",
                        "\u0129",
                        "\u00e5\u00ba",
                        "\u00ab",
                        ",",
                        " H",
                        "osh",
                        "ino",
                        " bunk"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        4.021200180053711,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " most rich countries. But \u00e2\u0122\u013epremiumisation\u00e2\u0122\u013f is working. Though still",
                    "tokens": [
                        " most",
                        " rich",
                        " countries",
                        ".",
                        " But",
                        " \u00e2\u0122",
                        "\u013e",
                        "prem",
                        "ium",
                        "isation",
                        "\u00e2\u0122",
                        "\u013f",
                        " is",
                        " working",
                        ".",
                        " Though",
                        " still"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " outfit in the top 20. The Denver Nuggets reportedly spend \u00c2\u00a32,939,",
                    "tokens": [
                        " outfit",
                        " in",
                        " the",
                        " top",
                        " 20",
                        ".",
                        " The",
                        " Denver",
                        " Nuggets",
                        " reportedly",
                        " spend",
                        " \u00c2\u00a3",
                        "2",
                        ",",
                        "9",
                        "39",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "if Al-Arab al-Qathafi, where three of Colonel Gaddafi's grandchildren",
                    "tokens": [
                        "if",
                        " Al",
                        "-",
                        "Arab",
                        " al",
                        "-",
                        "Q",
                        "ath",
                        "afi",
                        ",",
                        " where",
                        " three",
                        " of",
                        " Colonel",
                        " Gaddafi",
                        "'s",
                        " grandchildren"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "2 14 Lubbock +4 20 Denver +2 23 Portland +2 7 Birmingham",
                    "tokens": [
                        "2",
                        " 14",
                        " L",
                        "ubb",
                        "ock",
                        " +",
                        "4",
                        " 20",
                        " Denver",
                        " +",
                        "2",
                        " 23",
                        " Portland",
                        " +",
                        "2",
                        " 7",
                        " Birmingham"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " has to be levied, it should be levied on the Central and state governments and the",
                    "tokens": [
                        " has",
                        " to",
                        " be",
                        " levied",
                        ",",
                        " it",
                        " should",
                        " be",
                        " levied",
                        " on",
                        " the",
                        " Central",
                        " and",
                        " state",
                        " governments",
                        " and",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 19.15429306030273
        },
        {
            "feature_index": 10737,
            "gpt_predictions": [
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "phrases indicating probability or likelihood",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "s League football last year and they look likely to knock Arsenal out of next year\u00e2\u0122",
                    "max_token": " likely",
                    "tokens": [
                        "s",
                        " League",
                        " football",
                        " last",
                        " year",
                        " and",
                        " they",
                        " look",
                        " likely",
                        " to",
                        " knock",
                        " Arsenal",
                        " out",
                        " of",
                        " next",
                        " year",
                        "\u00e2\u0122"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        7.36757755279541,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "Self-identified libertarians are 22 points more likely than self-identified liberals to say the",
                    "max_token": " likely",
                    "tokens": [
                        "Self",
                        "-",
                        "identified",
                        " libertarians",
                        " are",
                        " 22",
                        " points",
                        " more",
                        " likely",
                        " than",
                        " self",
                        "-",
                        "identified",
                        " liberals",
                        " to",
                        " say",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        30.36898422241211,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " lower verbal intelligence. They are also more likely to believe in conspiracy theories and the paranormal",
                    "max_token": " likely",
                    "tokens": [
                        " lower",
                        " verbal",
                        " intelligence",
                        ".",
                        " They",
                        " are",
                        " also",
                        " more",
                        " likely",
                        " to",
                        " believe",
                        " in",
                        " conspiracy",
                        " theories",
                        " and",
                        " the",
                        " paranormal"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        30.8703670501709,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " that preannouncements are also more likely to provoke retaliation in industries with high patent",
                    "max_token": " likely",
                    "tokens": [
                        " that",
                        " pre",
                        "ann",
                        "ounce",
                        "ments",
                        " are",
                        " also",
                        " more",
                        " likely",
                        " to",
                        " provoke",
                        " retaliation",
                        " in",
                        " industries",
                        " with",
                        " high",
                        " patent"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        31.05582809448242,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " a few hundred kilometres apart \u00e2\u0122\u0135 were more likely to have children with each other<|endoftext|> into",
                    "max_token": " likely",
                    "tokens": [
                        " a",
                        " few",
                        " hundred",
                        " kilometres",
                        " apart",
                        " \u00e2\u0122\u0135",
                        " were",
                        " more",
                        " likely",
                        " to",
                        " have",
                        " children",
                        " with",
                        " each",
                        " other",
                        "<|endoftext|>",
                        " into"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        30.91801071166992,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " most rich countries. But \u00e2\u0122\u013epremiumisation\u00e2\u0122\u013f is working. Though still",
                    "tokens": [
                        " most",
                        " rich",
                        " countries",
                        ".",
                        " But",
                        " \u00e2\u0122",
                        "\u013e",
                        "prem",
                        "ium",
                        "isation",
                        "\u00e2\u0122",
                        "\u013f",
                        " is",
                        " working",
                        ".",
                        " Though",
                        " still"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " outfit in the top 20. The Denver Nuggets reportedly spend \u00c2\u00a32,939,",
                    "tokens": [
                        " outfit",
                        " in",
                        " the",
                        " top",
                        " 20",
                        ".",
                        " The",
                        " Denver",
                        " Nuggets",
                        " reportedly",
                        " spend",
                        " \u00c2\u00a3",
                        "2",
                        ",",
                        "9",
                        "39",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "if Al-Arab al-Qathafi, where three of Colonel Gaddafi's grandchildren",
                    "tokens": [
                        "if",
                        " Al",
                        "-",
                        "Arab",
                        " al",
                        "-",
                        "Q",
                        "ath",
                        "afi",
                        ",",
                        " where",
                        " three",
                        " of",
                        " Colonel",
                        " Gaddafi",
                        "'s",
                        " grandchildren"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "2 14 Lubbock +4 20 Denver +2 23 Portland +2 7 Birmingham",
                    "tokens": [
                        "2",
                        " 14",
                        " L",
                        "ubb",
                        "ock",
                        " +",
                        "4",
                        " 20",
                        " Denver",
                        " +",
                        "2",
                        " 23",
                        " Portland",
                        " +",
                        "2",
                        " 7",
                        " Birmingham"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " has to be levied, it should be levied on the Central and state governments and the",
                    "tokens": [
                        " has",
                        " to",
                        " be",
                        " levied",
                        ",",
                        " it",
                        " should",
                        " be",
                        " levied",
                        " on",
                        " the",
                        " Central",
                        " and",
                        " state",
                        " governments",
                        " and",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 33.64354705810547
        },
        {
            "feature_index": 3199,
            "gpt_predictions": [
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "dates in the year 2018",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " 2015 second-round pick and either their 2018 or 2019 second-round pick to Brooklyn",
                    "max_token": " 2018",
                    "tokens": [
                        " 2015",
                        " second",
                        "-",
                        "round",
                        " pick",
                        " and",
                        " either",
                        " their",
                        " 2018",
                        " or",
                        " 2019",
                        " second",
                        "-",
                        "round",
                        " pick",
                        " to",
                        " Brooklyn"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        34.00782012939453,
                        0,
                        0.0003426074981689453,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " arrive sometime during Ubisoft's fiscal 2017-2018, which translates to April 2017-March",
                    "max_token": "2018",
                    "tokens": [
                        " arrive",
                        " sometime",
                        " during",
                        " Ubisoft",
                        "'s",
                        " fiscal",
                        " 2017",
                        "-",
                        "2018",
                        ",",
                        " which",
                        " translates",
                        " to",
                        " April",
                        " 2017",
                        "-",
                        "March"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        26.34188270568848,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " will be included in a group of fiscal 2018 spending bills, combined into one measure,",
                    "max_token": " 2018",
                    "tokens": [
                        " will",
                        " be",
                        " included",
                        " in",
                        " a",
                        " group",
                        " of",
                        " fiscal",
                        " 2018",
                        " spending",
                        " bills",
                        ",",
                        " combined",
                        " into",
                        " one",
                        " measure",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        34.00791549682617,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "WEAR NO HUBRIS HERE 2018-03-20 How deep is the",
                    "max_token": " 2018",
                    "tokens": [
                        "WE",
                        "AR",
                        " NO",
                        " H",
                        "UB",
                        "R",
                        "IS",
                        " HERE",
                        " 2018",
                        "-",
                        "03",
                        "-",
                        "20",
                        " How",
                        " deep",
                        " is",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        27.97385025024414,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "NewConstMetric(0xc42018e460, 0x<|endoftext|>Hello Kickstarter",
                    "max_token": "2018",
                    "tokens": [
                        "New",
                        "Const",
                        "Met",
                        "ric",
                        "(",
                        "0",
                        "xc",
                        "4",
                        "2018",
                        "e",
                        "460",
                        ",",
                        " 0",
                        "x",
                        "<|endoftext|>",
                        "Hello",
                        " Kickstarter"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        25.67233467102051,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " most rich countries. But \u00e2\u0122\u013epremiumisation\u00e2\u0122\u013f is working. Though still",
                    "tokens": [
                        " most",
                        " rich",
                        " countries",
                        ".",
                        " But",
                        " \u00e2\u0122",
                        "\u013e",
                        "prem",
                        "ium",
                        "isation",
                        "\u00e2\u0122",
                        "\u013f",
                        " is",
                        " working",
                        ".",
                        " Though",
                        " still"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " outfit in the top 20. The Denver Nuggets reportedly spend \u00c2\u00a32,939,",
                    "tokens": [
                        " outfit",
                        " in",
                        " the",
                        " top",
                        " 20",
                        ".",
                        " The",
                        " Denver",
                        " Nuggets",
                        " reportedly",
                        " spend",
                        " \u00c2\u00a3",
                        "2",
                        ",",
                        "9",
                        "39",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "if Al-Arab al-Qathafi, where three of Colonel Gaddafi's grandchildren",
                    "tokens": [
                        "if",
                        " Al",
                        "-",
                        "Arab",
                        " al",
                        "-",
                        "Q",
                        "ath",
                        "afi",
                        ",",
                        " where",
                        " three",
                        " of",
                        " Colonel",
                        " Gaddafi",
                        "'s",
                        " grandchildren"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "2 14 Lubbock +4 20 Denver +2 23 Portland +2 7 Birmingham",
                    "tokens": [
                        "2",
                        " 14",
                        " L",
                        "ubb",
                        "ock",
                        " +",
                        "4",
                        " 20",
                        " Denver",
                        " +",
                        "2",
                        " 23",
                        " Portland",
                        " +",
                        "2",
                        " 7",
                        " Birmingham"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " has to be levied, it should be levied on the Central and state governments and the",
                    "tokens": [
                        " has",
                        " to",
                        " be",
                        " levied",
                        ",",
                        " it",
                        " should",
                        " be",
                        " levied",
                        " on",
                        " the",
                        " Central",
                        " and",
                        " state",
                        " governments",
                        " and",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 34.97655487060547
        },
        {
            "feature_index": 1757,
            "gpt_predictions": [
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "names of individuals, particularly focusing on the name \"Hayes\" with varying degrees of activation",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " members who stayed true to himself, and Harmon plucked him from the audience to become",
                    "max_token": " Harmon",
                    "tokens": [
                        " members",
                        " who",
                        " stayed",
                        " true",
                        " to",
                        " himself",
                        ",",
                        " and",
                        " Harmon",
                        " pl",
                        "ucked",
                        " him",
                        " from",
                        " the",
                        " audience",
                        " to",
                        " become"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        5.665621280670166,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " straight red card for Aberdeen winger Jonny Hayes when the score was 1-1,",
                    "max_token": " Hayes",
                    "tokens": [
                        " straight",
                        " red",
                        " card",
                        " for",
                        " Aberdeen",
                        " winger",
                        " Jon",
                        "ny",
                        " Hayes",
                        " when",
                        " the",
                        " score",
                        " was",
                        " 1",
                        "-",
                        "1",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        43.9275016784668,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "You all pitched in thousands of dollars to Hayes\u00e2\u0122\u013b recovery fund, which is currently",
                    "max_token": " Hayes",
                    "tokens": [
                        "You",
                        " all",
                        " pitched",
                        " in",
                        " thousands",
                        " of",
                        " dollars",
                        " to",
                        " Hayes",
                        "\u00e2\u0122",
                        "\u013b",
                        " recovery",
                        " fund",
                        ",",
                        " which",
                        " is",
                        " currently"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        44.70268630981445,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " missing,\" Hayley said.\u010a\u010aHayley eventually married her childhood friend Sam,",
                    "max_token": "Hay",
                    "tokens": [
                        " missing",
                        ",\"",
                        " Hay",
                        "ley",
                        " said",
                        ".",
                        "\u010a",
                        "\u010a",
                        "Hay",
                        "ley",
                        " eventually",
                        " married",
                        " her",
                        " childhood",
                        " friend",
                        " Sam",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        7.532956600189209,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " back and<|endoftext|> 11, 2016\u010a\u010aHayne\u00e2\u0122\u013bs potential return to the",
                    "max_token": "Hay",
                    "tokens": [
                        " back",
                        " and",
                        "<|endoftext|>",
                        " 11",
                        ",",
                        " 2016",
                        "\u010a",
                        "\u010a",
                        "Hay",
                        "ne",
                        "\u00e2\u0122",
                        "\u013b",
                        "s",
                        " potential",
                        " return",
                        " to",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        8.142264366149902,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " most rich countries. But \u00e2\u0122\u013epremiumisation\u00e2\u0122\u013f is working. Though still",
                    "tokens": [
                        " most",
                        " rich",
                        " countries",
                        ".",
                        " But",
                        " \u00e2\u0122",
                        "\u013e",
                        "prem",
                        "ium",
                        "isation",
                        "\u00e2\u0122",
                        "\u013f",
                        " is",
                        " working",
                        ".",
                        " Though",
                        " still"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " outfit in the top 20. The Denver Nuggets reportedly spend \u00c2\u00a32,939,",
                    "tokens": [
                        " outfit",
                        " in",
                        " the",
                        " top",
                        " 20",
                        ".",
                        " The",
                        " Denver",
                        " Nuggets",
                        " reportedly",
                        " spend",
                        " \u00c2\u00a3",
                        "2",
                        ",",
                        "9",
                        "39",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "if Al-Arab al-Qathafi, where three of Colonel Gaddafi's grandchildren",
                    "tokens": [
                        "if",
                        " Al",
                        "-",
                        "Arab",
                        " al",
                        "-",
                        "Q",
                        "ath",
                        "afi",
                        ",",
                        " where",
                        " three",
                        " of",
                        " Colonel",
                        " Gaddafi",
                        "'s",
                        " grandchildren"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "2 14 Lubbock +4 20 Denver +2 23 Portland +2 7 Birmingham",
                    "tokens": [
                        "2",
                        " 14",
                        " L",
                        "ubb",
                        "ock",
                        " +",
                        "4",
                        " 20",
                        " Denver",
                        " +",
                        "2",
                        " 23",
                        " Portland",
                        " +",
                        "2",
                        " 7",
                        " Birmingham"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " has to be levied, it should be levied on the Central and state governments and the",
                    "tokens": [
                        " has",
                        " to",
                        " be",
                        " levied",
                        ",",
                        " it",
                        " should",
                        " be",
                        " levied",
                        " on",
                        " the",
                        " Central",
                        " and",
                        " state",
                        " governments",
                        " and",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 46.51987838745117
        },
        {
            "feature_index": 2128,
            "gpt_predictions": [
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "phrases related to a specific unique identifier, like a product code or abbreviation",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "\u010aTRU Exclusive\u010a\u010aLocation: TRU\u010a\u010aRelease date: May 2017",
                    "max_token": " TR",
                    "tokens": [
                        "\u010a",
                        "TR",
                        "U",
                        " Exclusive",
                        "\u010a",
                        "\u010a",
                        "Location",
                        ":",
                        " TR",
                        "U",
                        "\u010a",
                        "\u010a",
                        "Release",
                        " date",
                        ":",
                        " May",
                        " 2017"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        8.181246757507324,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " time will tell. (And thanks to TR reader SH SOTN for the link",
                    "max_token": " TR",
                    "tokens": [
                        " time",
                        " will",
                        " tell",
                        ".",
                        " (",
                        "And",
                        " thanks",
                        " to",
                        " TR",
                        " reader",
                        " SH",
                        " S",
                        "OT",
                        "N",
                        " for",
                        " the",
                        " link"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        8.611254692077637,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " def on YouTube. The video is titled TRON: The Next Day - Flynn Lives",
                    "max_token": " TR",
                    "tokens": [
                        " def",
                        " on",
                        " YouTube",
                        ".",
                        " The",
                        " video",
                        " is",
                        " titled",
                        " TR",
                        "ON",
                        ":",
                        " The",
                        " Next",
                        " Day",
                        " -",
                        " Flynn",
                        " Lives"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        9.0370454788208,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "\u010a\u010aI was already moved off of HC due to the other issues and now this",
                    "max_token": " HC",
                    "tokens": [
                        "\u010a",
                        "\u010a",
                        "I",
                        " was",
                        " already",
                        " moved",
                        " off",
                        " of",
                        " HC",
                        " due",
                        " to",
                        " the",
                        " other",
                        " issues",
                        " and",
                        " now",
                        " this"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        3.814940214157104,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": ", Joe Caramagna\u010a\u010aThe Omega Effect shakes up the typical Daredevil/P",
                    "max_token": " Omega",
                    "tokens": [
                        ",",
                        " Joe",
                        " Car",
                        "am",
                        "agna",
                        "\u010a",
                        "\u010a",
                        "The",
                        " Omega",
                        " Effect",
                        " shakes",
                        " up",
                        " the",
                        " typical",
                        " Daredevil",
                        "/",
                        "P"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        5.95753812789917,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " most rich countries. But \u00e2\u0122\u013epremiumisation\u00e2\u0122\u013f is working. Though still",
                    "tokens": [
                        " most",
                        " rich",
                        " countries",
                        ".",
                        " But",
                        " \u00e2\u0122",
                        "\u013e",
                        "prem",
                        "ium",
                        "isation",
                        "\u00e2\u0122",
                        "\u013f",
                        " is",
                        " working",
                        ".",
                        " Though",
                        " still"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " outfit in the top 20. The Denver Nuggets reportedly spend \u00c2\u00a32,939,",
                    "tokens": [
                        " outfit",
                        " in",
                        " the",
                        " top",
                        " 20",
                        ".",
                        " The",
                        " Denver",
                        " Nuggets",
                        " reportedly",
                        " spend",
                        " \u00c2\u00a3",
                        "2",
                        ",",
                        "9",
                        "39",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "if Al-Arab al-Qathafi, where three of Colonel Gaddafi's grandchildren",
                    "tokens": [
                        "if",
                        " Al",
                        "-",
                        "Arab",
                        " al",
                        "-",
                        "Q",
                        "ath",
                        "afi",
                        ",",
                        " where",
                        " three",
                        " of",
                        " Colonel",
                        " Gaddafi",
                        "'s",
                        " grandchildren"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "2 14 Lubbock +4 20 Denver +2 23 Portland +2 7 Birmingham",
                    "tokens": [
                        "2",
                        " 14",
                        " L",
                        "ubb",
                        "ock",
                        " +",
                        "4",
                        " 20",
                        " Denver",
                        " +",
                        "2",
                        " 23",
                        " Portland",
                        " +",
                        "2",
                        " 7",
                        " Birmingham"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " has to be levied, it should be levied on the Central and state governments and the",
                    "tokens": [
                        " has",
                        " to",
                        " be",
                        " levied",
                        ",",
                        " it",
                        " should",
                        " be",
                        " levied",
                        " on",
                        " the",
                        " Central",
                        " and",
                        " state",
                        " governments",
                        " and",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 23.26931190490723
        },
        {
            "feature_index": 23103,
            "gpt_predictions": [
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    0,
                    1.0
                ],
                [
                    0,
                    1.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "words containing the substring \"Ver\"",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "out for a simple CPU, preferably in Verilog, in the Goldilocks",
                    "max_token": " Ver",
                    "tokens": [
                        "out",
                        " for",
                        " a",
                        " simple",
                        " CPU",
                        ",",
                        " preferably",
                        " in",
                        " Ver",
                        "il",
                        "og",
                        ",",
                        " in",
                        " the",
                        " Gold",
                        "il",
                        "ocks"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        43.39163589477539,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " perfect illustration<|endoftext|> voter fraud exposed by Project Veritas and, well\u00e2\u0122\u00a6nothing.\u010a",
                    "max_token": " Ver",
                    "tokens": [
                        " perfect",
                        " illustration",
                        "<|endoftext|>",
                        " voter",
                        " fraud",
                        " exposed",
                        " by",
                        " Project",
                        " Ver",
                        "itas",
                        " and",
                        ",",
                        " well",
                        "\u00e2\u0122\u00a6",
                        "nothing",
                        ".",
                        "\u010a"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        43.9980583190918,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " a statement that, \u00e2\u0122\u013eif Officer Verduzco is found guilty of what",
                    "max_token": " Ver",
                    "tokens": [
                        " a",
                        " statement",
                        " that",
                        ",",
                        " \u00e2\u0122",
                        "\u013e",
                        "if",
                        " Officer",
                        " Ver",
                        "du",
                        "z",
                        "co",
                        " is",
                        " found",
                        " guilty",
                        " of",
                        " what"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        43.31709289550781,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 5,
                    "sentence_string": "tex 450 Vertex 4 Vertex 3 Vertex 2 Vertex RevoDrive Rev",
                    "max_token": " Ver",
                    "tokens": [
                        "tex",
                        " 450",
                        " Ver",
                        "tex",
                        " 4",
                        " Ver",
                        "tex",
                        " 3",
                        " Ver",
                        "tex",
                        " 2",
                        " Ver",
                        "tex",
                        " Rev",
                        "o",
                        "Drive",
                        " Rev"
                    ],
                    "values": [
                        0,
                        0,
                        39.3089599609375,
                        0,
                        0,
                        39.37996292114258,
                        0,
                        0,
                        39.03812789916992,
                        0,
                        0,
                        38.75064849853516,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " film\u00e2\u0122\u0136\u010a\u010aMonsieur Verdoux is hardly a romantic comedy though.",
                    "max_token": " Verd",
                    "tokens": [
                        " film",
                        "\u00e2\u0122\u0136",
                        "\u010a",
                        "\u010a",
                        "M",
                        "ons",
                        "ie",
                        "ur",
                        " Verd",
                        "oux",
                        " is",
                        " hardly",
                        " a",
                        " romantic",
                        " comedy",
                        " though",
                        "."
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        7.621052742004395,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " most rich countries. But \u00e2\u0122\u013epremiumisation\u00e2\u0122\u013f is working. Though still",
                    "tokens": [
                        " most",
                        " rich",
                        " countries",
                        ".",
                        " But",
                        " \u00e2\u0122",
                        "\u013e",
                        "prem",
                        "ium",
                        "isation",
                        "\u00e2\u0122",
                        "\u013f",
                        " is",
                        " working",
                        ".",
                        " Though",
                        " still"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " outfit in the top 20. The Denver Nuggets reportedly spend \u00c2\u00a32,939,",
                    "tokens": [
                        " outfit",
                        " in",
                        " the",
                        " top",
                        " 20",
                        ".",
                        " The",
                        " Denver",
                        " Nuggets",
                        " reportedly",
                        " spend",
                        " \u00c2\u00a3",
                        "2",
                        ",",
                        "9",
                        "39",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "if Al-Arab al-Qathafi, where three of Colonel Gaddafi's grandchildren",
                    "tokens": [
                        "if",
                        " Al",
                        "-",
                        "Arab",
                        " al",
                        "-",
                        "Q",
                        "ath",
                        "afi",
                        ",",
                        " where",
                        " three",
                        " of",
                        " Colonel",
                        " Gaddafi",
                        "'s",
                        " grandchildren"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "2 14 Lubbock +4 20 Denver +2 23 Portland +2 7 Birmingham",
                    "tokens": [
                        "2",
                        " 14",
                        " L",
                        "ubb",
                        "ock",
                        " +",
                        "4",
                        " 20",
                        " Denver",
                        " +",
                        "2",
                        " 23",
                        " Portland",
                        " +",
                        "2",
                        " 7",
                        " Birmingham"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " has to be levied, it should be levied on the Central and state governments and the",
                    "tokens": [
                        " has",
                        " to",
                        " be",
                        " levied",
                        ",",
                        " it",
                        " should",
                        " be",
                        " levied",
                        " on",
                        " the",
                        " Central",
                        " and",
                        " state",
                        " governments",
                        " and",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 45.59708404541016
        },
        {
            "feature_index": 3335,
            "gpt_predictions": [
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "phrases related to trade unions and trade-related activities",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " in that Heavy-Acid-Blues genre Radio Moscow started some years ago.",
                    "max_token": "ues",
                    "tokens": [
                        " in",
                        " that",
                        " Heavy",
                        "-",
                        "Ac",
                        "id",
                        "-",
                        "Bl",
                        "ues",
                        " genre",
                        " Radio",
                        " Moscow",
                        " started",
                        " some",
                        " years",
                        " ago",
                        "."
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        5.176676273345947,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " his team.\u010a\u010aUnfortunately for Renegades, Aleksi \"allu\"",
                    "max_token": "ades",
                    "tokens": [
                        " his",
                        " team",
                        ".",
                        "\u010a",
                        "\u010a",
                        "Unfortunately",
                        " for",
                        " Reneg",
                        "ades",
                        ",",
                        " Ale",
                        "ks",
                        "i",
                        " \"",
                        "all",
                        "u",
                        "\""
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        12.6120138168335,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " of Obamacare could well be the issue that swings a critical mass of voters to say \u00e2\u0122",
                    "max_token": " swings",
                    "tokens": [
                        " of",
                        " Obamacare",
                        " could",
                        " well",
                        " be",
                        " the",
                        " issue",
                        " that",
                        " swings",
                        " a",
                        " critical",
                        " mass",
                        " of",
                        " voters",
                        " to",
                        " say",
                        " \u00e2\u0122"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        3.162581205368042,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " event\u00e2\u0122\u013f for the fragile Everglades that is threatening scores of endangered species.",
                    "max_token": "ades",
                    "tokens": [
                        " event",
                        "\u00e2\u0122",
                        "\u013f",
                        " for",
                        " the",
                        " fragile",
                        " Ever",
                        "gl",
                        "ades",
                        " that",
                        " is",
                        " threatening",
                        " scores",
                        " of",
                        " endangered",
                        " species",
                        "."
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        13.44363784790039,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " responsibility is to protect the global Everglades and all 69 endangered species,\u00e2\u0122\u013f",
                    "max_token": "ades",
                    "tokens": [
                        " responsibility",
                        " is",
                        " to",
                        " protect",
                        " the",
                        " global",
                        " Ever",
                        "gl",
                        "ades",
                        " and",
                        " all",
                        " 69",
                        " endangered",
                        " species",
                        ",",
                        "\u00e2\u0122",
                        "\u013f"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        13.16483592987061,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " most rich countries. But \u00e2\u0122\u013epremiumisation\u00e2\u0122\u013f is working. Though still",
                    "tokens": [
                        " most",
                        " rich",
                        " countries",
                        ".",
                        " But",
                        " \u00e2\u0122",
                        "\u013e",
                        "prem",
                        "ium",
                        "isation",
                        "\u00e2\u0122",
                        "\u013f",
                        " is",
                        " working",
                        ".",
                        " Though",
                        " still"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " outfit in the top 20. The Denver Nuggets reportedly spend \u00c2\u00a32,939,",
                    "tokens": [
                        " outfit",
                        " in",
                        " the",
                        " top",
                        " 20",
                        ".",
                        " The",
                        " Denver",
                        " Nuggets",
                        " reportedly",
                        " spend",
                        " \u00c2\u00a3",
                        "2",
                        ",",
                        "9",
                        "39",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "if Al-Arab al-Qathafi, where three of Colonel Gaddafi's grandchildren",
                    "tokens": [
                        "if",
                        " Al",
                        "-",
                        "Arab",
                        " al",
                        "-",
                        "Q",
                        "ath",
                        "afi",
                        ",",
                        " where",
                        " three",
                        " of",
                        " Colonel",
                        " Gaddafi",
                        "'s",
                        " grandchildren"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "2 14 Lubbock +4 20 Denver +2 23 Portland +2 7 Birmingham",
                    "tokens": [
                        "2",
                        " 14",
                        " L",
                        "ubb",
                        "ock",
                        " +",
                        "4",
                        " 20",
                        " Denver",
                        " +",
                        "2",
                        " 23",
                        " Portland",
                        " +",
                        "2",
                        " 7",
                        " Birmingham"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " has to be levied, it should be levied on the Central and state governments and the",
                    "tokens": [
                        " has",
                        " to",
                        " be",
                        " levied",
                        ",",
                        " it",
                        " should",
                        " be",
                        " levied",
                        " on",
                        " the",
                        " Central",
                        " and",
                        " state",
                        " governments",
                        " and",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 14.61572647094727
        },
        {
            "feature_index": 14112,
            "gpt_predictions": [
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "terms related to political geography, especially related to states and territories",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " effect. Officials expect fierce disagreements among member states and MEPs as they debate the draft",
                    "max_token": " states",
                    "tokens": [
                        " effect",
                        ".",
                        " Officials",
                        " expect",
                        " fierce",
                        " disagreements",
                        " among",
                        " member",
                        " states",
                        " and",
                        " MEP",
                        "s",
                        " as",
                        " they",
                        " debate",
                        " the",
                        " draft"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        7.846203327178955,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": ". Just after the Bolshevik revolution, the state used innovative policies to approach the Russian Empire",
                    "max_token": " state",
                    "tokens": [
                        ".",
                        " Just",
                        " after",
                        " the",
                        " Bolshevik",
                        " revolution",
                        ",",
                        " the",
                        " state",
                        " used",
                        " innovative",
                        " policies",
                        " to",
                        " approach",
                        " the",
                        " Russian",
                        " Empire"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        14.3074369430542,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "\u010a\u010aA chief feature of the administrative state is its relentless centralization, but with",
                    "max_token": " state",
                    "tokens": [
                        "\u010a",
                        "\u010a",
                        "A",
                        " chief",
                        " feature",
                        " of",
                        " the",
                        " administrative",
                        " state",
                        " is",
                        " its",
                        " relentless",
                        " central",
                        "ization",
                        ",",
                        " but",
                        " with"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        5.421022891998291,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " secure the creditor banks against the consequences of state bankruptcy.\u010a\u010aThe change of course",
                    "max_token": " state",
                    "tokens": [
                        " secure",
                        " the",
                        " creditor",
                        " banks",
                        " against",
                        " the",
                        " consequences",
                        " of",
                        " state",
                        " bankruptcy",
                        ".",
                        "\u010a",
                        "\u010a",
                        "The",
                        " change",
                        " of",
                        " course"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        11.45354175567627,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " other Kurdish forces as well as the Turkish state inevitably has left many of the Turkish left",
                    "max_token": " state",
                    "tokens": [
                        " other",
                        " Kurdish",
                        " forces",
                        " as",
                        " well",
                        " as",
                        " the",
                        " Turkish",
                        " state",
                        " inevitably",
                        " has",
                        " left",
                        " many",
                        " of",
                        " the",
                        " Turkish",
                        " left"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        15.82731628417969,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " most rich countries. But \u00e2\u0122\u013epremiumisation\u00e2\u0122\u013f is working. Though still",
                    "tokens": [
                        " most",
                        " rich",
                        " countries",
                        ".",
                        " But",
                        " \u00e2\u0122",
                        "\u013e",
                        "prem",
                        "ium",
                        "isation",
                        "\u00e2\u0122",
                        "\u013f",
                        " is",
                        " working",
                        ".",
                        " Though",
                        " still"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " outfit in the top 20. The Denver Nuggets reportedly spend \u00c2\u00a32,939,",
                    "tokens": [
                        " outfit",
                        " in",
                        " the",
                        " top",
                        " 20",
                        ".",
                        " The",
                        " Denver",
                        " Nuggets",
                        " reportedly",
                        " spend",
                        " \u00c2\u00a3",
                        "2",
                        ",",
                        "9",
                        "39",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "if Al-Arab al-Qathafi, where three of Colonel Gaddafi's grandchildren",
                    "tokens": [
                        "if",
                        " Al",
                        "-",
                        "Arab",
                        " al",
                        "-",
                        "Q",
                        "ath",
                        "afi",
                        ",",
                        " where",
                        " three",
                        " of",
                        " Colonel",
                        " Gaddafi",
                        "'s",
                        " grandchildren"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "2 14 Lubbock +4 20 Denver +2 23 Portland +2 7 Birmingham",
                    "tokens": [
                        "2",
                        " 14",
                        " L",
                        "ubb",
                        "ock",
                        " +",
                        "4",
                        " 20",
                        " Denver",
                        " +",
                        "2",
                        " 23",
                        " Portland",
                        " +",
                        "2",
                        " 7",
                        " Birmingham"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " has to be levied, it should be levied on the Central and state governments and the",
                    "tokens": [
                        " has",
                        " to",
                        " be",
                        " levied",
                        ",",
                        " it",
                        " should",
                        " be",
                        " levied",
                        " on",
                        " the",
                        " Central",
                        " and",
                        " state",
                        " governments",
                        " and",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 20.06779479980469
        },
        {
            "feature_index": 20178,
            "gpt_predictions": [
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "phrases related to using something",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " because a) Hillary Clinton was no longer using the email address at the time the emails",
                    "max_token": " using",
                    "tokens": [
                        " because",
                        " a",
                        ")",
                        " Hillary",
                        " Clinton",
                        " was",
                        " no",
                        " longer",
                        " using",
                        " the",
                        " email",
                        " address",
                        " at",
                        " the",
                        " time",
                        " the",
                        " emails"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        23.16133689880371,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " Well, it means that he only speaks using hash tag language and, for the past",
                    "max_token": " using",
                    "tokens": [
                        " Well",
                        ",",
                        " it",
                        " means",
                        " that",
                        " he",
                        " only",
                        " speaks",
                        " using",
                        " hash",
                        " tag",
                        " language",
                        " and",
                        ",",
                        " for",
                        " the",
                        " past"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        25.00118446350098,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " again, they depict testosterone junkies defusing IEDs, battling terrorists and themselves",
                    "max_token": "using",
                    "tokens": [
                        " again",
                        ",",
                        " they",
                        " depict",
                        " testosterone",
                        " junk",
                        "ies",
                        " def",
                        "using",
                        " I",
                        "ED",
                        "s",
                        ",",
                        " battling",
                        " terrorists",
                        " and",
                        " themselves"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        6.2798171043396,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " personnel today kill real people in distant countries using remotely piloted drones, on interfaces modeled",
                    "max_token": " using",
                    "tokens": [
                        " personnel",
                        " today",
                        " kill",
                        " real",
                        " people",
                        " in",
                        " distant",
                        " countries",
                        " using",
                        " remotely",
                        " pil",
                        "oted",
                        " drones",
                        ",",
                        " on",
                        " interfaces",
                        " modeled"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        26.29989433288574,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " waters<|endoftext|> Minister Benjamin Netanyahu's cabinet approve using any means necessary to stop the flot",
                    "max_token": " using",
                    "tokens": [
                        " waters",
                        "<|endoftext|>",
                        " Minister",
                        " Benjamin",
                        " Netanyahu",
                        "'s",
                        " cabinet",
                        " approve",
                        " using",
                        " any",
                        " means",
                        " necessary",
                        " to",
                        " stop",
                        " the",
                        " fl",
                        "ot"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        26.19907760620117,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " most rich countries. But \u00e2\u0122\u013epremiumisation\u00e2\u0122\u013f is working. Though still",
                    "tokens": [
                        " most",
                        " rich",
                        " countries",
                        ".",
                        " But",
                        " \u00e2\u0122",
                        "\u013e",
                        "prem",
                        "ium",
                        "isation",
                        "\u00e2\u0122",
                        "\u013f",
                        " is",
                        " working",
                        ".",
                        " Though",
                        " still"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " outfit in the top 20. The Denver Nuggets reportedly spend \u00c2\u00a32,939,",
                    "tokens": [
                        " outfit",
                        " in",
                        " the",
                        " top",
                        " 20",
                        ".",
                        " The",
                        " Denver",
                        " Nuggets",
                        " reportedly",
                        " spend",
                        " \u00c2\u00a3",
                        "2",
                        ",",
                        "9",
                        "39",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "if Al-Arab al-Qathafi, where three of Colonel Gaddafi's grandchildren",
                    "tokens": [
                        "if",
                        " Al",
                        "-",
                        "Arab",
                        " al",
                        "-",
                        "Q",
                        "ath",
                        "afi",
                        ",",
                        " where",
                        " three",
                        " of",
                        " Colonel",
                        " Gaddafi",
                        "'s",
                        " grandchildren"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "2 14 Lubbock +4 20 Denver +2 23 Portland +2 7 Birmingham",
                    "tokens": [
                        "2",
                        " 14",
                        " L",
                        "ubb",
                        "ock",
                        " +",
                        "4",
                        " 20",
                        " Denver",
                        " +",
                        "2",
                        " 23",
                        " Portland",
                        " +",
                        "2",
                        " 7",
                        " Birmingham"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " has to be levied, it should be levied on the Central and state governments and the",
                    "tokens": [
                        " has",
                        " to",
                        " be",
                        " levied",
                        ",",
                        " it",
                        " should",
                        " be",
                        " levied",
                        " on",
                        " the",
                        " Central",
                        " and",
                        " state",
                        " governments",
                        " and",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 26.85873222351074
        },
        {
            "feature_index": 16498,
            "gpt_predictions": [
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "the name \"Silva\" with different variations and contexts",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " supermarket in Fort Lupton gave Mareena Silva the drug methotrexate last week",
                    "max_token": " Silva",
                    "tokens": [
                        " supermarket",
                        " in",
                        " Fort",
                        " Lu",
                        "pton",
                        " gave",
                        " Mare",
                        "ena",
                        " Silva",
                        " the",
                        " drug",
                        " met",
                        "hot",
                        "rex",
                        "ate",
                        " last",
                        " week"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        39.71578216552734,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " looks like \u00e2\u0122\u013eWanderlei Silva in a dress\u00e2\u0122\u013f \u00e2\u0122\u0135 the UFC",
                    "max_token": " Silva",
                    "tokens": [
                        " looks",
                        " like",
                        " \u00e2\u0122",
                        "\u013e",
                        "W",
                        "ander",
                        "le",
                        "i",
                        " Silva",
                        " in",
                        " a",
                        " dress",
                        "\u00e2\u0122",
                        "\u013f",
                        " \u00e2\u0122\u0135",
                        " the",
                        " UFC"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        40.87135696411133,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "\u013eHe was depressed afterward,\u00e2\u0122\u013f Silva recalls. \u00e2\u0122\u013eHe kept saying he",
                    "max_token": " Silva",
                    "tokens": [
                        "\u013e",
                        "He",
                        " was",
                        " depressed",
                        " afterward",
                        ",",
                        "\u00e2\u0122",
                        "\u013f",
                        " Silva",
                        " recalls",
                        ".",
                        " \u00e2\u0122",
                        "\u013e",
                        "He",
                        " kept",
                        " saying",
                        " he"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        41.26698303222656,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " terminate early-stage pregnancies.\u010a\u010aSilva, who is six weeks pregnant,",
                    "max_token": "Sil",
                    "tokens": [
                        " terminate",
                        " early",
                        "-",
                        "stage",
                        " pregnancies",
                        ".",
                        "\u010a",
                        "\u010a",
                        "Sil",
                        "va",
                        ",",
                        " who",
                        " is",
                        " six",
                        " weeks",
                        " pregnant",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        4.686852931976318,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "ary Robinson (USA); Olabisi Silva (Nigeria/UK).\u010a",
                    "max_token": " Silva",
                    "tokens": [
                        "ary",
                        " Robinson",
                        " (",
                        "USA",
                        ");",
                        " Ol",
                        "abis",
                        "i",
                        " Silva",
                        " (",
                        "N",
                        "ig",
                        "eria",
                        "/",
                        "UK",
                        ").",
                        "\u010a"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        39.39495849609375,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " most rich countries. But \u00e2\u0122\u013epremiumisation\u00e2\u0122\u013f is working. Though still",
                    "tokens": [
                        " most",
                        " rich",
                        " countries",
                        ".",
                        " But",
                        " \u00e2\u0122",
                        "\u013e",
                        "prem",
                        "ium",
                        "isation",
                        "\u00e2\u0122",
                        "\u013f",
                        " is",
                        " working",
                        ".",
                        " Though",
                        " still"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " outfit in the top 20. The Denver Nuggets reportedly spend \u00c2\u00a32,939,",
                    "tokens": [
                        " outfit",
                        " in",
                        " the",
                        " top",
                        " 20",
                        ".",
                        " The",
                        " Denver",
                        " Nuggets",
                        " reportedly",
                        " spend",
                        " \u00c2\u00a3",
                        "2",
                        ",",
                        "9",
                        "39",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "if Al-Arab al-Qathafi, where three of Colonel Gaddafi's grandchildren",
                    "tokens": [
                        "if",
                        " Al",
                        "-",
                        "Arab",
                        " al",
                        "-",
                        "Q",
                        "ath",
                        "afi",
                        ",",
                        " where",
                        " three",
                        " of",
                        " Colonel",
                        " Gaddafi",
                        "'s",
                        " grandchildren"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "2 14 Lubbock +4 20 Denver +2 23 Portland +2 7 Birmingham",
                    "tokens": [
                        "2",
                        " 14",
                        " L",
                        "ubb",
                        "ock",
                        " +",
                        "4",
                        " 20",
                        " Denver",
                        " +",
                        "2",
                        " 23",
                        " Portland",
                        " +",
                        "2",
                        " 7",
                        " Birmingham"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " has to be levied, it should be levied on the Central and state governments and the",
                    "tokens": [
                        " has",
                        " to",
                        " be",
                        " levied",
                        ",",
                        " it",
                        " should",
                        " be",
                        " levied",
                        " on",
                        " the",
                        " Central",
                        " and",
                        " state",
                        " governments",
                        " and",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 43.78973770141602
        },
        {
            "feature_index": 16092,
            "gpt_predictions": [
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "words related to advice or consultancy",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " try anything.\u00e2\u0122\u013f<|endoftext|>Abu Dhabi: Credit card fees charged at petrol stations",
                    "max_token": " Dhabi",
                    "tokens": [
                        " try",
                        " anything",
                        ".",
                        "\u00e2\u0122",
                        "\u013f",
                        "<|endoftext|>",
                        "Ab",
                        "u",
                        " Dhabi",
                        ":",
                        " Credit",
                        " card",
                        " fees",
                        " charged",
                        " at",
                        " petrol",
                        " stations"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        7.171017646789551,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " to the governor\u00e2\u0122\u013bs chief policy advisor, Lauren Kintner. He has",
                    "max_token": " advisor",
                    "tokens": [
                        " to",
                        " the",
                        " governor",
                        "\u00e2\u0122",
                        "\u013b",
                        "s",
                        " chief",
                        " policy",
                        " advisor",
                        ",",
                        " Lauren",
                        " K",
                        "int",
                        "ner",
                        ".",
                        " He",
                        " has"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        10.39851570129395,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " news conference with John King Jr., senior advisor at the U.S. Department of",
                    "max_token": " advisor",
                    "tokens": [
                        " news",
                        " conference",
                        " with",
                        " John",
                        " King",
                        " Jr",
                        ".,",
                        " senior",
                        " advisor",
                        " at",
                        " the",
                        " U",
                        ".",
                        "S",
                        ".",
                        " Department",
                        " of"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        10.29196357727051,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " crisis drafted by Lawrence Summers and other economic advisors.\u010a\u010aThe memo informed the president",
                    "max_token": " advisors",
                    "tokens": [
                        " crisis",
                        " drafted",
                        " by",
                        " Lawrence",
                        " Summers",
                        " and",
                        " other",
                        " economic",
                        " advisors",
                        ".",
                        "\u010a",
                        "\u010a",
                        "The",
                        " memo",
                        " informed",
                        " the",
                        " president"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        12.85723686218262,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " general manager\u010a\u010aDoug Collins \u00e2\u0122\u0136 senior advisor of basketball operations\u010a\u010aBrian Hagen",
                    "max_token": " advisor",
                    "tokens": [
                        " general",
                        " manager",
                        "\u010a",
                        "\u010a",
                        "Doug",
                        " Collins",
                        " \u00e2\u0122\u0136",
                        " senior",
                        " advisor",
                        " of",
                        " basketball",
                        " operations",
                        "\u010a",
                        "\u010a",
                        "Brian",
                        " H",
                        "agen"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        10.74002838134766,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " most rich countries. But \u00e2\u0122\u013epremiumisation\u00e2\u0122\u013f is working. Though still",
                    "tokens": [
                        " most",
                        " rich",
                        " countries",
                        ".",
                        " But",
                        " \u00e2\u0122",
                        "\u013e",
                        "prem",
                        "ium",
                        "isation",
                        "\u00e2\u0122",
                        "\u013f",
                        " is",
                        " working",
                        ".",
                        " Though",
                        " still"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " outfit in the top 20. The Denver Nuggets reportedly spend \u00c2\u00a32,939,",
                    "tokens": [
                        " outfit",
                        " in",
                        " the",
                        " top",
                        " 20",
                        ".",
                        " The",
                        " Denver",
                        " Nuggets",
                        " reportedly",
                        " spend",
                        " \u00c2\u00a3",
                        "2",
                        ",",
                        "9",
                        "39",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "if Al-Arab al-Qathafi, where three of Colonel Gaddafi's grandchildren",
                    "tokens": [
                        "if",
                        " Al",
                        "-",
                        "Arab",
                        " al",
                        "-",
                        "Q",
                        "ath",
                        "afi",
                        ",",
                        " where",
                        " three",
                        " of",
                        " Colonel",
                        " Gaddafi",
                        "'s",
                        " grandchildren"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "2 14 Lubbock +4 20 Denver +2 23 Portland +2 7 Birmingham",
                    "tokens": [
                        "2",
                        " 14",
                        " L",
                        "ubb",
                        "ock",
                        " +",
                        "4",
                        " 20",
                        " Denver",
                        " +",
                        "2",
                        " 23",
                        " Portland",
                        " +",
                        "2",
                        " 7",
                        " Birmingham"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " has to be levied, it should be levied on the Central and state governments and the",
                    "tokens": [
                        " has",
                        " to",
                        " be",
                        " levied",
                        ",",
                        " it",
                        " should",
                        " be",
                        " levied",
                        " on",
                        " the",
                        " Central",
                        " and",
                        " state",
                        " governments",
                        " and",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 14.9902286529541
        },
        {
            "feature_index": 3519,
            "gpt_predictions": [
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "references to trains",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "ically inserted screw in his right foot. Trainers have to stuff the right amount of",
                    "max_token": " Train",
                    "tokens": [
                        "ically",
                        " inserted",
                        " screw",
                        " in",
                        " his",
                        " right",
                        " foot",
                        ".",
                        " Train",
                        "ers",
                        " have",
                        " to",
                        " stuff",
                        " the",
                        " right",
                        " amount",
                        " of"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        16.30036926269531,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "uke, Inferno, Dust 2, and Train, just to name a few. A",
                    "max_token": " Train",
                    "tokens": [
                        "uke",
                        ",",
                        " Inferno",
                        ",",
                        " Dust",
                        " 2",
                        ",",
                        " and",
                        " Train",
                        ",",
                        " just",
                        " to",
                        " name",
                        " a",
                        " few",
                        ".",
                        " A"
                    ],
                    "values": [
                        0,
                        0,
                        2.030273675918579,
                        0,
                        0,
                        0,
                        0,
                        0,
                        17.31968879699707,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " Brown and More To Perform at 2014 Soul Train Awards]\u010a\u010aWatch the video for",
                    "max_token": " Train",
                    "tokens": [
                        " Brown",
                        " and",
                        " More",
                        " To",
                        " Perform",
                        " at",
                        " 2014",
                        " Soul",
                        " Train",
                        " Awards",
                        "]",
                        "\u010a",
                        "\u010a",
                        "Watch",
                        " the",
                        " video",
                        " for"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        1.209755778312683,
                        0,
                        0,
                        0,
                        16.20976448059082,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": ": Sao Paulo, Brazil\u010a\u010aFirst Tennis Memory: \u00e2\u0122\u013eI think I started",
                    "max_token": " Tennis",
                    "tokens": [
                        ":",
                        " Sao",
                        " Paulo",
                        ",",
                        " Brazil",
                        "\u010a",
                        "\u010a",
                        "First",
                        " Tennis",
                        " Memory",
                        ":",
                        " \u00e2\u0122",
                        "\u013e",
                        "I",
                        " think",
                        " I",
                        " started"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        3.016403913497925,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "els are a great way to house-train your new puppy (or puppies). It",
                    "max_token": "train",
                    "tokens": [
                        "els",
                        " are",
                        " a",
                        " great",
                        " way",
                        " to",
                        " house",
                        "-",
                        "train",
                        " your",
                        " new",
                        " puppy",
                        " (",
                        "or",
                        " puppies",
                        ").",
                        " It"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        7.381822109222412,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " most rich countries. But \u00e2\u0122\u013epremiumisation\u00e2\u0122\u013f is working. Though still",
                    "tokens": [
                        " most",
                        " rich",
                        " countries",
                        ".",
                        " But",
                        " \u00e2\u0122",
                        "\u013e",
                        "prem",
                        "ium",
                        "isation",
                        "\u00e2\u0122",
                        "\u013f",
                        " is",
                        " working",
                        ".",
                        " Though",
                        " still"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " outfit in the top 20. The Denver Nuggets reportedly spend \u00c2\u00a32,939,",
                    "tokens": [
                        " outfit",
                        " in",
                        " the",
                        " top",
                        " 20",
                        ".",
                        " The",
                        " Denver",
                        " Nuggets",
                        " reportedly",
                        " spend",
                        " \u00c2\u00a3",
                        "2",
                        ",",
                        "9",
                        "39",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "if Al-Arab al-Qathafi, where three of Colonel Gaddafi's grandchildren",
                    "tokens": [
                        "if",
                        " Al",
                        "-",
                        "Arab",
                        " al",
                        "-",
                        "Q",
                        "ath",
                        "afi",
                        ",",
                        " where",
                        " three",
                        " of",
                        " Colonel",
                        " Gaddafi",
                        "'s",
                        " grandchildren"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "2 14 Lubbock +4 20 Denver +2 23 Portland +2 7 Birmingham",
                    "tokens": [
                        "2",
                        " 14",
                        " L",
                        "ubb",
                        "ock",
                        " +",
                        "4",
                        " 20",
                        " Denver",
                        " +",
                        "2",
                        " 23",
                        " Portland",
                        " +",
                        "2",
                        " 7",
                        " Birmingham"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " has to be levied, it should be levied on the Central and state governments and the",
                    "tokens": [
                        " has",
                        " to",
                        " be",
                        " levied",
                        ",",
                        " it",
                        " should",
                        " be",
                        " levied",
                        " on",
                        " the",
                        " Central",
                        " and",
                        " state",
                        " governments",
                        " and",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 18.19181823730469
        },
        {
            "feature_index": 22883,
            "gpt_predictions": [
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "phrases indicating partnerships or collaborations between entities",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " You\u00e2\u0122\u013bd also hope that having partnered with IBM, Elemental Path will invest plenty",
                    "max_token": " partnered",
                    "tokens": [
                        " You",
                        "\u00e2\u0122",
                        "\u013b",
                        "d",
                        " also",
                        " hope",
                        " that",
                        " having",
                        " partnered",
                        " with",
                        " IBM",
                        ",",
                        " Elemental",
                        " Path",
                        " will",
                        " invest",
                        " plenty"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        18.5189037322998,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "\u010a\u010a\u00e2\u0122\u013eThe SDF, partnered with enabling support from U.S.",
                    "max_token": " partnered",
                    "tokens": [
                        "\u010a",
                        "\u010a",
                        "\u00e2\u0122",
                        "\u013e",
                        "The",
                        " S",
                        "DF",
                        ",",
                        " partnered",
                        " with",
                        " enabling",
                        " support",
                        " from",
                        " U",
                        ".",
                        "S",
                        "."
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        19.12094879150391,
                        0.1651315093040466,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " in March 2017.\u010a\u010aThey're partnered with the German Aerospace Center, which is",
                    "max_token": " partnered",
                    "tokens": [
                        " in",
                        " March",
                        " 2017",
                        ".",
                        "\u010a",
                        "\u010a",
                        "They",
                        "'re",
                        " partnered",
                        " with",
                        " the",
                        " German",
                        " Aerospace",
                        " Center",
                        ",",
                        " which",
                        " is"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        18.48234939575195,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "inese artist jody xiong has collaborated with 16 handicapped people \u00e2\u0122\u0136 recruited via",
                    "max_token": " collaborated",
                    "tokens": [
                        "inese",
                        " artist",
                        " j",
                        "ody",
                        " x",
                        "ion",
                        "g",
                        " has",
                        " collaborated",
                        " with",
                        " 16",
                        " handic",
                        "apped",
                        " people",
                        " \u00e2\u0122\u0136",
                        " recruited",
                        " via"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        8.342995643615723,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " disclose the relevant information, and also to collaborate with those suppliers to implement change,\" she",
                    "max_token": " collaborate",
                    "tokens": [
                        " disclose",
                        " the",
                        " relevant",
                        " information",
                        ",",
                        " and",
                        " also",
                        " to",
                        " collaborate",
                        " with",
                        " those",
                        " suppliers",
                        " to",
                        " implement",
                        " change",
                        ",\"",
                        " she"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        4.124988079071045,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " most rich countries. But \u00e2\u0122\u013epremiumisation\u00e2\u0122\u013f is working. Though still",
                    "tokens": [
                        " most",
                        " rich",
                        " countries",
                        ".",
                        " But",
                        " \u00e2\u0122",
                        "\u013e",
                        "prem",
                        "ium",
                        "isation",
                        "\u00e2\u0122",
                        "\u013f",
                        " is",
                        " working",
                        ".",
                        " Though",
                        " still"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " outfit in the top 20. The Denver Nuggets reportedly spend \u00c2\u00a32,939,",
                    "tokens": [
                        " outfit",
                        " in",
                        " the",
                        " top",
                        " 20",
                        ".",
                        " The",
                        " Denver",
                        " Nuggets",
                        " reportedly",
                        " spend",
                        " \u00c2\u00a3",
                        "2",
                        ",",
                        "9",
                        "39",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "if Al-Arab al-Qathafi, where three of Colonel Gaddafi's grandchildren",
                    "tokens": [
                        "if",
                        " Al",
                        "-",
                        "Arab",
                        " al",
                        "-",
                        "Q",
                        "ath",
                        "afi",
                        ",",
                        " where",
                        " three",
                        " of",
                        " Colonel",
                        " Gaddafi",
                        "'s",
                        " grandchildren"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "2 14 Lubbock +4 20 Denver +2 23 Portland +2 7 Birmingham",
                    "tokens": [
                        "2",
                        " 14",
                        " L",
                        "ubb",
                        "ock",
                        " +",
                        "4",
                        " 20",
                        " Denver",
                        " +",
                        "2",
                        " 23",
                        " Portland",
                        " +",
                        "2",
                        " 7",
                        " Birmingham"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " has to be levied, it should be levied on the Central and state governments and the",
                    "tokens": [
                        " has",
                        " to",
                        " be",
                        " levied",
                        ",",
                        " it",
                        " should",
                        " be",
                        " levied",
                        " on",
                        " the",
                        " Central",
                        " and",
                        " state",
                        " governments",
                        " and",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 19.70412063598633
        },
        {
            "feature_index": 23351,
            "gpt_predictions": [
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "verbs related to sweeping actions",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "box that can be used to defend, sweep your opponents, or submit them outright.",
                    "max_token": " sweep",
                    "tokens": [
                        "box",
                        " that",
                        " can",
                        " be",
                        " used",
                        " to",
                        " defend",
                        ",",
                        " sweep",
                        " your",
                        " opponents",
                        ",",
                        " or",
                        " submit",
                        " them",
                        " outright",
                        "."
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        36.07857131958008,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " chord at the root; a leading edge sweep of 1 box of chord decaying every 6",
                    "max_token": " sweep",
                    "tokens": [
                        " chord",
                        " at",
                        " the",
                        " root",
                        ";",
                        " a",
                        " leading",
                        " edge",
                        " sweep",
                        " of",
                        " 1",
                        " box",
                        " of",
                        " chord",
                        " decaying",
                        " every",
                        " 6"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        34.5970344543457,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "\u010a\u010a\"The statement says during the sweep last week, from Nov. 26 to",
                    "max_token": " sweep",
                    "tokens": [
                        "\u010a",
                        "\u010a",
                        "\"",
                        "The",
                        " statement",
                        " says",
                        " during",
                        " the",
                        " sweep",
                        " last",
                        " week",
                        ",",
                        " from",
                        " Nov",
                        ".",
                        " 26",
                        " to"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        37.20829772949219,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": ") April 5, 2015\u010a\u010aStreet Sweepers now taking over State Street after a",
                    "max_token": " Sweep",
                    "tokens": [
                        ")",
                        " April",
                        " 5",
                        ",",
                        " 2015",
                        "\u010a",
                        "\u010a",
                        "Street",
                        " Sweep",
                        "ers",
                        " now",
                        " taking",
                        " over",
                        " State",
                        " Street",
                        " after",
                        " a"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        25.6606559753418,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " knock down a moose with a paw swipe, toss rivals through the<|endoftext|>YOUT",
                    "max_token": " swipe",
                    "tokens": [
                        " knock",
                        " down",
                        " a",
                        " mo",
                        "ose",
                        " with",
                        " a",
                        " paw",
                        " swipe",
                        ",",
                        " toss",
                        " rivals",
                        " through",
                        " the",
                        "<|endoftext|>",
                        "Y",
                        "OUT"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        4.810325145721436,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " most rich countries. But \u00e2\u0122\u013epremiumisation\u00e2\u0122\u013f is working. Though still",
                    "tokens": [
                        " most",
                        " rich",
                        " countries",
                        ".",
                        " But",
                        " \u00e2\u0122",
                        "\u013e",
                        "prem",
                        "ium",
                        "isation",
                        "\u00e2\u0122",
                        "\u013f",
                        " is",
                        " working",
                        ".",
                        " Though",
                        " still"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " outfit in the top 20. The Denver Nuggets reportedly spend \u00c2\u00a32,939,",
                    "tokens": [
                        " outfit",
                        " in",
                        " the",
                        " top",
                        " 20",
                        ".",
                        " The",
                        " Denver",
                        " Nuggets",
                        " reportedly",
                        " spend",
                        " \u00c2\u00a3",
                        "2",
                        ",",
                        "9",
                        "39",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "if Al-Arab al-Qathafi, where three of Colonel Gaddafi's grandchildren",
                    "tokens": [
                        "if",
                        " Al",
                        "-",
                        "Arab",
                        " al",
                        "-",
                        "Q",
                        "ath",
                        "afi",
                        ",",
                        " where",
                        " three",
                        " of",
                        " Colonel",
                        " Gaddafi",
                        "'s",
                        " grandchildren"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "2 14 Lubbock +4 20 Denver +2 23 Portland +2 7 Birmingham",
                    "tokens": [
                        "2",
                        " 14",
                        " L",
                        "ubb",
                        "ock",
                        " +",
                        "4",
                        " 20",
                        " Denver",
                        " +",
                        "2",
                        " 23",
                        " Portland",
                        " +",
                        "2",
                        " 7",
                        " Birmingham"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " has to be levied, it should be levied on the Central and state governments and the",
                    "tokens": [
                        " has",
                        " to",
                        " be",
                        " levied",
                        ",",
                        " it",
                        " should",
                        " be",
                        " levied",
                        " on",
                        " the",
                        " Central",
                        " and",
                        " state",
                        " governments",
                        " and",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 39.12868118286133
        },
        {
            "feature_index": 1210,
            "gpt_predictions": [
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "terms related to competition and competency",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "ri, a futsal team which competes in Italy's top Serie A league",
                    "max_token": " compet",
                    "tokens": [
                        "ri",
                        ",",
                        " a",
                        " f",
                        "uts",
                        "al",
                        " team",
                        " which",
                        " compet",
                        "es",
                        " in",
                        " Italy",
                        "'s",
                        " top",
                        " Serie",
                        " A",
                        " league"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        39.02326965332031,
                        2.880811452865601,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " a chuckle.\u010a\u010aThe national team competes in a series of tours in locations",
                    "max_token": " compet",
                    "tokens": [
                        " a",
                        " chuckle",
                        ".",
                        "\u010a",
                        "\u010a",
                        "The",
                        " national",
                        " team",
                        " compet",
                        "es",
                        " in",
                        " a",
                        " series",
                        " of",
                        " tours",
                        " in",
                        " locations"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        39.95222473144531,
                        3.107820272445679,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 7,
                    "sentence_string": " loop business model that is our core competency and focus.\"\u010a\u010aThe project is",
                    "max_token": " compet",
                    "tokens": [
                        " loop",
                        " business",
                        " model",
                        " that",
                        " is",
                        " our",
                        " core",
                        " compet",
                        "ency",
                        " and",
                        " focus",
                        ".\"",
                        "\u010a",
                        "\u010a",
                        "The",
                        " project",
                        " is"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        41.54837799072266,
                        7.383108139038086,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " Competition\u010a\u010aEasily Archive Your Competitors\u00e2\u0122\u013b Emails\u010a\u010aWHAT IT",
                    "max_token": " Compet",
                    "tokens": [
                        " Competition",
                        "\u010a",
                        "\u010a",
                        "E",
                        "as",
                        "ily",
                        " Archive",
                        " Your",
                        " Compet",
                        "itors",
                        "\u00e2\u0122",
                        "\u013b",
                        " Emails",
                        "\u010a",
                        "\u010a",
                        "WHAT",
                        " IT"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        22.00620079040527,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " doc for reference.\u010a\u010aTrack Your Competitors\u00e2\u0122\u013b Website Changes\u010a\u010aWHAT",
                    "max_token": " Compet",
                    "tokens": [
                        " doc",
                        " for",
                        " reference",
                        ".",
                        "\u010a",
                        "\u010a",
                        "Track",
                        " Your",
                        " Compet",
                        "itors",
                        "\u00e2\u0122",
                        "\u013b",
                        " Website",
                        " Changes",
                        "\u010a",
                        "\u010a",
                        "WHAT"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        22.56588935852051,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " most rich countries. But \u00e2\u0122\u013epremiumisation\u00e2\u0122\u013f is working. Though still",
                    "tokens": [
                        " most",
                        " rich",
                        " countries",
                        ".",
                        " But",
                        " \u00e2\u0122",
                        "\u013e",
                        "prem",
                        "ium",
                        "isation",
                        "\u00e2\u0122",
                        "\u013f",
                        " is",
                        " working",
                        ".",
                        " Though",
                        " still"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " outfit in the top 20. The Denver Nuggets reportedly spend \u00c2\u00a32,939,",
                    "tokens": [
                        " outfit",
                        " in",
                        " the",
                        " top",
                        " 20",
                        ".",
                        " The",
                        " Denver",
                        " Nuggets",
                        " reportedly",
                        " spend",
                        " \u00c2\u00a3",
                        "2",
                        ",",
                        "9",
                        "39",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "if Al-Arab al-Qathafi, where three of Colonel Gaddafi's grandchildren",
                    "tokens": [
                        "if",
                        " Al",
                        "-",
                        "Arab",
                        " al",
                        "-",
                        "Q",
                        "ath",
                        "afi",
                        ",",
                        " where",
                        " three",
                        " of",
                        " Colonel",
                        " Gaddafi",
                        "'s",
                        " grandchildren"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "2 14 Lubbock +4 20 Denver +2 23 Portland +2 7 Birmingham",
                    "tokens": [
                        "2",
                        " 14",
                        " L",
                        "ubb",
                        "ock",
                        " +",
                        "4",
                        " 20",
                        " Denver",
                        " +",
                        "2",
                        " 23",
                        " Portland",
                        " +",
                        "2",
                        " 7",
                        " Birmingham"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " has to be levied, it should be levied on the Central and state governments and the",
                    "tokens": [
                        " has",
                        " to",
                        " be",
                        " levied",
                        ",",
                        " it",
                        " should",
                        " be",
                        " levied",
                        " on",
                        " the",
                        " Central",
                        " and",
                        " state",
                        " governments",
                        " and",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 43.96957015991211
        },
        {
            "feature_index": 3443,
            "gpt_predictions": [
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "references to \"Newcastle University.\"",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " club's promotion push.\u010a\u010aNewcastle chasing Everton pair James McCarthy and Tom Clever",
                    "max_token": "castle",
                    "tokens": [
                        " club",
                        "'s",
                        " promotion",
                        " push",
                        ".",
                        "\u010a",
                        "\u010a",
                        "New",
                        "castle",
                        " chasing",
                        " Everton",
                        " pair",
                        " James",
                        " McCarthy",
                        " and",
                        " Tom",
                        " Clever"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        22.99827766418457,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": ": Getty)\u010a\u010a(Image: Newcastle Chronicle)\u010a\u010aThese were challenging times",
                    "max_token": " Newcastle",
                    "tokens": [
                        ":",
                        " Getty",
                        ")",
                        "\u010a",
                        "\u010a",
                        "(",
                        "Image",
                        ":",
                        " Newcastle",
                        " Chronicle",
                        ")",
                        "\u010a",
                        "\u010a",
                        "These",
                        " were",
                        " challenging",
                        " times"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        46.18155670166016,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " Ajax captain, is also a target for Newcastle United and Arsenal while Manchester City manager Roberto",
                    "max_token": " Newcastle",
                    "tokens": [
                        " Ajax",
                        " captain",
                        ",",
                        " is",
                        " also",
                        " a",
                        " target",
                        " for",
                        " Newcastle",
                        " United",
                        " and",
                        " Arsenal",
                        " while",
                        " Manchester",
                        " City",
                        " manager",
                        " Roberto"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        45.51028060913086,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " proportions. There is no current treatment. Newcastle University said that human trials will take place",
                    "max_token": " Newcastle",
                    "tokens": [
                        " proportions",
                        ".",
                        " There",
                        " is",
                        " no",
                        " current",
                        " treatment",
                        ".",
                        " Newcastle",
                        " University",
                        " said",
                        " that",
                        " human",
                        " trials",
                        " will",
                        " take",
                        " place"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        44.74617004394531,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "\u010aRead the full story\u010a\u010aNewcastle told Remy will cost \u00c2\u00a312m\u010a",
                    "max_token": "castle",
                    "tokens": [
                        "\u010a",
                        "Read",
                        " the",
                        " full",
                        " story",
                        "\u010a",
                        "\u010a",
                        "New",
                        "castle",
                        " told",
                        " Remy",
                        " will",
                        " cost",
                        " \u00c2\u00a3",
                        "12",
                        "m",
                        "\u010a"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        24.32184600830078,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " most rich countries. But \u00e2\u0122\u013epremiumisation\u00e2\u0122\u013f is working. Though still",
                    "tokens": [
                        " most",
                        " rich",
                        " countries",
                        ".",
                        " But",
                        " \u00e2\u0122",
                        "\u013e",
                        "prem",
                        "ium",
                        "isation",
                        "\u00e2\u0122",
                        "\u013f",
                        " is",
                        " working",
                        ".",
                        " Though",
                        " still"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " outfit in the top 20. The Denver Nuggets reportedly spend \u00c2\u00a32,939,",
                    "tokens": [
                        " outfit",
                        " in",
                        " the",
                        " top",
                        " 20",
                        ".",
                        " The",
                        " Denver",
                        " Nuggets",
                        " reportedly",
                        " spend",
                        " \u00c2\u00a3",
                        "2",
                        ",",
                        "9",
                        "39",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "if Al-Arab al-Qathafi, where three of Colonel Gaddafi's grandchildren",
                    "tokens": [
                        "if",
                        " Al",
                        "-",
                        "Arab",
                        " al",
                        "-",
                        "Q",
                        "ath",
                        "afi",
                        ",",
                        " where",
                        " three",
                        " of",
                        " Colonel",
                        " Gaddafi",
                        "'s",
                        " grandchildren"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "2 14 Lubbock +4 20 Denver +2 23 Portland +2 7 Birmingham",
                    "tokens": [
                        "2",
                        " 14",
                        " L",
                        "ubb",
                        "ock",
                        " +",
                        "4",
                        " 20",
                        " Denver",
                        " +",
                        "2",
                        " 23",
                        " Portland",
                        " +",
                        "2",
                        " 7",
                        " Birmingham"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " has to be levied, it should be levied on the Central and state governments and the",
                    "tokens": [
                        " has",
                        " to",
                        " be",
                        " levied",
                        ",",
                        " it",
                        " should",
                        " be",
                        " levied",
                        " on",
                        " the",
                        " Central",
                        " and",
                        " state",
                        " governments",
                        " and",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 47.77549743652344
        },
        {
            "feature_index": 8835,
            "gpt_predictions": [
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "words related to science fiction",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " on the statue's face, yelling at Scioli: \"Don't! Leave it",
                    "max_token": " Sci",
                    "tokens": [
                        " on",
                        " the",
                        " statue",
                        "'s",
                        " face",
                        ",",
                        " yelling",
                        " at",
                        " Sci",
                        "oli",
                        ":",
                        " \"",
                        "Don",
                        "'t",
                        "!",
                        " Leave",
                        " it"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        22.57920074462891,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "s analysis.\"[3]\u010a\u010aThe sci-fi cover artwork for the album and",
                    "max_token": " sci",
                    "tokens": [
                        "s",
                        " analysis",
                        ".\"[",
                        "3",
                        "]",
                        "\u010a",
                        "\u010a",
                        "The",
                        " sci",
                        "-",
                        "fi",
                        " cover",
                        " artwork",
                        " for",
                        " the",
                        " album",
                        " and"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        45.59072113037109,
                        0,
                        1.631390690803528,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "s described as a contained thriller with either sci-fi or horror elements\u00e2\u0122\u0136which doesn",
                    "max_token": " sci",
                    "tokens": [
                        "s",
                        " described",
                        " as",
                        " a",
                        " contained",
                        " thriller",
                        " with",
                        " either",
                        " sci",
                        "-",
                        "fi",
                        " or",
                        " horror",
                        " elements",
                        "\u00e2\u0122\u0136",
                        "which",
                        " doesn"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0.272472620010376,
                        0,
                        0,
                        45.1923713684082,
                        0,
                        1.664120554924011,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": ", \u00e2\u0122\u013eexploring what happens when sci-fi and video games collide,\u00e2\u0122",
                    "max_token": " sci",
                    "tokens": [
                        ",",
                        " \u00e2\u0122",
                        "\u013e",
                        "expl",
                        "oring",
                        " what",
                        " happens",
                        " when",
                        " sci",
                        "-",
                        "fi",
                        " and",
                        " video",
                        " games",
                        " collide",
                        ",",
                        "\u00e2\u0122"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        43.69633483886719,
                        0,
                        1.903591394424438,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " so she could afford to buy a Volkswagen Scirocco. However, thanks to",
                    "max_token": " Sci",
                    "tokens": [
                        " so",
                        " she",
                        " could",
                        " afford",
                        " to",
                        " buy",
                        " a",
                        " Volkswagen",
                        " Sci",
                        "ro",
                        "cc",
                        "o",
                        ".",
                        " However",
                        ",",
                        " thanks",
                        " to"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        23.08806800842285,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " most rich countries. But \u00e2\u0122\u013epremiumisation\u00e2\u0122\u013f is working. Though still",
                    "tokens": [
                        " most",
                        " rich",
                        " countries",
                        ".",
                        " But",
                        " \u00e2\u0122",
                        "\u013e",
                        "prem",
                        "ium",
                        "isation",
                        "\u00e2\u0122",
                        "\u013f",
                        " is",
                        " working",
                        ".",
                        " Though",
                        " still"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " outfit in the top 20. The Denver Nuggets reportedly spend \u00c2\u00a32,939,",
                    "tokens": [
                        " outfit",
                        " in",
                        " the",
                        " top",
                        " 20",
                        ".",
                        " The",
                        " Denver",
                        " Nuggets",
                        " reportedly",
                        " spend",
                        " \u00c2\u00a3",
                        "2",
                        ",",
                        "9",
                        "39",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "if Al-Arab al-Qathafi, where three of Colonel Gaddafi's grandchildren",
                    "tokens": [
                        "if",
                        " Al",
                        "-",
                        "Arab",
                        " al",
                        "-",
                        "Q",
                        "ath",
                        "afi",
                        ",",
                        " where",
                        " three",
                        " of",
                        " Colonel",
                        " Gaddafi",
                        "'s",
                        " grandchildren"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "2 14 Lubbock +4 20 Denver +2 23 Portland +2 7 Birmingham",
                    "tokens": [
                        "2",
                        " 14",
                        " L",
                        "ubb",
                        "ock",
                        " +",
                        "4",
                        " 20",
                        " Denver",
                        " +",
                        "2",
                        " 23",
                        " Portland",
                        " +",
                        "2",
                        " 7",
                        " Birmingham"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " has to be levied, it should be levied on the Central and state governments and the",
                    "tokens": [
                        " has",
                        " to",
                        " be",
                        " levied",
                        ",",
                        " it",
                        " should",
                        " be",
                        " levied",
                        " on",
                        " the",
                        " Central",
                        " and",
                        " state",
                        " governments",
                        " and",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 47.71056747436523
        },
        {
            "feature_index": 16238,
            "gpt_predictions": [
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    0,
                    1.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "phrases related to change or disruption",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " Caramagna\u010a\u010aThe Omega Effect shakes up the typical Daredevil/Punisher",
                    "max_token": " shakes",
                    "tokens": [
                        " Car",
                        "am",
                        "agna",
                        "\u010a",
                        "\u010a",
                        "The",
                        " Omega",
                        " Effect",
                        " shakes",
                        " up",
                        " the",
                        " typical",
                        " Daredevil",
                        "/",
                        "P",
                        "un",
                        "isher"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        10.96185970306396,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " clear how this whole war is going to shake out.\u010a\u010aI quickly learn that",
                    "max_token": " shake",
                    "tokens": [
                        " clear",
                        " how",
                        " this",
                        " whole",
                        " war",
                        " is",
                        " going",
                        " to",
                        " shake",
                        " out",
                        ".",
                        "\u010a",
                        "\u010a",
                        "I",
                        " quickly",
                        " learn",
                        " that"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        19.11759757995605,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " left plenty of questions for how things will shake out in the West on Decision Day.",
                    "max_token": " shake",
                    "tokens": [
                        " left",
                        " plenty",
                        " of",
                        " questions",
                        " for",
                        " how",
                        " things",
                        " will",
                        " shake",
                        " out",
                        " in",
                        " the",
                        " West",
                        " on",
                        " Decision",
                        " Day",
                        "."
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        18.94436645507812,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "100 percent [that] we need to shake it up,\u00e2\u0122\u013f said Rep.",
                    "max_token": " shake",
                    "tokens": [
                        "100",
                        " percent",
                        " [",
                        "that",
                        "]",
                        " we",
                        " need",
                        " to",
                        " shake",
                        " it",
                        " up",
                        ",",
                        "\u00e2\u0122",
                        "\u013f",
                        " said",
                        " Rep",
                        "."
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        21.77297401428223,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " a glimpse at next season that promises to shake things up under new showrunners Alexi",
                    "max_token": " shake",
                    "tokens": [
                        " a",
                        " glimpse",
                        " at",
                        " next",
                        " season",
                        " that",
                        " promises",
                        " to",
                        " shake",
                        " things",
                        " up",
                        " under",
                        " new",
                        " show",
                        "runners",
                        " Alex",
                        "i"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        20.59773063659668,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " most rich countries. But \u00e2\u0122\u013epremiumisation\u00e2\u0122\u013f is working. Though still",
                    "tokens": [
                        " most",
                        " rich",
                        " countries",
                        ".",
                        " But",
                        " \u00e2\u0122",
                        "\u013e",
                        "prem",
                        "ium",
                        "isation",
                        "\u00e2\u0122",
                        "\u013f",
                        " is",
                        " working",
                        ".",
                        " Though",
                        " still"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " outfit in the top 20. The Denver Nuggets reportedly spend \u00c2\u00a32,939,",
                    "tokens": [
                        " outfit",
                        " in",
                        " the",
                        " top",
                        " 20",
                        ".",
                        " The",
                        " Denver",
                        " Nuggets",
                        " reportedly",
                        " spend",
                        " \u00c2\u00a3",
                        "2",
                        ",",
                        "9",
                        "39",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "if Al-Arab al-Qathafi, where three of Colonel Gaddafi's grandchildren",
                    "tokens": [
                        "if",
                        " Al",
                        "-",
                        "Arab",
                        " al",
                        "-",
                        "Q",
                        "ath",
                        "afi",
                        ",",
                        " where",
                        " three",
                        " of",
                        " Colonel",
                        " Gaddafi",
                        "'s",
                        " grandchildren"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "2 14 Lubbock +4 20 Denver +2 23 Portland +2 7 Birmingham",
                    "tokens": [
                        "2",
                        " 14",
                        " L",
                        "ubb",
                        "ock",
                        " +",
                        "4",
                        " 20",
                        " Denver",
                        " +",
                        "2",
                        " 23",
                        " Portland",
                        " +",
                        "2",
                        " 7",
                        " Birmingham"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " has to be levied, it should be levied on the Central and state governments and the",
                    "tokens": [
                        " has",
                        " to",
                        " be",
                        " levied",
                        ",",
                        " it",
                        " should",
                        " be",
                        " levied",
                        " on",
                        " the",
                        " Central",
                        " and",
                        " state",
                        " governments",
                        " and",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 24.11347579956055
        },
        {
            "feature_index": 15866,
            "gpt_predictions": [
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "keywords related to transparency and accountability",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " the defensive. It also resulted in more transparency in regard to its algorithms that power so",
                    "max_token": " transparency",
                    "tokens": [
                        " the",
                        " defensive",
                        ".",
                        " It",
                        " also",
                        " resulted",
                        " in",
                        " more",
                        " transparency",
                        " in",
                        " regard",
                        " to",
                        " its",
                        " algorithms",
                        " that",
                        " power",
                        " so"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        39.58529281616211,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " several vital principles, one of which is transparency.\"\u010a\u010aRowland Does Not Test",
                    "max_token": " transparency",
                    "tokens": [
                        " several",
                        " vital",
                        " principles",
                        ",",
                        " one",
                        " of",
                        " which",
                        " is",
                        " transparency",
                        ".\"",
                        "\u010a",
                        "\u010a",
                        "Row",
                        "land",
                        " Does",
                        " Not",
                        " Test"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        40.1870231628418,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "en three years ago, he demanded more transparency, saying this would be better for business",
                    "max_token": " transparency",
                    "tokens": [
                        "en",
                        " three",
                        " years",
                        " ago",
                        ",",
                        " he",
                        " demanded",
                        " more",
                        " transparency",
                        ",",
                        " saying",
                        " this",
                        " would",
                        " be",
                        " better",
                        " for",
                        " business"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        39.5279541015625,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "-left communists, have all signaled their openness to governing with the ANO, although",
                    "max_token": " openness",
                    "tokens": [
                        "-",
                        "left",
                        " communists",
                        ",",
                        " have",
                        " all",
                        " signaled",
                        " their",
                        " openness",
                        " to",
                        " governing",
                        " with",
                        " the",
                        " AN",
                        "O",
                        ",",
                        " although"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        7.51639986038208,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " the photocathode layer could have variable transparency, making it suitable for solar windows.",
                    "max_token": " transparency",
                    "tokens": [
                        " the",
                        " photoc",
                        "ath",
                        "ode",
                        " layer",
                        " could",
                        " have",
                        " variable",
                        " transparency",
                        ",",
                        " making",
                        " it",
                        " suitable",
                        " for",
                        " solar",
                        " windows",
                        "."
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        32.82054138183594,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " most rich countries. But \u00e2\u0122\u013epremiumisation\u00e2\u0122\u013f is working. Though still",
                    "tokens": [
                        " most",
                        " rich",
                        " countries",
                        ".",
                        " But",
                        " \u00e2\u0122",
                        "\u013e",
                        "prem",
                        "ium",
                        "isation",
                        "\u00e2\u0122",
                        "\u013f",
                        " is",
                        " working",
                        ".",
                        " Though",
                        " still"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " outfit in the top 20. The Denver Nuggets reportedly spend \u00c2\u00a32,939,",
                    "tokens": [
                        " outfit",
                        " in",
                        " the",
                        " top",
                        " 20",
                        ".",
                        " The",
                        " Denver",
                        " Nuggets",
                        " reportedly",
                        " spend",
                        " \u00c2\u00a3",
                        "2",
                        ",",
                        "9",
                        "39",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "if Al-Arab al-Qathafi, where three of Colonel Gaddafi's grandchildren",
                    "tokens": [
                        "if",
                        " Al",
                        "-",
                        "Arab",
                        " al",
                        "-",
                        "Q",
                        "ath",
                        "afi",
                        ",",
                        " where",
                        " three",
                        " of",
                        " Colonel",
                        " Gaddafi",
                        "'s",
                        " grandchildren"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "2 14 Lubbock +4 20 Denver +2 23 Portland +2 7 Birmingham",
                    "tokens": [
                        "2",
                        " 14",
                        " L",
                        "ubb",
                        "ock",
                        " +",
                        "4",
                        " 20",
                        " Denver",
                        " +",
                        "2",
                        " 23",
                        " Portland",
                        " +",
                        "2",
                        " 7",
                        " Birmingham"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " has to be levied, it should be levied on the Central and state governments and the",
                    "tokens": [
                        " has",
                        " to",
                        " be",
                        " levied",
                        ",",
                        " it",
                        " should",
                        " be",
                        " levied",
                        " on",
                        " the",
                        " Central",
                        " and",
                        " state",
                        " governments",
                        " and",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 40.80093383789062
        },
        {
            "feature_index": 13740,
            "gpt_predictions": [
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "mentions of the verb \"isn't.\"",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": ".\u010a\u010aWhat\u00e2\u0122\u013bs crucial isn\u00e2\u0122\u013bt so much the numbers but",
                    "max_token": " isn",
                    "tokens": [
                        ".",
                        "\u010a",
                        "\u010a",
                        "What",
                        "\u00e2\u0122",
                        "\u013b",
                        "s",
                        " crucial",
                        " isn",
                        "\u00e2\u0122",
                        "\u013b",
                        "t",
                        " so",
                        " much",
                        " the",
                        " numbers",
                        " but"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        27.90852355957031,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": ".\u010a\u010aYan noted that the region isn\u00e2\u0122\u013bt just facing an \u00e2\u0122\u013e",
                    "max_token": " isn",
                    "tokens": [
                        ".",
                        "\u010a",
                        "\u010a",
                        "Yan",
                        " noted",
                        " that",
                        " the",
                        " region",
                        " isn",
                        "\u00e2\u0122",
                        "\u013b",
                        "t",
                        " just",
                        " facing",
                        " an",
                        " \u00e2\u0122",
                        "\u013e"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        28.84140586853027,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " causing bottlenecks. Perhaps JSON isn't suitable at all (binary formats)",
                    "max_token": " isn",
                    "tokens": [
                        " causing",
                        " bott",
                        "len",
                        "ec",
                        "ks",
                        ".",
                        " Perhaps",
                        " JSON",
                        " isn",
                        "'t",
                        " suitable",
                        " at",
                        " all",
                        " (",
                        "binary",
                        " formats",
                        ")"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        29.52854537963867,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " potential constitutional violation.\u010a\u010aThe practice isn't new, according to attorneys, but",
                    "max_token": " isn",
                    "tokens": [
                        " potential",
                        " constitutional",
                        " violation",
                        ".",
                        "\u010a",
                        "\u010a",
                        "The",
                        " practice",
                        " isn",
                        "'t",
                        " new",
                        ",",
                        " according",
                        " to",
                        " attorneys",
                        ",",
                        " but"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        29.73483848571777,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "-American population<|endoftext|>Drinking in moderation isn\u00e2\u0122\u013bt necessarily a bad thing;",
                    "max_token": " isn",
                    "tokens": [
                        "-",
                        "American",
                        " population",
                        "<|endoftext|>",
                        "Dr",
                        "inking",
                        " in",
                        " moderation",
                        " isn",
                        "\u00e2\u0122",
                        "\u013b",
                        "t",
                        " necessarily",
                        " a",
                        " bad",
                        " thing",
                        ";"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        30.14573097229004,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " most rich countries. But \u00e2\u0122\u013epremiumisation\u00e2\u0122\u013f is working. Though still",
                    "tokens": [
                        " most",
                        " rich",
                        " countries",
                        ".",
                        " But",
                        " \u00e2\u0122",
                        "\u013e",
                        "prem",
                        "ium",
                        "isation",
                        "\u00e2\u0122",
                        "\u013f",
                        " is",
                        " working",
                        ".",
                        " Though",
                        " still"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " outfit in the top 20. The Denver Nuggets reportedly spend \u00c2\u00a32,939,",
                    "tokens": [
                        " outfit",
                        " in",
                        " the",
                        " top",
                        " 20",
                        ".",
                        " The",
                        " Denver",
                        " Nuggets",
                        " reportedly",
                        " spend",
                        " \u00c2\u00a3",
                        "2",
                        ",",
                        "9",
                        "39",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "if Al-Arab al-Qathafi, where three of Colonel Gaddafi's grandchildren",
                    "tokens": [
                        "if",
                        " Al",
                        "-",
                        "Arab",
                        " al",
                        "-",
                        "Q",
                        "ath",
                        "afi",
                        ",",
                        " where",
                        " three",
                        " of",
                        " Colonel",
                        " Gaddafi",
                        "'s",
                        " grandchildren"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "2 14 Lubbock +4 20 Denver +2 23 Portland +2 7 Birmingham",
                    "tokens": [
                        "2",
                        " 14",
                        " L",
                        "ubb",
                        "ock",
                        " +",
                        "4",
                        " 20",
                        " Denver",
                        " +",
                        "2",
                        " 23",
                        " Portland",
                        " +",
                        "2",
                        " 7",
                        " Birmingham"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " has to be levied, it should be levied on the Central and state governments and the",
                    "tokens": [
                        " has",
                        " to",
                        " be",
                        " levied",
                        ",",
                        " it",
                        " should",
                        " be",
                        " levied",
                        " on",
                        " the",
                        " Central",
                        " and",
                        " state",
                        " governments",
                        " and",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 30.90035057067871
        },
        {
            "feature_index": 8541,
            "gpt_predictions": [
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    1.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "proper nouns, specifically names like \"Brian.\"",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " the country.\u010a\u010aCouncillor Brian Morris (Ukip, Keighley",
                    "max_token": " Brian",
                    "tokens": [
                        " the",
                        " country",
                        ".",
                        "\u010a",
                        "\u010a",
                        "Coun",
                        "cill",
                        "or",
                        " Brian",
                        " Morris",
                        " (",
                        "Uk",
                        "ip",
                        ",",
                        " Ke",
                        "igh",
                        "ley"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        37.40304565429688,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " haste\".\u010a\u010aMaster of the posts Brian Tuke was the first\u010a\u010aHenry",
                    "max_token": " Brian",
                    "tokens": [
                        " haste",
                        "\".",
                        "\u010a",
                        "\u010a",
                        "Master",
                        " of",
                        " the",
                        " posts",
                        " Brian",
                        " T",
                        "uke",
                        " was",
                        " the",
                        " first",
                        "\u010a",
                        "\u010a",
                        "Henry"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        40.50722503662109,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " comment on the record, but Airbnb CEO Brian Chesky addressed the criticism in a recent",
                    "max_token": " Brian",
                    "tokens": [
                        " comment",
                        " on",
                        " the",
                        " record",
                        ",",
                        " but",
                        " Airbnb",
                        " CEO",
                        " Brian",
                        " Ches",
                        "ky",
                        " addressed",
                        " the",
                        " criticism",
                        " in",
                        " a",
                        " recent"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        40.16215896606445,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " Bobby Kotick and co-chairman Brian Kelly participated in the $8 billion buy",
                    "max_token": " Brian",
                    "tokens": [
                        " Bobby",
                        " Kot",
                        "ick",
                        " and",
                        " co",
                        "-",
                        "chair",
                        "man",
                        " Brian",
                        " Kelly",
                        " participated",
                        " in",
                        " the",
                        " $",
                        "8",
                        " billion",
                        " buy"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        39.94149780273438,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "y City Bulls (Brad Seymour president, Brian Hagen general manager, Charlie Henry head",
                    "max_token": " Brian",
                    "tokens": [
                        "y",
                        " City",
                        " Bulls",
                        " (",
                        "Brad",
                        " Seymour",
                        " president",
                        ",",
                        " Brian",
                        " H",
                        "agen",
                        " general",
                        " manager",
                        ",",
                        " Charlie",
                        " Henry",
                        " head"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        37.60403060913086,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " most rich countries. But \u00e2\u0122\u013epremiumisation\u00e2\u0122\u013f is working. Though still",
                    "tokens": [
                        " most",
                        " rich",
                        " countries",
                        ".",
                        " But",
                        " \u00e2\u0122",
                        "\u013e",
                        "prem",
                        "ium",
                        "isation",
                        "\u00e2\u0122",
                        "\u013f",
                        " is",
                        " working",
                        ".",
                        " Though",
                        " still"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " outfit in the top 20. The Denver Nuggets reportedly spend \u00c2\u00a32,939,",
                    "tokens": [
                        " outfit",
                        " in",
                        " the",
                        " top",
                        " 20",
                        ".",
                        " The",
                        " Denver",
                        " Nuggets",
                        " reportedly",
                        " spend",
                        " \u00c2\u00a3",
                        "2",
                        ",",
                        "9",
                        "39",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "if Al-Arab al-Qathafi, where three of Colonel Gaddafi's grandchildren",
                    "tokens": [
                        "if",
                        " Al",
                        "-",
                        "Arab",
                        " al",
                        "-",
                        "Q",
                        "ath",
                        "afi",
                        ",",
                        " where",
                        " three",
                        " of",
                        " Colonel",
                        " Gaddafi",
                        "'s",
                        " grandchildren"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "2 14 Lubbock +4 20 Denver +2 23 Portland +2 7 Birmingham",
                    "tokens": [
                        "2",
                        " 14",
                        " L",
                        "ubb",
                        "ock",
                        " +",
                        "4",
                        " 20",
                        " Denver",
                        " +",
                        "2",
                        " 23",
                        " Portland",
                        " +",
                        "2",
                        " 7",
                        " Birmingham"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " has to be levied, it should be levied on the Central and state governments and the",
                    "tokens": [
                        " has",
                        " to",
                        " be",
                        " levied",
                        ",",
                        " it",
                        " should",
                        " be",
                        " levied",
                        " on",
                        " the",
                        " Central",
                        " and",
                        " state",
                        " governments",
                        " and",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 41.94441604614258
        },
        {
            "feature_index": 23140,
            "gpt_predictions": [
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "mentions of a specific name \"Britt\"",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " quite emphatic that the allegations against Lord Brittan were nonsense.'\u010a\u010aSo outraged",
                    "max_token": " Britt",
                    "tokens": [
                        " quite",
                        " emph",
                        "atic",
                        " that",
                        " the",
                        " allegations",
                        " against",
                        " Lord",
                        " Britt",
                        "an",
                        " were",
                        " nonsense",
                        ".'",
                        "\u010a",
                        "\u010a",
                        "So",
                        " outraged"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        35.44095611572266,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "little people that lived in the walls,\" Brittney Blue said.\u010a\u010aShe said",
                    "max_token": " Britt",
                    "tokens": [
                        "little",
                        " people",
                        " that",
                        " lived",
                        " in",
                        " the",
                        " walls",
                        ",\"",
                        " Britt",
                        "ney",
                        " Blue",
                        " said",
                        ".",
                        "\u010a",
                        "\u010a",
                        "She",
                        " said"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        34.97449493408203,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " twice, is survived by two daughters, Brittney, 38, and Brandi,",
                    "max_token": " Britt",
                    "tokens": [
                        " twice",
                        ",",
                        " is",
                        " survived",
                        " by",
                        " two",
                        " daughters",
                        ",",
                        " Britt",
                        "ney",
                        ",",
                        " 38",
                        ",",
                        " and",
                        " Brand",
                        "i",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        35.84846115112305,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " leader Tom Watson's controversial intervention in the Brittan rape case for the demise of his",
                    "max_token": " Britt",
                    "tokens": [
                        " leader",
                        " Tom",
                        " Watson",
                        "'s",
                        " controversial",
                        " intervention",
                        " in",
                        " the",
                        " Britt",
                        "an",
                        " rape",
                        " case",
                        " for",
                        " the",
                        " demise",
                        " of",
                        " his"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        33.27105331420898,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " this small, symbolic statement \u00e2\u0122\u0136 Mr. Shimano\u00e2\u0122\u013bs resigning from<|endoftext|>",
                    "max_token": " Shim",
                    "tokens": [
                        " this",
                        " small",
                        ",",
                        " symbolic",
                        " statement",
                        " \u00e2\u0122\u0136",
                        " Mr",
                        ".",
                        " Shim",
                        "ano",
                        "\u00e2\u0122",
                        "\u013b",
                        "s",
                        " resign",
                        "ing",
                        " from",
                        "<|endoftext|>"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        5.992286205291748,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " most rich countries. But \u00e2\u0122\u013epremiumisation\u00e2\u0122\u013f is working. Though still",
                    "tokens": [
                        " most",
                        " rich",
                        " countries",
                        ".",
                        " But",
                        " \u00e2\u0122",
                        "\u013e",
                        "prem",
                        "ium",
                        "isation",
                        "\u00e2\u0122",
                        "\u013f",
                        " is",
                        " working",
                        ".",
                        " Though",
                        " still"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " outfit in the top 20. The Denver Nuggets reportedly spend \u00c2\u00a32,939,",
                    "tokens": [
                        " outfit",
                        " in",
                        " the",
                        " top",
                        " 20",
                        ".",
                        " The",
                        " Denver",
                        " Nuggets",
                        " reportedly",
                        " spend",
                        " \u00c2\u00a3",
                        "2",
                        ",",
                        "9",
                        "39",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "if Al-Arab al-Qathafi, where three of Colonel Gaddafi's grandchildren",
                    "tokens": [
                        "if",
                        " Al",
                        "-",
                        "Arab",
                        " al",
                        "-",
                        "Q",
                        "ath",
                        "afi",
                        ",",
                        " where",
                        " three",
                        " of",
                        " Colonel",
                        " Gaddafi",
                        "'s",
                        " grandchildren"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "2 14 Lubbock +4 20 Denver +2 23 Portland +2 7 Birmingham",
                    "tokens": [
                        "2",
                        " 14",
                        " L",
                        "ubb",
                        "ock",
                        " +",
                        "4",
                        " 20",
                        " Denver",
                        " +",
                        "2",
                        " 23",
                        " Portland",
                        " +",
                        "2",
                        " 7",
                        " Birmingham"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " has to be levied, it should be levied on the Central and state governments and the",
                    "tokens": [
                        " has",
                        " to",
                        " be",
                        " levied",
                        ",",
                        " it",
                        " should",
                        " be",
                        " levied",
                        " on",
                        " the",
                        " Central",
                        " and",
                        " state",
                        " governments",
                        " and",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 36.83099746704102
        },
        {
            "feature_index": 3101,
            "gpt_predictions": [
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "phrases related to distinguishing or differentiation",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " under relatively low induced loading) can be differentiated from fragile skin (that is, skin",
                    "max_token": " differentiated",
                    "tokens": [
                        " under",
                        " relatively",
                        " low",
                        " induced",
                        " loading",
                        ")",
                        " can",
                        " be",
                        " differentiated",
                        " from",
                        " fragile",
                        " skin",
                        " (",
                        "that",
                        " is",
                        ",",
                        " skin"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        19.06857681274414,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " team need to think carefully about how they differentiate between what counts as official campaign communications and",
                    "max_token": " differentiate",
                    "tokens": [
                        " team",
                        " need",
                        " to",
                        " think",
                        " carefully",
                        " about",
                        " how",
                        " they",
                        " differentiate",
                        " between",
                        " what",
                        " counts",
                        " as",
                        " official",
                        " campaign",
                        " communications",
                        " and"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        28.43475151062012,
                        4.559692859649658,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "able desert known as the Outback that separates the inhabited coasts. Elon Musk, who",
                    "max_token": " separates",
                    "tokens": [
                        "able",
                        " desert",
                        " known",
                        " as",
                        " the",
                        " Out",
                        "back",
                        " that",
                        " separates",
                        " the",
                        " inhabited",
                        " coasts",
                        ".",
                        " Elon",
                        " Musk",
                        ",",
                        " who"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        7.617452144622803,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " four topics. We see no reason to distinguish between at least the first three, being",
                    "max_token": " distinguish",
                    "tokens": [
                        " four",
                        " topics",
                        ".",
                        " We",
                        " see",
                        " no",
                        " reason",
                        " to",
                        " distinguish",
                        " between",
                        " at",
                        " least",
                        " the",
                        " first",
                        " three",
                        ",",
                        " being"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        33.09549713134766,
                        3.559412956237793,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " odor concentrations were very close and difficult to distinguish, the flies took much longer to make",
                    "max_token": " distinguish",
                    "tokens": [
                        " odor",
                        " concentrations",
                        " were",
                        " very",
                        " close",
                        " and",
                        " difficult",
                        " to",
                        " distinguish",
                        ",",
                        " the",
                        " flies",
                        " took",
                        " much",
                        " longer",
                        " to",
                        " make"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        32.58559799194336,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " most rich countries. But \u00e2\u0122\u013epremiumisation\u00e2\u0122\u013f is working. Though still",
                    "tokens": [
                        " most",
                        " rich",
                        " countries",
                        ".",
                        " But",
                        " \u00e2\u0122",
                        "\u013e",
                        "prem",
                        "ium",
                        "isation",
                        "\u00e2\u0122",
                        "\u013f",
                        " is",
                        " working",
                        ".",
                        " Though",
                        " still"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " outfit in the top 20. The Denver Nuggets reportedly spend \u00c2\u00a32,939,",
                    "tokens": [
                        " outfit",
                        " in",
                        " the",
                        " top",
                        " 20",
                        ".",
                        " The",
                        " Denver",
                        " Nuggets",
                        " reportedly",
                        " spend",
                        " \u00c2\u00a3",
                        "2",
                        ",",
                        "9",
                        "39",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "if Al-Arab al-Qathafi, where three of Colonel Gaddafi's grandchildren",
                    "tokens": [
                        "if",
                        " Al",
                        "-",
                        "Arab",
                        " al",
                        "-",
                        "Q",
                        "ath",
                        "afi",
                        ",",
                        " where",
                        " three",
                        " of",
                        " Colonel",
                        " Gaddafi",
                        "'s",
                        " grandchildren"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "2 14 Lubbock +4 20 Denver +2 23 Portland +2 7 Birmingham",
                    "tokens": [
                        "2",
                        " 14",
                        " L",
                        "ubb",
                        "ock",
                        " +",
                        "4",
                        " 20",
                        " Denver",
                        " +",
                        "2",
                        " 23",
                        " Portland",
                        " +",
                        "2",
                        " 7",
                        " Birmingham"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " has to be levied, it should be levied on the Central and state governments and the",
                    "tokens": [
                        " has",
                        " to",
                        " be",
                        " levied",
                        ",",
                        " it",
                        " should",
                        " be",
                        " levied",
                        " on",
                        " the",
                        " Central",
                        " and",
                        " state",
                        " governments",
                        " and",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 35.01192474365234
        },
        {
            "feature_index": 10556,
            "gpt_predictions": [
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "phrases related to contracts, negotiations, and extensions",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " all 90 players who will take part in training camp.\u010a\u010aWe\u00e2\u0122\u013bll",
                    "max_token": " training",
                    "tokens": [
                        " all",
                        " 90",
                        " players",
                        " who",
                        " will",
                        " take",
                        " part",
                        " in",
                        " training",
                        " camp",
                        ".",
                        "\u010a",
                        "\u010a",
                        "We",
                        "\u00e2\u0122",
                        "\u013b",
                        "ll"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        5.117327690124512,
                        3.875161170959473,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "legrini signed a one-year contract extension with Manchester City on Friday and insisted",
                    "max_token": " contract",
                    "tokens": [
                        "leg",
                        "r",
                        "ini",
                        " signed",
                        " a",
                        " one",
                        "-",
                        "year",
                        " contract",
                        " extension",
                        " with",
                        " Manchester",
                        " City",
                        " on",
                        " Friday",
                        " and",
                        " insisted"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        2.683044672012329,
                        0,
                        0,
                        0,
                        0,
                        7.710672855377197,
                        6.861431121826172,
                        0.03971852362155914,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " average last season. At UCLA, they auditioned Devin Fuller (24.2 kickoff",
                    "max_token": " audition",
                    "tokens": [
                        " average",
                        " last",
                        " season",
                        ".",
                        " At",
                        " UCLA",
                        ",",
                        " they",
                        " audition",
                        "ed",
                        " Devin",
                        " Fuller",
                        " (",
                        "24",
                        ".",
                        "2",
                        " kickoff"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        2.155864953994751,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " on their roster and are in precarious salary cap shape, but they\u00e2\u0122\u013bll get",
                    "max_token": " cap",
                    "tokens": [
                        " on",
                        " their",
                        " roster",
                        " and",
                        " are",
                        " in",
                        " precarious",
                        " salary",
                        " cap",
                        " shape",
                        ",",
                        " but",
                        " they",
                        "\u00e2\u0122",
                        "\u013b",
                        "ll",
                        " get"
                    ],
                    "values": [
                        0,
                        0,
                        2.403852701187134,
                        0,
                        0,
                        0,
                        0,
                        5.703145503997803,
                        8.760437965393066,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " likely take a non-guaranteed contract assuming he\u00e2\u0122\u013bll make the roster",
                    "max_token": " contract",
                    "tokens": [
                        " likely",
                        " take",
                        " a",
                        " non",
                        "-",
                        "gu",
                        "arant",
                        "eed",
                        " contract",
                        " assuming",
                        " he",
                        "\u00e2\u0122",
                        "\u013b",
                        "ll",
                        " make",
                        " the",
                        " roster"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        2.229421138763428,
                        8.666106224060059,
                        0.6717684268951416,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        4.355569839477539
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " most rich countries. But \u00e2\u0122\u013epremiumisation\u00e2\u0122\u013f is working. Though still",
                    "tokens": [
                        " most",
                        " rich",
                        " countries",
                        ".",
                        " But",
                        " \u00e2\u0122",
                        "\u013e",
                        "prem",
                        "ium",
                        "isation",
                        "\u00e2\u0122",
                        "\u013f",
                        " is",
                        " working",
                        ".",
                        " Though",
                        " still"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " outfit in the top 20. The Denver Nuggets reportedly spend \u00c2\u00a32,939,",
                    "tokens": [
                        " outfit",
                        " in",
                        " the",
                        " top",
                        " 20",
                        ".",
                        " The",
                        " Denver",
                        " Nuggets",
                        " reportedly",
                        " spend",
                        " \u00c2\u00a3",
                        "2",
                        ",",
                        "9",
                        "39",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "if Al-Arab al-Qathafi, where three of Colonel Gaddafi's grandchildren",
                    "tokens": [
                        "if",
                        " Al",
                        "-",
                        "Arab",
                        " al",
                        "-",
                        "Q",
                        "ath",
                        "afi",
                        ",",
                        " where",
                        " three",
                        " of",
                        " Colonel",
                        " Gaddafi",
                        "'s",
                        " grandchildren"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "2 14 Lubbock +4 20 Denver +2 23 Portland +2 7 Birmingham",
                    "tokens": [
                        "2",
                        " 14",
                        " L",
                        "ubb",
                        "ock",
                        " +",
                        "4",
                        " 20",
                        " Denver",
                        " +",
                        "2",
                        " 23",
                        " Portland",
                        " +",
                        "2",
                        " 7",
                        " Birmingham"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " has to be levied, it should be levied on the Central and state governments and the",
                    "tokens": [
                        " has",
                        " to",
                        " be",
                        " levied",
                        ",",
                        " it",
                        " should",
                        " be",
                        " levied",
                        " on",
                        " the",
                        " Central",
                        " and",
                        " state",
                        " governments",
                        " and",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 10.02310752868652
        },
        {
            "feature_index": 22774,
            "gpt_predictions": [
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "terms related to violent acts or crimes",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " off the Gujarat chain of communal riots and killings. Almost on cue, the committee leaked",
                    "max_token": " killings",
                    "tokens": [
                        " off",
                        " the",
                        " Gujarat",
                        " chain",
                        " of",
                        " communal",
                        " riots",
                        " and",
                        " killings",
                        ".",
                        " Almost",
                        " on",
                        " cue",
                        ",",
                        " the",
                        " committee",
                        " leaked"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        22.88342475891113,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " the Gulbarg Society fire and communal killings, to directly implicate Modi. A",
                    "max_token": " killings",
                    "tokens": [
                        " the",
                        " Gul",
                        "b",
                        "arg",
                        " Society",
                        " fire",
                        " and",
                        " communal",
                        " killings",
                        ",",
                        " to",
                        " directly",
                        " impl",
                        "icate",
                        " Modi",
                        ".",
                        " A"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        21.98242950439453,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "For here again, as with the campus shootings in Oregon, Texas, and Arizona,",
                    "max_token": " shootings",
                    "tokens": [
                        "For",
                        " here",
                        " again",
                        ",",
                        " as",
                        " with",
                        " the",
                        " campus",
                        " shootings",
                        " in",
                        " Oregon",
                        ",",
                        " Texas",
                        ",",
                        " and",
                        " Arizona",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        12.28258991241455,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "able.\u00e2\u0122\u013f Every day, mass killings are imagined, rehearsed, and enacted",
                    "max_token": " killings",
                    "tokens": [
                        "able",
                        ".",
                        "\u00e2\u0122",
                        "\u013f",
                        " Every",
                        " day",
                        ",",
                        " mass",
                        " killings",
                        " are",
                        " imagined",
                        ",",
                        " rehears",
                        "ed",
                        ",",
                        " and",
                        " enacted"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        23.58927726745605,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " of highly publicized police custody deaths and fatal shootings, such as that of Tamir Rice",
                    "max_token": " shootings",
                    "tokens": [
                        " of",
                        " highly",
                        " publicized",
                        " police",
                        " custody",
                        " deaths",
                        " and",
                        " fatal",
                        " shootings",
                        ",",
                        " such",
                        " as",
                        " that",
                        " of",
                        " Tam",
                        "ir",
                        " Rice"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0.6888651847839355,
                        0,
                        0,
                        14.47861194610596,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " most rich countries. But \u00e2\u0122\u013epremiumisation\u00e2\u0122\u013f is working. Though still",
                    "tokens": [
                        " most",
                        " rich",
                        " countries",
                        ".",
                        " But",
                        " \u00e2\u0122",
                        "\u013e",
                        "prem",
                        "ium",
                        "isation",
                        "\u00e2\u0122",
                        "\u013f",
                        " is",
                        " working",
                        ".",
                        " Though",
                        " still"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " outfit in the top 20. The Denver Nuggets reportedly spend \u00c2\u00a32,939,",
                    "tokens": [
                        " outfit",
                        " in",
                        " the",
                        " top",
                        " 20",
                        ".",
                        " The",
                        " Denver",
                        " Nuggets",
                        " reportedly",
                        " spend",
                        " \u00c2\u00a3",
                        "2",
                        ",",
                        "9",
                        "39",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "if Al-Arab al-Qathafi, where three of Colonel Gaddafi's grandchildren",
                    "tokens": [
                        "if",
                        " Al",
                        "-",
                        "Arab",
                        " al",
                        "-",
                        "Q",
                        "ath",
                        "afi",
                        ",",
                        " where",
                        " three",
                        " of",
                        " Colonel",
                        " Gaddafi",
                        "'s",
                        " grandchildren"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "2 14 Lubbock +4 20 Denver +2 23 Portland +2 7 Birmingham",
                    "tokens": [
                        "2",
                        " 14",
                        " L",
                        "ubb",
                        "ock",
                        " +",
                        "4",
                        " 20",
                        " Denver",
                        " +",
                        "2",
                        " 23",
                        " Portland",
                        " +",
                        "2",
                        " 7",
                        " Birmingham"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " has to be levied, it should be levied on the Central and state governments and the",
                    "tokens": [
                        " has",
                        " to",
                        " be",
                        " levied",
                        ",",
                        " it",
                        " should",
                        " be",
                        " levied",
                        " on",
                        " the",
                        " Central",
                        " and",
                        " state",
                        " governments",
                        " and",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 25.94718933105469
        },
        {
            "feature_index": 3905,
            "gpt_predictions": [
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "terms related to cobblestones",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " he interrupted regular programming to inform viewers that Cobain was found dead.[7] L",
                    "max_token": " Cob",
                    "tokens": [
                        " he",
                        " interrupted",
                        " regular",
                        " programming",
                        " to",
                        " inform",
                        " viewers",
                        " that",
                        " Cob",
                        "ain",
                        " was",
                        " found",
                        " dead",
                        ".[",
                        "7",
                        "]",
                        " L"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        12.92317771911621,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "\u010a\u010aBut while Gottlieb and Coburn were heralding tamper-proof",
                    "max_token": " Cob",
                    "tokens": [
                        "\u010a",
                        "\u010a",
                        "But",
                        " while",
                        " Gott",
                        "lie",
                        "b",
                        " and",
                        " Cob",
                        "urn",
                        " were",
                        " herald",
                        "ing",
                        " tam",
                        "per",
                        "-",
                        "proof"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        13.36345481872559,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " a huge margin of progression. Juan Jose Cobo, the only rider to beat Sky",
                    "max_token": " Cob",
                    "tokens": [
                        " a",
                        " huge",
                        " margin",
                        " of",
                        " progression",
                        ".",
                        " Juan",
                        " Jose",
                        " Cob",
                        "o",
                        ",",
                        " the",
                        " only",
                        " rider",
                        " to",
                        " beat",
                        " Sky"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        14.71219444274902,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " have sounded<|endoftext|>) Chris Latta as Cobra Commander, G.I. Joe:",
                    "max_token": " Cobra",
                    "tokens": [
                        " have",
                        " sounded",
                        "<|endoftext|>",
                        ")",
                        " Chris",
                        " L",
                        "atta",
                        " as",
                        " Cobra",
                        " Commander",
                        ",",
                        " G",
                        ".",
                        "I",
                        ".",
                        " Joe",
                        ":"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        12.29830551147461,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " abandoned warehouse that you've rigged with booby traps and explosions. Realize this plan",
                    "max_token": "oby",
                    "tokens": [
                        " abandoned",
                        " warehouse",
                        " that",
                        " you",
                        "'ve",
                        " rigged",
                        " with",
                        " bo",
                        "oby",
                        " traps",
                        " and",
                        " explosions",
                        ".",
                        " Real",
                        "ize",
                        " this",
                        " plan"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        5.083224773406982,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " most rich countries. But \u00e2\u0122\u013epremiumisation\u00e2\u0122\u013f is working. Though still",
                    "tokens": [
                        " most",
                        " rich",
                        " countries",
                        ".",
                        " But",
                        " \u00e2\u0122",
                        "\u013e",
                        "prem",
                        "ium",
                        "isation",
                        "\u00e2\u0122",
                        "\u013f",
                        " is",
                        " working",
                        ".",
                        " Though",
                        " still"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " outfit in the top 20. The Denver Nuggets reportedly spend \u00c2\u00a32,939,",
                    "tokens": [
                        " outfit",
                        " in",
                        " the",
                        " top",
                        " 20",
                        ".",
                        " The",
                        " Denver",
                        " Nuggets",
                        " reportedly",
                        " spend",
                        " \u00c2\u00a3",
                        "2",
                        ",",
                        "9",
                        "39",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "if Al-Arab al-Qathafi, where three of Colonel Gaddafi's grandchildren",
                    "tokens": [
                        "if",
                        " Al",
                        "-",
                        "Arab",
                        " al",
                        "-",
                        "Q",
                        "ath",
                        "afi",
                        ",",
                        " where",
                        " three",
                        " of",
                        " Colonel",
                        " Gaddafi",
                        "'s",
                        " grandchildren"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "2 14 Lubbock +4 20 Denver +2 23 Portland +2 7 Birmingham",
                    "tokens": [
                        "2",
                        " 14",
                        " L",
                        "ubb",
                        "ock",
                        " +",
                        "4",
                        " 20",
                        " Denver",
                        " +",
                        "2",
                        " 23",
                        " Portland",
                        " +",
                        "2",
                        " 7",
                        " Birmingham"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " has to be levied, it should be levied on the Central and state governments and the",
                    "tokens": [
                        " has",
                        " to",
                        " be",
                        " levied",
                        ",",
                        " it",
                        " should",
                        " be",
                        " levied",
                        " on",
                        " the",
                        " Central",
                        " and",
                        " state",
                        " governments",
                        " and",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 50.59541702270508
        },
        {
            "feature_index": 23003,
            "gpt_predictions": [
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "words related to motivation and positive influence",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "iolet Palmer, one of Eastin's inspirations, began officiating NBA games in",
                    "max_token": " inspir",
                    "tokens": [
                        "iolet",
                        " Palmer",
                        ",",
                        " one",
                        " of",
                        " East",
                        "in",
                        "'s",
                        " inspir",
                        "ations",
                        ",",
                        " began",
                        " offic",
                        "iating",
                        " NBA",
                        " games",
                        " in"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        27.69408988952637,
                        9.509218215942383,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "Still, the monuments are key sources of inspiration and Graham believes that there is much to",
                    "max_token": " inspiration",
                    "tokens": [
                        "Still",
                        ",",
                        " the",
                        " monuments",
                        " are",
                        " key",
                        " sources",
                        " of",
                        " inspiration",
                        " and",
                        " Graham",
                        " believes",
                        " that",
                        " there",
                        " is",
                        " much",
                        " to"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        34.83220291137695,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " K\u00c3\u00b6nigsforst from which the inspiration for the project stems - sure to be",
                    "max_token": " inspiration",
                    "tokens": [
                        " K",
                        "\u00c3\u00b6n",
                        "igs",
                        "for",
                        "st",
                        " from",
                        " which",
                        " the",
                        " inspiration",
                        " for",
                        " the",
                        " project",
                        " stems",
                        " -",
                        " sure",
                        " to",
                        " be"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        36.60494995117188,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "etic approach to her body size is an inspiration. The series is a heartfelt look into",
                    "max_token": " inspiration",
                    "tokens": [
                        "etic",
                        " approach",
                        " to",
                        " her",
                        " body",
                        " size",
                        " is",
                        " an",
                        " inspiration",
                        ".",
                        " The",
                        " series",
                        " is",
                        " a",
                        " heartfelt",
                        " look",
                        " into"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        36.94754028320312,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " 14, 2014\u010a\u010aToday my most inspiration comes from Jose Mujica. Sal",
                    "max_token": " inspiration",
                    "tokens": [
                        " 14",
                        ",",
                        " 2014",
                        "\u010a",
                        "\u010a",
                        "Today",
                        " my",
                        " most",
                        " inspiration",
                        " comes",
                        " from",
                        " Jose",
                        " Mu",
                        "j",
                        "ica",
                        ".",
                        " Sal"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        37.95166397094727,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " most rich countries. But \u00e2\u0122\u013epremiumisation\u00e2\u0122\u013f is working. Though still",
                    "tokens": [
                        " most",
                        " rich",
                        " countries",
                        ".",
                        " But",
                        " \u00e2\u0122",
                        "\u013e",
                        "prem",
                        "ium",
                        "isation",
                        "\u00e2\u0122",
                        "\u013f",
                        " is",
                        " working",
                        ".",
                        " Though",
                        " still"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " outfit in the top 20. The Denver Nuggets reportedly spend \u00c2\u00a32,939,",
                    "tokens": [
                        " outfit",
                        " in",
                        " the",
                        " top",
                        " 20",
                        ".",
                        " The",
                        " Denver",
                        " Nuggets",
                        " reportedly",
                        " spend",
                        " \u00c2\u00a3",
                        "2",
                        ",",
                        "9",
                        "39",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "if Al-Arab al-Qathafi, where three of Colonel Gaddafi's grandchildren",
                    "tokens": [
                        "if",
                        " Al",
                        "-",
                        "Arab",
                        " al",
                        "-",
                        "Q",
                        "ath",
                        "afi",
                        ",",
                        " where",
                        " three",
                        " of",
                        " Colonel",
                        " Gaddafi",
                        "'s",
                        " grandchildren"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "2 14 Lubbock +4 20 Denver +2 23 Portland +2 7 Birmingham",
                    "tokens": [
                        "2",
                        " 14",
                        " L",
                        "ubb",
                        "ock",
                        " +",
                        "4",
                        " 20",
                        " Denver",
                        " +",
                        "2",
                        " 23",
                        " Portland",
                        " +",
                        "2",
                        " 7",
                        " Birmingham"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " has to be levied, it should be levied on the Central and state governments and the",
                    "tokens": [
                        " has",
                        " to",
                        " be",
                        " levied",
                        ",",
                        " it",
                        " should",
                        " be",
                        " levied",
                        " on",
                        " the",
                        " Central",
                        " and",
                        " state",
                        " governments",
                        " and",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 40.0796012878418
        },
        {
            "feature_index": 8094,
            "gpt_predictions": [
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "terms related to riots and rioting",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": ",400 to retrofit one officer in riot gear.\u010a\u010a\u00e2\u0122\u013eSince 2006",
                    "max_token": " riot",
                    "tokens": [
                        ",",
                        "400",
                        " to",
                        " retro",
                        "fit",
                        " one",
                        " officer",
                        " in",
                        " riot",
                        " gear",
                        ".",
                        "\u010a",
                        "\u010a",
                        "\u00e2\u0122",
                        "\u013e",
                        "Since",
                        " 2006"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        42.86198806762695,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " a hard hat takes cover during clashes with riot police who tried to prevent demonstrators from reaching",
                    "max_token": " riot",
                    "tokens": [
                        " a",
                        " hard",
                        " hat",
                        " takes",
                        " cover",
                        " during",
                        " clashes",
                        " with",
                        " riot",
                        " police",
                        " who",
                        " tried",
                        " to",
                        " prevent",
                        " demonstrators",
                        " from",
                        " reaching"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        42.64183044433594,
                        2.796077728271484,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " Greenwich Village bar with a six-day riot in June 1969 is considered the birth of",
                    "max_token": " riot",
                    "tokens": [
                        " Greenwich",
                        " Village",
                        " bar",
                        " with",
                        " a",
                        " six",
                        "-",
                        "day",
                        " riot",
                        " in",
                        " June",
                        " 1969",
                        " is",
                        " considered",
                        " the",
                        " birth",
                        " of"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        40.30853271484375,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": ": Present & Celebrate<|endoftext|>Police using riot batons have clashed with refugees on the",
                    "max_token": " riot",
                    "tokens": [
                        ":",
                        " Present",
                        " &",
                        " Celebr",
                        "ate",
                        "<|endoftext|>",
                        "Police",
                        " using",
                        " riot",
                        " bat",
                        "ons",
                        " have",
                        " clashed",
                        " with",
                        " refugees",
                        " on",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        43.16905212402344,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " Thursday afternoon, over a hundred officers in riot gear with automatic rifles lined up across North",
                    "max_token": " riot",
                    "tokens": [
                        " Thursday",
                        " afternoon",
                        ",",
                        " over",
                        " a",
                        " hundred",
                        " officers",
                        " in",
                        " riot",
                        " gear",
                        " with",
                        " automatic",
                        " rifles",
                        " lined",
                        " up",
                        " across",
                        " North"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        41.96281433105469,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " most rich countries. But \u00e2\u0122\u013epremiumisation\u00e2\u0122\u013f is working. Though still",
                    "tokens": [
                        " most",
                        " rich",
                        " countries",
                        ".",
                        " But",
                        " \u00e2\u0122",
                        "\u013e",
                        "prem",
                        "ium",
                        "isation",
                        "\u00e2\u0122",
                        "\u013f",
                        " is",
                        " working",
                        ".",
                        " Though",
                        " still"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " outfit in the top 20. The Denver Nuggets reportedly spend \u00c2\u00a32,939,",
                    "tokens": [
                        " outfit",
                        " in",
                        " the",
                        " top",
                        " 20",
                        ".",
                        " The",
                        " Denver",
                        " Nuggets",
                        " reportedly",
                        " spend",
                        " \u00c2\u00a3",
                        "2",
                        ",",
                        "9",
                        "39",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "if Al-Arab al-Qathafi, where three of Colonel Gaddafi's grandchildren",
                    "tokens": [
                        "if",
                        " Al",
                        "-",
                        "Arab",
                        " al",
                        "-",
                        "Q",
                        "ath",
                        "afi",
                        ",",
                        " where",
                        " three",
                        " of",
                        " Colonel",
                        " Gaddafi",
                        "'s",
                        " grandchildren"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "2 14 Lubbock +4 20 Denver +2 23 Portland +2 7 Birmingham",
                    "tokens": [
                        "2",
                        " 14",
                        " L",
                        "ubb",
                        "ock",
                        " +",
                        "4",
                        " 20",
                        " Denver",
                        " +",
                        "2",
                        " 23",
                        " Portland",
                        " +",
                        "2",
                        " 7",
                        " Birmingham"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " has to be levied, it should be levied on the Central and state governments and the",
                    "tokens": [
                        " has",
                        " to",
                        " be",
                        " levied",
                        ",",
                        " it",
                        " should",
                        " be",
                        " levied",
                        " on",
                        " the",
                        " Central",
                        " and",
                        " state",
                        " governments",
                        " and",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 44.94107437133789
        },
        {
            "feature_index": 1728,
            "gpt_predictions": [
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    1.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "phrases related to political delegate allocation",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "(TALLINN) - Delegates from the Council of Europe's GRECO",
                    "max_token": "legates",
                    "tokens": [
                        "(",
                        "T",
                        "ALL",
                        "IN",
                        "N",
                        ")",
                        " -",
                        " De",
                        "legates",
                        " from",
                        " the",
                        " Council",
                        " of",
                        " Europe",
                        "'s",
                        " GRE",
                        "CO"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        27.03589057922363,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " Palin's speech Wednesday night fired up Republican delegates.\u010a\u010aPalin's speech at",
                    "max_token": " delegates",
                    "tokens": [
                        " Palin",
                        "'s",
                        " speech",
                        " Wednesday",
                        " night",
                        " fired",
                        " up",
                        " Republican",
                        " delegates",
                        ".",
                        "\u010a",
                        "\u010a",
                        "Pal",
                        "in",
                        "'s",
                        " speech",
                        " at"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        36.78322601318359,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "<|endoftext|> session had been called for the Soviet delegates. First Secretary Khrushchev's morning",
                    "max_token": " delegates",
                    "tokens": [
                        "<|endoftext|>",
                        " session",
                        " had",
                        " been",
                        " called",
                        " for",
                        " the",
                        " Soviet",
                        " delegates",
                        ".",
                        " First",
                        " Secretary",
                        " Kh",
                        "rush",
                        "chev",
                        "'s",
                        " morning"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        36.37491226196289,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 11,
                    "sentence_string": " represent both pledged and unpledged Republican delegates. The delegates at stake underneath the state",
                    "max_token": " delegates",
                    "tokens": [
                        " represent",
                        " both",
                        " pledged",
                        " and",
                        " un",
                        "pled",
                        "ged",
                        " Republican",
                        " delegates",
                        ".",
                        " The",
                        " delegates",
                        " at",
                        " stake",
                        " underneath",
                        " the",
                        " state"
                    ],
                    "values": [
                        0,
                        0,
                        1.527916431427002,
                        0,
                        0,
                        0,
                        0,
                        0,
                        39.40104675292969,
                        0,
                        0,
                        40.09348297119141,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " Mitt Romney \u00e2\u0122\u013eundocumented Republican super delegates\u00e2\u0122\u013f who are really pissed that they",
                    "max_token": " delegates",
                    "tokens": [
                        " Mitt",
                        " Romney",
                        " \u00e2\u0122",
                        "\u013e",
                        "und",
                        "ocumented",
                        " Republican",
                        " super",
                        " delegates",
                        "\u00e2\u0122",
                        "\u013f",
                        " who",
                        " are",
                        " really",
                        " pissed",
                        " that",
                        " they"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        38.32999801635742,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " most rich countries. But \u00e2\u0122\u013epremiumisation\u00e2\u0122\u013f is working. Though still",
                    "tokens": [
                        " most",
                        " rich",
                        " countries",
                        ".",
                        " But",
                        " \u00e2\u0122",
                        "\u013e",
                        "prem",
                        "ium",
                        "isation",
                        "\u00e2\u0122",
                        "\u013f",
                        " is",
                        " working",
                        ".",
                        " Though",
                        " still"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " outfit in the top 20. The Denver Nuggets reportedly spend \u00c2\u00a32,939,",
                    "tokens": [
                        " outfit",
                        " in",
                        " the",
                        " top",
                        " 20",
                        ".",
                        " The",
                        " Denver",
                        " Nuggets",
                        " reportedly",
                        " spend",
                        " \u00c2\u00a3",
                        "2",
                        ",",
                        "9",
                        "39",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "if Al-Arab al-Qathafi, where three of Colonel Gaddafi's grandchildren",
                    "tokens": [
                        "if",
                        " Al",
                        "-",
                        "Arab",
                        " al",
                        "-",
                        "Q",
                        "ath",
                        "afi",
                        ",",
                        " where",
                        " three",
                        " of",
                        " Colonel",
                        " Gaddafi",
                        "'s",
                        " grandchildren"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "2 14 Lubbock +4 20 Denver +2 23 Portland +2 7 Birmingham",
                    "tokens": [
                        "2",
                        " 14",
                        " L",
                        "ubb",
                        "ock",
                        " +",
                        "4",
                        " 20",
                        " Denver",
                        " +",
                        "2",
                        " 23",
                        " Portland",
                        " +",
                        "2",
                        " 7",
                        " Birmingham"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " has to be levied, it should be levied on the Central and state governments and the",
                    "tokens": [
                        " has",
                        " to",
                        " be",
                        " levied",
                        ",",
                        " it",
                        " should",
                        " be",
                        " levied",
                        " on",
                        " the",
                        " Central",
                        " and",
                        " state",
                        " governments",
                        " and",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 41.54202651977539
        },
        {
            "feature_index": 12692,
            "gpt_predictions": [
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "words related to being impressed",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "arty said the SPCA isn't impressed with most sled-dog businesses, either",
                    "max_token": " impressed",
                    "tokens": [
                        "arty",
                        " said",
                        " the",
                        " S",
                        "PC",
                        "A",
                        " isn",
                        "'t",
                        " impressed",
                        " with",
                        " most",
                        " sled",
                        "-",
                        "dog",
                        " businesses",
                        ",",
                        " either"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        34.69370269775391,
                        0.6388174891471863,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "ennett who did not appear to be much impressed. He would only grant that this was",
                    "max_token": " impressed",
                    "tokens": [
                        "ennett",
                        " who",
                        " did",
                        " not",
                        " appear",
                        " to",
                        " be",
                        " much",
                        " impressed",
                        ".",
                        " He",
                        " would",
                        " only",
                        " grant",
                        " that",
                        " this",
                        " was"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        35.20983505249023,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "Perhaps because I wasn\u00e2\u0122\u013bt that impressed with the franchise\u00e2\u0122\u013bs second film",
                    "max_token": " impressed",
                    "tokens": [
                        "Perhaps",
                        " because",
                        " I",
                        " wasn",
                        "\u00e2\u0122",
                        "\u013b",
                        "t",
                        " that",
                        " impressed",
                        " with",
                        " the",
                        " franchise",
                        "\u00e2\u0122",
                        "\u013b",
                        "s",
                        " second",
                        " film"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        34.5314826965332,
                        0.3126053214073181,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " Contemporary Novel?\u00e2\u0122\u013f If you are intrigued by the title, don\u00e2\u0122\u013bt",
                    "max_token": " intrigued",
                    "tokens": [
                        " Contemporary",
                        " Novel",
                        "?",
                        "\u00e2\u0122",
                        "\u013f",
                        " If",
                        " you",
                        " are",
                        " intrigued",
                        " by",
                        " the",
                        " title",
                        ",",
                        " don",
                        "\u00e2\u0122",
                        "\u013b",
                        "t"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        5.435439586639404,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " Mr Sanderson said Ms Mead did not impress him as \"a gold digger or",
                    "max_token": " impress",
                    "tokens": [
                        " Mr",
                        " Sand",
                        "erson",
                        " said",
                        " Ms",
                        " Mead",
                        " did",
                        " not",
                        " impress",
                        " him",
                        " as",
                        " \"",
                        "a",
                        " gold",
                        " dig",
                        "ger",
                        " or"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        17.71026992797852,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " most rich countries. But \u00e2\u0122\u013epremiumisation\u00e2\u0122\u013f is working. Though still",
                    "tokens": [
                        " most",
                        " rich",
                        " countries",
                        ".",
                        " But",
                        " \u00e2\u0122",
                        "\u013e",
                        "prem",
                        "ium",
                        "isation",
                        "\u00e2\u0122",
                        "\u013f",
                        " is",
                        " working",
                        ".",
                        " Though",
                        " still"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " outfit in the top 20. The Denver Nuggets reportedly spend \u00c2\u00a32,939,",
                    "tokens": [
                        " outfit",
                        " in",
                        " the",
                        " top",
                        " 20",
                        ".",
                        " The",
                        " Denver",
                        " Nuggets",
                        " reportedly",
                        " spend",
                        " \u00c2\u00a3",
                        "2",
                        ",",
                        "9",
                        "39",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "if Al-Arab al-Qathafi, where three of Colonel Gaddafi's grandchildren",
                    "tokens": [
                        "if",
                        " Al",
                        "-",
                        "Arab",
                        " al",
                        "-",
                        "Q",
                        "ath",
                        "afi",
                        ",",
                        " where",
                        " three",
                        " of",
                        " Colonel",
                        " Gaddafi",
                        "'s",
                        " grandchildren"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "2 14 Lubbock +4 20 Denver +2 23 Portland +2 7 Birmingham",
                    "tokens": [
                        "2",
                        " 14",
                        " L",
                        "ubb",
                        "ock",
                        " +",
                        "4",
                        " 20",
                        " Denver",
                        " +",
                        "2",
                        " 23",
                        " Portland",
                        " +",
                        "2",
                        " 7",
                        " Birmingham"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " has to be levied, it should be levied on the Central and state governments and the",
                    "tokens": [
                        " has",
                        " to",
                        " be",
                        " levied",
                        ",",
                        " it",
                        " should",
                        " be",
                        " levied",
                        " on",
                        " the",
                        " Central",
                        " and",
                        " state",
                        " governments",
                        " and",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 37.25972366333008
        },
        {
            "feature_index": 13606,
            "gpt_predictions": [
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    1.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "phrases or acronyms containing \"AM\"",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "OLF CHANNEL 11:00 AM 12:00 PM 649 222 SPORTS",
                    "max_token": " AM",
                    "tokens": [
                        "OL",
                        "F",
                        " CH",
                        "ANN",
                        "EL",
                        " 11",
                        ":",
                        "00",
                        " AM",
                        " 12",
                        ":",
                        "00",
                        " PM",
                        " 6",
                        "49",
                        " 222",
                        " SPORTS"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        29.39812469482422,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "<|endoftext|>UPDATE 11/10 9:45 AM EST: The filmmakers behind Basmati",
                    "max_token": " AM",
                    "tokens": [
                        "<|endoftext|>",
                        "UPDATE",
                        " 11",
                        "/",
                        "10",
                        " 9",
                        ":",
                        "45",
                        " AM",
                        " EST",
                        ":",
                        " The",
                        " filmmakers",
                        " behind",
                        " Bas",
                        "mat",
                        "i"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        32.65145492553711,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " Hassan Cleric. Former board member of AMAL House stormed early morning 21 March.",
                    "max_token": " AM",
                    "tokens": [
                        " Hassan",
                        " Cl",
                        "eric",
                        ".",
                        " Former",
                        " board",
                        " member",
                        " of",
                        " AM",
                        "AL",
                        " House",
                        " stormed",
                        " early",
                        " morning",
                        " 21",
                        " March",
                        "."
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        32.51301193237305,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "s quite hard to pin down, on AMOK, how much of the percussion is",
                    "max_token": " AM",
                    "tokens": [
                        "s",
                        " quite",
                        " hard",
                        " to",
                        " pin",
                        " down",
                        ",",
                        " on",
                        " AM",
                        "OK",
                        ",",
                        " how",
                        " much",
                        " of",
                        " the",
                        " percussion",
                        " is"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        36.07683563232422,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " his or her ability to handle that 3 AM phone call was the ability to not send",
                    "max_token": " AM",
                    "tokens": [
                        " his",
                        " or",
                        " her",
                        " ability",
                        " to",
                        " handle",
                        " that",
                        " 3",
                        " AM",
                        " phone",
                        " call",
                        " was",
                        " the",
                        " ability",
                        " to",
                        " not",
                        " send"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        34.2798957824707,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " most rich countries. But \u00e2\u0122\u013epremiumisation\u00e2\u0122\u013f is working. Though still",
                    "tokens": [
                        " most",
                        " rich",
                        " countries",
                        ".",
                        " But",
                        " \u00e2\u0122",
                        "\u013e",
                        "prem",
                        "ium",
                        "isation",
                        "\u00e2\u0122",
                        "\u013f",
                        " is",
                        " working",
                        ".",
                        " Though",
                        " still"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " outfit in the top 20. The Denver Nuggets reportedly spend \u00c2\u00a32,939,",
                    "tokens": [
                        " outfit",
                        " in",
                        " the",
                        " top",
                        " 20",
                        ".",
                        " The",
                        " Denver",
                        " Nuggets",
                        " reportedly",
                        " spend",
                        " \u00c2\u00a3",
                        "2",
                        ",",
                        "9",
                        "39",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "if Al-Arab al-Qathafi, where three of Colonel Gaddafi's grandchildren",
                    "tokens": [
                        "if",
                        " Al",
                        "-",
                        "Arab",
                        " al",
                        "-",
                        "Q",
                        "ath",
                        "afi",
                        ",",
                        " where",
                        " three",
                        " of",
                        " Colonel",
                        " Gaddafi",
                        "'s",
                        " grandchildren"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "2 14 Lubbock +4 20 Denver +2 23 Portland +2 7 Birmingham",
                    "tokens": [
                        "2",
                        " 14",
                        " L",
                        "ubb",
                        "ock",
                        " +",
                        "4",
                        " 20",
                        " Denver",
                        " +",
                        "2",
                        " 23",
                        " Portland",
                        " +",
                        "2",
                        " 7",
                        " Birmingham"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " has to be levied, it should be levied on the Central and state governments and the",
                    "tokens": [
                        " has",
                        " to",
                        " be",
                        " levied",
                        ",",
                        " it",
                        " should",
                        " be",
                        " levied",
                        " on",
                        " the",
                        " Central",
                        " and",
                        " state",
                        " governments",
                        " and",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 37.41681289672852
        },
        {
            "feature_index": 13254,
            "gpt_predictions": [
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "mentions or discussions related to wind power",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "I was going to do a long-winded explanation of this, but I can",
                    "max_token": "wind",
                    "tokens": [
                        "I",
                        " was",
                        " going",
                        " to",
                        " do",
                        " a",
                        " long",
                        "-",
                        "wind",
                        "ed",
                        " explanation",
                        " of",
                        " this",
                        ",",
                        " but",
                        " I",
                        " can"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        18.44174766540527,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " should fall apart with a tiny gust of wind, but it actually hangs together pretty well",
                    "max_token": " wind",
                    "tokens": [
                        " should",
                        " fall",
                        " apart",
                        " with",
                        " a",
                        " tiny",
                        " gust",
                        " of",
                        " wind",
                        ",",
                        " but",
                        " it",
                        " actually",
                        " hangs",
                        " together",
                        " pretty",
                        " well"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        40.43045806884766,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " you as sunshine flows into trees. The winds will blow their own freshness into you",
                    "max_token": " winds",
                    "tokens": [
                        " you",
                        " as",
                        " sunshine",
                        " flows",
                        " into",
                        " trees",
                        ".",
                        " The",
                        " winds",
                        " will",
                        " blow",
                        " their",
                        " own",
                        " fresh",
                        "ness",
                        " into",
                        " you"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        11.49083042144775,
                        0,
                        1.389937400817871,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "So many similarities between offshore oil and offshore wind that it's a natural transition for people",
                    "max_token": " wind",
                    "tokens": [
                        "So",
                        " many",
                        " similarities",
                        " between",
                        " offshore",
                        " oil",
                        " and",
                        " offshore",
                        " wind",
                        " that",
                        " it",
                        "'s",
                        " a",
                        " natural",
                        " transition",
                        " for",
                        " people"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        35.36783981323242,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "BENGALURU: In a windfall to legislators and ministers, both Houses",
                    "max_token": " wind",
                    "tokens": [
                        "B",
                        "ENG",
                        "AL",
                        "UR",
                        "U",
                        ":",
                        " In",
                        " a",
                        " wind",
                        "fall",
                        " to",
                        " legislators",
                        " and",
                        " ministers",
                        ",",
                        " both",
                        " Houses"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        41.47150802612305,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " most rich countries. But \u00e2\u0122\u013epremiumisation\u00e2\u0122\u013f is working. Though still",
                    "tokens": [
                        " most",
                        " rich",
                        " countries",
                        ".",
                        " But",
                        " \u00e2\u0122",
                        "\u013e",
                        "prem",
                        "ium",
                        "isation",
                        "\u00e2\u0122",
                        "\u013f",
                        " is",
                        " working",
                        ".",
                        " Though",
                        " still"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " outfit in the top 20. The Denver Nuggets reportedly spend \u00c2\u00a32,939,",
                    "tokens": [
                        " outfit",
                        " in",
                        " the",
                        " top",
                        " 20",
                        ".",
                        " The",
                        " Denver",
                        " Nuggets",
                        " reportedly",
                        " spend",
                        " \u00c2\u00a3",
                        "2",
                        ",",
                        "9",
                        "39",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "if Al-Arab al-Qathafi, where three of Colonel Gaddafi's grandchildren",
                    "tokens": [
                        "if",
                        " Al",
                        "-",
                        "Arab",
                        " al",
                        "-",
                        "Q",
                        "ath",
                        "afi",
                        ",",
                        " where",
                        " three",
                        " of",
                        " Colonel",
                        " Gaddafi",
                        "'s",
                        " grandchildren"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "2 14 Lubbock +4 20 Denver +2 23 Portland +2 7 Birmingham",
                    "tokens": [
                        "2",
                        " 14",
                        " L",
                        "ubb",
                        "ock",
                        " +",
                        "4",
                        " 20",
                        " Denver",
                        " +",
                        "2",
                        " 23",
                        " Portland",
                        " +",
                        "2",
                        " 7",
                        " Birmingham"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " has to be levied, it should be levied on the Central and state governments and the",
                    "tokens": [
                        " has",
                        " to",
                        " be",
                        " levied",
                        ",",
                        " it",
                        " should",
                        " be",
                        " levied",
                        " on",
                        " the",
                        " Central",
                        " and",
                        " state",
                        " governments",
                        " and",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 43.41487121582031
        },
        {
            "feature_index": 196,
            "gpt_predictions": [
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "phrases related to legal rights and regulations, particularly those concerning medical practices",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "\u010aclass ThreadViewController: UIViewController, UITableViewDataSource",
                    "max_token": "iew",
                    "tokens": [
                        "\u010a",
                        "class",
                        " Thread",
                        "View",
                        "Controller",
                        ":",
                        " U",
                        "IV",
                        "iew",
                        "Controller",
                        ",",
                        " U",
                        "IT",
                        "able",
                        "View",
                        "Data",
                        "Source"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        7.009487152099609,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " Measure Punching Power and Speed During Fights<|endoftext|>Throughout 2012, The Caucus will occasionally",
                    "max_token": "ights",
                    "tokens": [
                        " Measure",
                        " Punch",
                        "ing",
                        " Power",
                        " and",
                        " Speed",
                        " During",
                        " F",
                        "ights",
                        "<|endoftext|>",
                        "Throughout",
                        " 2012",
                        ",",
                        " The",
                        " Caucus",
                        " will",
                        " occasionally"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        8.423912048339844,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "<|endoftext|> were tested. The results of these experiments show that parietal lobe damage disproportionately imp",
                    "max_token": " experiments",
                    "tokens": [
                        "<|endoftext|>",
                        " were",
                        " tested",
                        ".",
                        " The",
                        " results",
                        " of",
                        " these",
                        " experiments",
                        " show",
                        " that",
                        " par",
                        "ietal",
                        " lobe",
                        " damage",
                        " disproportionately",
                        " imp"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        2.744857311248779,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " the new rules: Doctors must have hospital privileges, and clinics must function like outpatient surgery",
                    "max_token": " privileges",
                    "tokens": [
                        " the",
                        " new",
                        " rules",
                        ":",
                        " Doctors",
                        " must",
                        " have",
                        " hospital",
                        " privileges",
                        ",",
                        " and",
                        " clinics",
                        " must",
                        " function",
                        " like",
                        " outpatient",
                        " surgery"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        10.5646390914917,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " the Bhutan Civil Aviation Authority. Flights could occur to other parts of the country",
                    "max_token": "ights",
                    "tokens": [
                        " the",
                        " Bh",
                        "utan",
                        " Civil",
                        " Aviation",
                        " Authority",
                        ".",
                        " Fl",
                        "ights",
                        " could",
                        " occur",
                        " to",
                        " other",
                        " parts",
                        " of",
                        " the",
                        " country"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        10.01171016693115,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " most rich countries. But \u00e2\u0122\u013epremiumisation\u00e2\u0122\u013f is working. Though still",
                    "tokens": [
                        " most",
                        " rich",
                        " countries",
                        ".",
                        " But",
                        " \u00e2\u0122",
                        "\u013e",
                        "prem",
                        "ium",
                        "isation",
                        "\u00e2\u0122",
                        "\u013f",
                        " is",
                        " working",
                        ".",
                        " Though",
                        " still"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " outfit in the top 20. The Denver Nuggets reportedly spend \u00c2\u00a32,939,",
                    "tokens": [
                        " outfit",
                        " in",
                        " the",
                        " top",
                        " 20",
                        ".",
                        " The",
                        " Denver",
                        " Nuggets",
                        " reportedly",
                        " spend",
                        " \u00c2\u00a3",
                        "2",
                        ",",
                        "9",
                        "39",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "if Al-Arab al-Qathafi, where three of Colonel Gaddafi's grandchildren",
                    "tokens": [
                        "if",
                        " Al",
                        "-",
                        "Arab",
                        " al",
                        "-",
                        "Q",
                        "ath",
                        "afi",
                        ",",
                        " where",
                        " three",
                        " of",
                        " Colonel",
                        " Gaddafi",
                        "'s",
                        " grandchildren"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "2 14 Lubbock +4 20 Denver +2 23 Portland +2 7 Birmingham",
                    "tokens": [
                        "2",
                        " 14",
                        " L",
                        "ubb",
                        "ock",
                        " +",
                        "4",
                        " 20",
                        " Denver",
                        " +",
                        "2",
                        " 23",
                        " Portland",
                        " +",
                        "2",
                        " 7",
                        " Birmingham"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " has to be levied, it should be levied on the Central and state governments and the",
                    "tokens": [
                        " has",
                        " to",
                        " be",
                        " levied",
                        ",",
                        " it",
                        " should",
                        " be",
                        " levied",
                        " on",
                        " the",
                        " Central",
                        " and",
                        " state",
                        " governments",
                        " and",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 11.64654445648193
        },
        {
            "feature_index": 20164,
            "gpt_predictions": [
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "descriptions of something being visually appealing",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 9,
                    "sentence_string": " provocative?\" you continued. \"Are you unattractive?\"\u010a\u010aWhen you give up",
                    "max_token": "ractive",
                    "tokens": [
                        " provocative",
                        "?\"",
                        " you",
                        " continued",
                        ".",
                        " \"",
                        "Are",
                        " you",
                        " unatt",
                        "ractive",
                        "?\"",
                        "\u010a",
                        "\u010a",
                        "When",
                        " you",
                        " give",
                        " up"
                    ],
                    "values": [
                        1.740396857261658,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        6.441860675811768,
                        20.99432754516602,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " the ways China is struggling to make itself attractive, its image tarnished by rampant pollution",
                    "max_token": " attractive",
                    "tokens": [
                        " the",
                        " ways",
                        " China",
                        " is",
                        " struggling",
                        " to",
                        " make",
                        " itself",
                        " attractive",
                        ",",
                        " its",
                        " image",
                        " tarn",
                        "ished",
                        " by",
                        " rampant",
                        " pollution"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        33.54533386230469,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " importantly because technological progress is making them more attractive, economically and politically.\u010a\u010aThis",
                    "max_token": " attractive",
                    "tokens": [
                        " importantly",
                        " because",
                        " technological",
                        " progress",
                        " is",
                        " making",
                        " them",
                        " more",
                        " attractive",
                        ",",
                        " economically",
                        " and",
                        " politically",
                        ".",
                        "\u010a",
                        "\u010a",
                        "This"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        33.14410018920898,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " with a woman that is either not as attractive or treats him like shit because she was",
                    "max_token": " attractive",
                    "tokens": [
                        " with",
                        " a",
                        " woman",
                        " that",
                        " is",
                        " either",
                        " not",
                        " as",
                        " attractive",
                        " or",
                        " treats",
                        " him",
                        " like",
                        " shit",
                        " because",
                        " she",
                        " was"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        35.94160079956055,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " recruited with the promise of decent wages, attractive bonuses and job satisfaction. What they often",
                    "max_token": " attractive",
                    "tokens": [
                        " recruited",
                        " with",
                        " the",
                        " promise",
                        " of",
                        " decent",
                        " wages",
                        ",",
                        " attractive",
                        " bonuses",
                        " and",
                        " job",
                        " satisfaction",
                        ".",
                        " What",
                        " they",
                        " often"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        33.58747100830078,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " most rich countries. But \u00e2\u0122\u013epremiumisation\u00e2\u0122\u013f is working. Though still",
                    "tokens": [
                        " most",
                        " rich",
                        " countries",
                        ".",
                        " But",
                        " \u00e2\u0122",
                        "\u013e",
                        "prem",
                        "ium",
                        "isation",
                        "\u00e2\u0122",
                        "\u013f",
                        " is",
                        " working",
                        ".",
                        " Though",
                        " still"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " outfit in the top 20. The Denver Nuggets reportedly spend \u00c2\u00a32,939,",
                    "tokens": [
                        " outfit",
                        " in",
                        " the",
                        " top",
                        " 20",
                        ".",
                        " The",
                        " Denver",
                        " Nuggets",
                        " reportedly",
                        " spend",
                        " \u00c2\u00a3",
                        "2",
                        ",",
                        "9",
                        "39",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "if Al-Arab al-Qathafi, where three of Colonel Gaddafi's grandchildren",
                    "tokens": [
                        "if",
                        " Al",
                        "-",
                        "Arab",
                        " al",
                        "-",
                        "Q",
                        "ath",
                        "afi",
                        ",",
                        " where",
                        " three",
                        " of",
                        " Colonel",
                        " Gaddafi",
                        "'s",
                        " grandchildren"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "2 14 Lubbock +4 20 Denver +2 23 Portland +2 7 Birmingham",
                    "tokens": [
                        "2",
                        " 14",
                        " L",
                        "ubb",
                        "ock",
                        " +",
                        "4",
                        " 20",
                        " Denver",
                        " +",
                        "2",
                        " 23",
                        " Portland",
                        " +",
                        "2",
                        " 7",
                        " Birmingham"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " has to be levied, it should be levied on the Central and state governments and the",
                    "tokens": [
                        " has",
                        " to",
                        " be",
                        " levied",
                        ",",
                        " it",
                        " should",
                        " be",
                        " levied",
                        " on",
                        " the",
                        " Central",
                        " and",
                        " state",
                        " governments",
                        " and",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 37.40777587890625
        },
        {
            "feature_index": 21763,
            "gpt_predictions": [
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "instances where something is missed or not present",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "\u013bs also a good chance Richardson will miss the first week or two of the season",
                    "max_token": " miss",
                    "tokens": [
                        "\u013b",
                        "s",
                        " also",
                        " a",
                        " good",
                        " chance",
                        " Richardson",
                        " will",
                        " miss",
                        " the",
                        " first",
                        " week",
                        " or",
                        " two",
                        " of",
                        " the",
                        " season"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        33.94426727294922,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "*4. \u00e2\u0122\u00a6 and the Jaguars do miss Allen Robinson. *He\u00e2\u0122\u013bs",
                    "max_token": " miss",
                    "tokens": [
                        "*",
                        "4",
                        ".",
                        " \u00e2\u0122\u00a6",
                        " and",
                        " the",
                        " Jaguars",
                        " do",
                        " miss",
                        " Allen",
                        " Robinson",
                        ".",
                        " *",
                        "He",
                        "\u00e2\u0122",
                        "\u013b",
                        "s"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        34.68500137329102,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " and featuring several players, including those which miss out onparticipation due to other commitments",
                    "max_token": " miss",
                    "tokens": [
                        " and",
                        " featuring",
                        " several",
                        " players",
                        ",",
                        " including",
                        " those",
                        " which",
                        " miss",
                        " out",
                        " on",
                        "particip",
                        "ation",
                        " due",
                        " to",
                        " other",
                        " commitments"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        38.41835784912109,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "avor judge!\u00e2\u0122\u013f\u010a\u010aI miss magic spells that aren't called \"Tact",
                    "max_token": " miss",
                    "tokens": [
                        "avor",
                        " judge",
                        "!",
                        "\u00e2\u0122",
                        "\u013f",
                        "\u010a",
                        "\u010a",
                        "I",
                        " miss",
                        " magic",
                        " spells",
                        " that",
                        " aren",
                        "'t",
                        " called",
                        " \"",
                        "Tact"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        39.90192794799805,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " everything but that LCD filter. Don't miss that. Like, digipaint struggles",
                    "max_token": " miss",
                    "tokens": [
                        " everything",
                        " but",
                        " that",
                        " LCD",
                        " filter",
                        ".",
                        " Don",
                        "'t",
                        " miss",
                        " that",
                        ".",
                        " Like",
                        ",",
                        " dig",
                        "ip",
                        "aint",
                        " struggles"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        40.75997543334961,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " most rich countries. But \u00e2\u0122\u013epremiumisation\u00e2\u0122\u013f is working. Though still",
                    "tokens": [
                        " most",
                        " rich",
                        " countries",
                        ".",
                        " But",
                        " \u00e2\u0122",
                        "\u013e",
                        "prem",
                        "ium",
                        "isation",
                        "\u00e2\u0122",
                        "\u013f",
                        " is",
                        " working",
                        ".",
                        " Though",
                        " still"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " outfit in the top 20. The Denver Nuggets reportedly spend \u00c2\u00a32,939,",
                    "tokens": [
                        " outfit",
                        " in",
                        " the",
                        " top",
                        " 20",
                        ".",
                        " The",
                        " Denver",
                        " Nuggets",
                        " reportedly",
                        " spend",
                        " \u00c2\u00a3",
                        "2",
                        ",",
                        "9",
                        "39",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "if Al-Arab al-Qathafi, where three of Colonel Gaddafi's grandchildren",
                    "tokens": [
                        "if",
                        " Al",
                        "-",
                        "Arab",
                        " al",
                        "-",
                        "Q",
                        "ath",
                        "afi",
                        ",",
                        " where",
                        " three",
                        " of",
                        " Colonel",
                        " Gaddafi",
                        "'s",
                        " grandchildren"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "2 14 Lubbock +4 20 Denver +2 23 Portland +2 7 Birmingham",
                    "tokens": [
                        "2",
                        " 14",
                        " L",
                        "ubb",
                        "ock",
                        " +",
                        "4",
                        " 20",
                        " Denver",
                        " +",
                        "2",
                        " 23",
                        " Portland",
                        " +",
                        "2",
                        " 7",
                        " Birmingham"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " has to be levied, it should be levied on the Central and state governments and the",
                    "tokens": [
                        " has",
                        " to",
                        " be",
                        " levied",
                        ",",
                        " it",
                        " should",
                        " be",
                        " levied",
                        " on",
                        " the",
                        " Central",
                        " and",
                        " state",
                        " governments",
                        " and",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 42.62459945678711
        },
        {
            "feature_index": 23835,
            "gpt_predictions": [
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "words related to improving, enhancing, and making something better",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "The code was designed to target Iranian nuclear enrichment facilities and disrupt a suspected nuclear weapons development",
                    "max_token": " enrichment",
                    "tokens": [
                        "The",
                        " code",
                        " was",
                        " designed",
                        " to",
                        " target",
                        " Iranian",
                        " nuclear",
                        " enrichment",
                        " facilities",
                        " and",
                        " disrupt",
                        " a",
                        " suspected",
                        " nuclear",
                        " weapons",
                        " development"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        24.72534942626953,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " agricultural method is carbon-negative since the enriched soil traps and holds carbon in the soil",
                    "max_token": " enriched",
                    "tokens": [
                        " agricultural",
                        " method",
                        " is",
                        " carbon",
                        "-",
                        "negative",
                        " since",
                        " the",
                        " enriched",
                        " soil",
                        " traps",
                        " and",
                        " holds",
                        " carbon",
                        " in",
                        " the",
                        " soil"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        27.95051193237305,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "jani is an indulgent who has enriched himself and his family through his access to",
                    "max_token": " enriched",
                    "tokens": [
                        "j",
                        "ani",
                        " is",
                        " an",
                        " indul",
                        "gent",
                        " who",
                        " has",
                        " enriched",
                        " himself",
                        " and",
                        " his",
                        " family",
                        " through",
                        " his",
                        " access",
                        " to"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        30.41753387451172,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "\u00e2\u0122\u013bs economic gain requires the further impoverishment of the American middle class.\u010a",
                    "max_token": " impover",
                    "tokens": [
                        "\u00e2\u0122",
                        "\u013b",
                        "s",
                        " economic",
                        " gain",
                        " requires",
                        " the",
                        " further",
                        " impover",
                        "ishment",
                        " of",
                        " the",
                        " American",
                        " middle",
                        " class",
                        ".",
                        "\u010a"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        11.25806045532227,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": ", the software designed to attack Iranian nuclear enrichment plants.\u010a\u010aThe South Korean military",
                    "max_token": " enrichment",
                    "tokens": [
                        ",",
                        " the",
                        " software",
                        " designed",
                        " to",
                        " attack",
                        " Iranian",
                        " nuclear",
                        " enrichment",
                        " plants",
                        ".",
                        "\u010a",
                        "\u010a",
                        "The",
                        " South",
                        " Korean",
                        " military"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        23.21693420410156,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " most rich countries. But \u00e2\u0122\u013epremiumisation\u00e2\u0122\u013f is working. Though still",
                    "tokens": [
                        " most",
                        " rich",
                        " countries",
                        ".",
                        " But",
                        " \u00e2\u0122",
                        "\u013e",
                        "prem",
                        "ium",
                        "isation",
                        "\u00e2\u0122",
                        "\u013f",
                        " is",
                        " working",
                        ".",
                        " Though",
                        " still"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " outfit in the top 20. The Denver Nuggets reportedly spend \u00c2\u00a32,939,",
                    "tokens": [
                        " outfit",
                        " in",
                        " the",
                        " top",
                        " 20",
                        ".",
                        " The",
                        " Denver",
                        " Nuggets",
                        " reportedly",
                        " spend",
                        " \u00c2\u00a3",
                        "2",
                        ",",
                        "9",
                        "39",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "if Al-Arab al-Qathafi, where three of Colonel Gaddafi's grandchildren",
                    "tokens": [
                        "if",
                        " Al",
                        "-",
                        "Arab",
                        " al",
                        "-",
                        "Q",
                        "ath",
                        "afi",
                        ",",
                        " where",
                        " three",
                        " of",
                        " Colonel",
                        " Gaddafi",
                        "'s",
                        " grandchildren"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "2 14 Lubbock +4 20 Denver +2 23 Portland +2 7 Birmingham",
                    "tokens": [
                        "2",
                        " 14",
                        " L",
                        "ubb",
                        "ock",
                        " +",
                        "4",
                        " 20",
                        " Denver",
                        " +",
                        "2",
                        " 23",
                        " Portland",
                        " +",
                        "2",
                        " 7",
                        " Birmingham"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " has to be levied, it should be levied on the Central and state governments and the",
                    "tokens": [
                        " has",
                        " to",
                        " be",
                        " levied",
                        ",",
                        " it",
                        " should",
                        " be",
                        " levied",
                        " on",
                        " the",
                        " Central",
                        " and",
                        " state",
                        " governments",
                        " and",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 34.02994155883789
        },
        {
            "feature_index": 3521,
            "gpt_predictions": [
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "immigration-related terms",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " repeat offenders able to exploit holes in our immigration laws. Nicodemo Coria-",
                    "max_token": " immigration",
                    "tokens": [
                        " repeat",
                        " offenders",
                        " able",
                        " to",
                        " exploit",
                        " holes",
                        " in",
                        " our",
                        " immigration",
                        " laws",
                        ".",
                        " Nic",
                        "od",
                        "emo",
                        " Cor",
                        "ia",
                        "-"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        41.63031768798828,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " Statue of Liberty has nothing to do with immigration. Or that pregnant women with jobs were",
                    "max_token": " immigration",
                    "tokens": [
                        " Statue",
                        " of",
                        " Liberty",
                        " has",
                        " nothing",
                        " to",
                        " do",
                        " with",
                        " immigration",
                        ".",
                        " Or",
                        " that",
                        " pregnant",
                        " women",
                        " with",
                        " jobs",
                        " were"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        40.07115173339844,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " in general: they typically don't like immigration restrictions, the same way other conservative constituency",
                    "max_token": " immigration",
                    "tokens": [
                        " in",
                        " general",
                        ":",
                        " they",
                        " typically",
                        " don",
                        "'t",
                        " like",
                        " immigration",
                        " restrictions",
                        ",",
                        " the",
                        " same",
                        " way",
                        " other",
                        " conservative",
                        " constituency"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        38.33876037597656,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " of this agenda you may advocate stopping further immigration to the west, but you will still",
                    "max_token": " immigration",
                    "tokens": [
                        " of",
                        " this",
                        " agenda",
                        " you",
                        " may",
                        " advocate",
                        " stopping",
                        " further",
                        " immigration",
                        " to",
                        " the",
                        " west",
                        ",",
                        " but",
                        " you",
                        " will",
                        " still"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        41.8079719543457,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " and state is absolute.\u00e2\u0122\u013f On immigration, the issue that will define the Republican",
                    "max_token": " immigration",
                    "tokens": [
                        " and",
                        " state",
                        " is",
                        " absolute",
                        ".",
                        "\u00e2\u0122",
                        "\u013f",
                        " On",
                        " immigration",
                        ",",
                        " the",
                        " issue",
                        " that",
                        " will",
                        " define",
                        " the",
                        " Republican"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        40.43438339233398,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " most rich countries. But \u00e2\u0122\u013epremiumisation\u00e2\u0122\u013f is working. Though still",
                    "tokens": [
                        " most",
                        " rich",
                        " countries",
                        ".",
                        " But",
                        " \u00e2\u0122",
                        "\u013e",
                        "prem",
                        "ium",
                        "isation",
                        "\u00e2\u0122",
                        "\u013f",
                        " is",
                        " working",
                        ".",
                        " Though",
                        " still"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " outfit in the top 20. The Denver Nuggets reportedly spend \u00c2\u00a32,939,",
                    "tokens": [
                        " outfit",
                        " in",
                        " the",
                        " top",
                        " 20",
                        ".",
                        " The",
                        " Denver",
                        " Nuggets",
                        " reportedly",
                        " spend",
                        " \u00c2\u00a3",
                        "2",
                        ",",
                        "9",
                        "39",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "if Al-Arab al-Qathafi, where three of Colonel Gaddafi's grandchildren",
                    "tokens": [
                        "if",
                        " Al",
                        "-",
                        "Arab",
                        " al",
                        "-",
                        "Q",
                        "ath",
                        "afi",
                        ",",
                        " where",
                        " three",
                        " of",
                        " Colonel",
                        " Gaddafi",
                        "'s",
                        " grandchildren"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "2 14 Lubbock +4 20 Denver +2 23 Portland +2 7 Birmingham",
                    "tokens": [
                        "2",
                        " 14",
                        " L",
                        "ubb",
                        "ock",
                        " +",
                        "4",
                        " 20",
                        " Denver",
                        " +",
                        "2",
                        " 23",
                        " Portland",
                        " +",
                        "2",
                        " 7",
                        " Birmingham"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " has to be levied, it should be levied on the Central and state governments and the",
                    "tokens": [
                        " has",
                        " to",
                        " be",
                        " levied",
                        ",",
                        " it",
                        " should",
                        " be",
                        " levied",
                        " on",
                        " the",
                        " Central",
                        " and",
                        " state",
                        " governments",
                        " and",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 43.7928581237793
        },
        {
            "feature_index": 10543,
            "gpt_predictions": [
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "adverbs and adjectives ending in 'ly' or words related to something being loosely defined",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": ". The Tremble portions of the film vaguely follow the actual story of Casino Royale:",
                    "max_token": " vaguely",
                    "tokens": [
                        ".",
                        " The",
                        " Trem",
                        "ble",
                        " portions",
                        " of",
                        " the",
                        " film",
                        " vaguely",
                        " follow",
                        " the",
                        " actual",
                        " story",
                        " of",
                        " Casino",
                        " Royale",
                        ":"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        9.171499252319336,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " national team, his motor, which I loosely define as a player\u00e2\u0122\u013bs give",
                    "max_token": " loosely",
                    "tokens": [
                        " national",
                        " team",
                        ",",
                        " his",
                        " motor",
                        ",",
                        " which",
                        " I",
                        " loosely",
                        " define",
                        " as",
                        " a",
                        " player",
                        "\u00e2\u0122",
                        "\u013b",
                        "s",
                        " give"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        24.28319549560547,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "azontles, a green vegetable that vaguely resembles broccoli, with chiapas cheese",
                    "max_token": " vaguely",
                    "tokens": [
                        "az",
                        "ont",
                        "les",
                        ",",
                        " a",
                        " green",
                        " vegetable",
                        " that",
                        " vaguely",
                        " resembles",
                        " broccoli",
                        ",",
                        " with",
                        " chi",
                        "ap",
                        "as",
                        " cheese"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        8.294873237609863,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " through the top; and\u010a\u010aa telescoping ladder, which uses a pin system",
                    "max_token": " telesc",
                    "tokens": [
                        " through",
                        " the",
                        " top",
                        ";",
                        " and",
                        "\u010a",
                        "\u010a",
                        "a",
                        " telesc",
                        "oping",
                        " ladder",
                        ",",
                        " which",
                        " uses",
                        " a",
                        " pin",
                        " system"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        2.767462253570557,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " English accent\u010a\u010a\"The North sounds vaguely Scottish to me, Robert sounded like he",
                    "max_token": " vaguely",
                    "tokens": [
                        " English",
                        " accent",
                        "\u010a",
                        "\u010a",
                        "\"",
                        "The",
                        " North",
                        " sounds",
                        " vaguely",
                        " Scottish",
                        " to",
                        " me",
                        ",",
                        " Robert",
                        " sounded",
                        " like",
                        " he"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        9.389006614685059,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " most rich countries. But \u00e2\u0122\u013epremiumisation\u00e2\u0122\u013f is working. Though still",
                    "tokens": [
                        " most",
                        " rich",
                        " countries",
                        ".",
                        " But",
                        " \u00e2\u0122",
                        "\u013e",
                        "prem",
                        "ium",
                        "isation",
                        "\u00e2\u0122",
                        "\u013f",
                        " is",
                        " working",
                        ".",
                        " Though",
                        " still"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " outfit in the top 20. The Denver Nuggets reportedly spend \u00c2\u00a32,939,",
                    "tokens": [
                        " outfit",
                        " in",
                        " the",
                        " top",
                        " 20",
                        ".",
                        " The",
                        " Denver",
                        " Nuggets",
                        " reportedly",
                        " spend",
                        " \u00c2\u00a3",
                        "2",
                        ",",
                        "9",
                        "39",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "if Al-Arab al-Qathafi, where three of Colonel Gaddafi's grandchildren",
                    "tokens": [
                        "if",
                        " Al",
                        "-",
                        "Arab",
                        " al",
                        "-",
                        "Q",
                        "ath",
                        "afi",
                        ",",
                        " where",
                        " three",
                        " of",
                        " Colonel",
                        " Gaddafi",
                        "'s",
                        " grandchildren"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "2 14 Lubbock +4 20 Denver +2 23 Portland +2 7 Birmingham",
                    "tokens": [
                        "2",
                        " 14",
                        " L",
                        "ubb",
                        "ock",
                        " +",
                        "4",
                        " 20",
                        " Denver",
                        " +",
                        "2",
                        " 23",
                        " Portland",
                        " +",
                        "2",
                        " 7",
                        " Birmingham"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " has to be levied, it should be levied on the Central and state governments and the",
                    "tokens": [
                        " has",
                        " to",
                        " be",
                        " levied",
                        ",",
                        " it",
                        " should",
                        " be",
                        " levied",
                        " on",
                        " the",
                        " Central",
                        " and",
                        " state",
                        " governments",
                        " and",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 27.09649085998535
        },
        {
            "feature_index": 21099,
            "gpt_predictions": [
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "references to a specific deity or place named \"Baal\"",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "verent skeptic and satirical poet, Baal. When the prophet returns to the city",
                    "max_token": " Baal",
                    "tokens": [
                        "ve",
                        "rent",
                        " skept",
                        "ic",
                        " and",
                        " satirical",
                        " poet",
                        ",",
                        " Baal",
                        ".",
                        " When",
                        " the",
                        " prophet",
                        " returns",
                        " to",
                        " the",
                        " city"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        7.281051635742188,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " Jensen pointed out, the monolith at Baalbek is considered to be one of the",
                    "max_token": " Baal",
                    "tokens": [
                        " Jensen",
                        " pointed",
                        " out",
                        ",",
                        " the",
                        " mon",
                        "olith",
                        " at",
                        " Baal",
                        "bek",
                        " is",
                        " considered",
                        " to",
                        " be",
                        " one",
                        " of",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        7.565923690795898,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " the ancient \u00e2\u0122\u013emoon god\u00e2\u0122\u013f Baal, which is why Islam is represented by",
                    "max_token": " Baal",
                    "tokens": [
                        " the",
                        " ancient",
                        " \u00e2\u0122",
                        "\u013e",
                        "moon",
                        " god",
                        "\u00e2\u0122",
                        "\u013f",
                        " Baal",
                        ",",
                        " which",
                        " is",
                        " why",
                        " Islam",
                        " is",
                        " represented",
                        " by"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        7.553182125091553,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " used to go around by the name of Baal,\u00e2\u0122\u013f he said. \u00e2\u0122\u013e",
                    "max_token": " Baal",
                    "tokens": [
                        " used",
                        " to",
                        " go",
                        " around",
                        " by",
                        " the",
                        " name",
                        " of",
                        " Baal",
                        ",",
                        "\u00e2\u0122",
                        "\u013f",
                        " he",
                        " said",
                        ".",
                        " \u00e2\u0122",
                        "\u013e"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        7.178238391876221,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": ": The Pregnant Woman Stone of Baalbek, Lebanon weighs in at approximately 1",
                    "max_token": " Baal",
                    "tokens": [
                        ":",
                        " The",
                        " P",
                        "reg",
                        "nant",
                        " Woman",
                        " Stone",
                        " of",
                        " Baal",
                        "bek",
                        ",",
                        " Lebanon",
                        " weighs",
                        " in",
                        " at",
                        " approximately",
                        " 1"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        8.291800498962402,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " most rich countries. But \u00e2\u0122\u013epremiumisation\u00e2\u0122\u013f is working. Though still",
                    "tokens": [
                        " most",
                        " rich",
                        " countries",
                        ".",
                        " But",
                        " \u00e2\u0122",
                        "\u013e",
                        "prem",
                        "ium",
                        "isation",
                        "\u00e2\u0122",
                        "\u013f",
                        " is",
                        " working",
                        ".",
                        " Though",
                        " still"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " outfit in the top 20. The Denver Nuggets reportedly spend \u00c2\u00a32,939,",
                    "tokens": [
                        " outfit",
                        " in",
                        " the",
                        " top",
                        " 20",
                        ".",
                        " The",
                        " Denver",
                        " Nuggets",
                        " reportedly",
                        " spend",
                        " \u00c2\u00a3",
                        "2",
                        ",",
                        "9",
                        "39",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "if Al-Arab al-Qathafi, where three of Colonel Gaddafi's grandchildren",
                    "tokens": [
                        "if",
                        " Al",
                        "-",
                        "Arab",
                        " al",
                        "-",
                        "Q",
                        "ath",
                        "afi",
                        ",",
                        " where",
                        " three",
                        " of",
                        " Colonel",
                        " Gaddafi",
                        "'s",
                        " grandchildren"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "2 14 Lubbock +4 20 Denver +2 23 Portland +2 7 Birmingham",
                    "tokens": [
                        "2",
                        " 14",
                        " L",
                        "ubb",
                        "ock",
                        " +",
                        "4",
                        " 20",
                        " Denver",
                        " +",
                        "2",
                        " 23",
                        " Portland",
                        " +",
                        "2",
                        " 7",
                        " Birmingham"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " has to be levied, it should be levied on the Central and state governments and the",
                    "tokens": [
                        " has",
                        " to",
                        " be",
                        " levied",
                        ",",
                        " it",
                        " should",
                        " be",
                        " levied",
                        " on",
                        " the",
                        " Central",
                        " and",
                        " state",
                        " governments",
                        " and",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 8.291800498962402
        },
        {
            "feature_index": 11187,
            "gpt_predictions": [
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "HTML tags with attributes and values",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "ProcEnviron:\u010a\u010aPATH=(custom, user)\u010a\u010aLANG",
                    "max_token": "=(",
                    "tokens": [
                        "Pro",
                        "c",
                        "En",
                        "viron",
                        ":",
                        "\u010a",
                        "\u010a",
                        "PATH",
                        "=(",
                        "custom",
                        ",",
                        " user",
                        ")",
                        "\u010a",
                        "\u010a",
                        "L",
                        "ANG"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        16.00184440612793,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "zo5- xcfi&ie= UTF-8&sa= Search&",
                    "max_token": "=",
                    "tokens": [
                        "zo",
                        "5",
                        "-",
                        " x",
                        "c",
                        "fi",
                        "&",
                        "ie",
                        "=",
                        " UTF",
                        "-",
                        "8",
                        "&",
                        "sa",
                        "=",
                        " Search",
                        "&"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        29.6142749786377,
                        0,
                        0,
                        0,
                        0,
                        0,
                        27.99608039855957,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " EDD in O control mice (93\u00c2\u00b13%), but had no effect in Y",
                    "max_token": "\u00c2\u00b1",
                    "tokens": [
                        " ED",
                        "D",
                        " in",
                        " O",
                        " control",
                        " mice",
                        " (",
                        "93",
                        "\u00c2\u00b1",
                        "3",
                        "%),",
                        " but",
                        " had",
                        " no",
                        " effect",
                        " in",
                        " Y"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        6.709304809570312,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 0,
                    "sentence_string": "=demand_widget&utm_campaign=learn_more_eventful_P",
                    "max_token": "=",
                    "tokens": [
                        "=",
                        "demand",
                        "_",
                        "widget",
                        "&",
                        "utm",
                        "_",
                        "campaign",
                        "=",
                        "learn",
                        "_",
                        "more",
                        "_",
                        "event",
                        "ful",
                        "_",
                        "P"
                    ],
                    "values": [
                        30.84126091003418,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        30.41272926330566,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 11,
                    "sentence_string": "com verify return:1 DONE subject= /C=US/ST=California",
                    "max_token": "=",
                    "tokens": [
                        "com",
                        " verify",
                        " return",
                        ":",
                        "1",
                        " D",
                        "ONE",
                        " subject",
                        "=",
                        " /",
                        "C",
                        "=",
                        "US",
                        "/",
                        "ST",
                        "=",
                        "California"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        30.38249206542969,
                        0,
                        0,
                        30.47407722473145,
                        0,
                        0,
                        0,
                        28.64343070983887,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " most rich countries. But \u00e2\u0122\u013epremiumisation\u00e2\u0122\u013f is working. Though still",
                    "tokens": [
                        " most",
                        " rich",
                        " countries",
                        ".",
                        " But",
                        " \u00e2\u0122",
                        "\u013e",
                        "prem",
                        "ium",
                        "isation",
                        "\u00e2\u0122",
                        "\u013f",
                        " is",
                        " working",
                        ".",
                        " Though",
                        " still"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " outfit in the top 20. The Denver Nuggets reportedly spend \u00c2\u00a32,939,",
                    "tokens": [
                        " outfit",
                        " in",
                        " the",
                        " top",
                        " 20",
                        ".",
                        " The",
                        " Denver",
                        " Nuggets",
                        " reportedly",
                        " spend",
                        " \u00c2\u00a3",
                        "2",
                        ",",
                        "9",
                        "39",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "if Al-Arab al-Qathafi, where three of Colonel Gaddafi's grandchildren",
                    "tokens": [
                        "if",
                        " Al",
                        "-",
                        "Arab",
                        " al",
                        "-",
                        "Q",
                        "ath",
                        "afi",
                        ",",
                        " where",
                        " three",
                        " of",
                        " Colonel",
                        " Gaddafi",
                        "'s",
                        " grandchildren"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "2 14 Lubbock +4 20 Denver +2 23 Portland +2 7 Birmingham",
                    "tokens": [
                        "2",
                        " 14",
                        " L",
                        "ubb",
                        "ock",
                        " +",
                        "4",
                        " 20",
                        " Denver",
                        " +",
                        "2",
                        " 23",
                        " Portland",
                        " +",
                        "2",
                        " 7",
                        " Birmingham"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " has to be levied, it should be levied on the Central and state governments and the",
                    "tokens": [
                        " has",
                        " to",
                        " be",
                        " levied",
                        ",",
                        " it",
                        " should",
                        " be",
                        " levied",
                        " on",
                        " the",
                        " Central",
                        " and",
                        " state",
                        " governments",
                        " and",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 32.68033981323242
        },
        {
            "feature_index": 5246,
            "gpt_predictions": [
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "mentions related to football player David Beckham",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " looked into it. (Reporting by Tom Sims; additional reporting by Pete Schroeder in",
                    "max_token": " Sims",
                    "tokens": [
                        " looked",
                        " into",
                        " it",
                        ".",
                        " (",
                        "Reporting",
                        " by",
                        " Tom",
                        " Sims",
                        ";",
                        " additional",
                        " reporting",
                        " by",
                        " Pete",
                        " Schro",
                        "eder",
                        " in"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        6.60434103012085,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "), Steven Gerrard (13) and David Beckham (12) had managed by the same",
                    "max_token": " Beckham",
                    "tokens": [
                        "),",
                        " Steven",
                        " Gerrard",
                        " (",
                        "13",
                        ")",
                        " and",
                        " David",
                        " Beckham",
                        " (",
                        "12",
                        ")",
                        " had",
                        " managed",
                        " by",
                        " the",
                        " same"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        41.38224029541016,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " the rule was designed to attract \u00e2\u0122\u0135 David Beckham and all the rest, whose magnetism",
                    "max_token": " Beckham",
                    "tokens": [
                        " the",
                        " rule",
                        " was",
                        " designed",
                        " to",
                        " attract",
                        " \u00e2\u0122\u0135",
                        " David",
                        " Beckham",
                        " and",
                        " all",
                        " the",
                        " rest",
                        ",",
                        " whose",
                        " magnet",
                        "ism"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        44.11223602294922,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " Klein flanked by Assistant City Manager Larisa Sims (left) and Director of Operations Lisa",
                    "max_token": " Sims",
                    "tokens": [
                        " Klein",
                        " flanked",
                        " by",
                        " Assistant",
                        " City",
                        " Manager",
                        " Lar",
                        "isa",
                        " Sims",
                        " (",
                        "left",
                        ")",
                        " and",
                        " Director",
                        " of",
                        " Operations",
                        " Lisa"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        7.359272003173828,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": ".\u010a\u010aSue sustained serious<|endoftext|>Beck: I Could Give A Flying Crap",
                    "max_token": "Beck",
                    "tokens": [
                        ".",
                        "\u010a",
                        "\u010a",
                        "S",
                        "ue",
                        " sustained",
                        " serious",
                        "<|endoftext|>",
                        "Beck",
                        ":",
                        " I",
                        " Could",
                        " Give",
                        " A",
                        " Flying",
                        " C",
                        "rap"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        11.41386222839355,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " most rich countries. But \u00e2\u0122\u013epremiumisation\u00e2\u0122\u013f is working. Though still",
                    "tokens": [
                        " most",
                        " rich",
                        " countries",
                        ".",
                        " But",
                        " \u00e2\u0122",
                        "\u013e",
                        "prem",
                        "ium",
                        "isation",
                        "\u00e2\u0122",
                        "\u013f",
                        " is",
                        " working",
                        ".",
                        " Though",
                        " still"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " outfit in the top 20. The Denver Nuggets reportedly spend \u00c2\u00a32,939,",
                    "tokens": [
                        " outfit",
                        " in",
                        " the",
                        " top",
                        " 20",
                        ".",
                        " The",
                        " Denver",
                        " Nuggets",
                        " reportedly",
                        " spend",
                        " \u00c2\u00a3",
                        "2",
                        ",",
                        "9",
                        "39",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "if Al-Arab al-Qathafi, where three of Colonel Gaddafi's grandchildren",
                    "tokens": [
                        "if",
                        " Al",
                        "-",
                        "Arab",
                        " al",
                        "-",
                        "Q",
                        "ath",
                        "afi",
                        ",",
                        " where",
                        " three",
                        " of",
                        " Colonel",
                        " Gaddafi",
                        "'s",
                        " grandchildren"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "2 14 Lubbock +4 20 Denver +2 23 Portland +2 7 Birmingham",
                    "tokens": [
                        "2",
                        " 14",
                        " L",
                        "ubb",
                        "ock",
                        " +",
                        "4",
                        " 20",
                        " Denver",
                        " +",
                        "2",
                        " 23",
                        " Portland",
                        " +",
                        "2",
                        " 7",
                        " Birmingham"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " has to be levied, it should be levied on the Central and state governments and the",
                    "tokens": [
                        " has",
                        " to",
                        " be",
                        " levied",
                        ",",
                        " it",
                        " should",
                        " be",
                        " levied",
                        " on",
                        " the",
                        " Central",
                        " and",
                        " state",
                        " governments",
                        " and",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 45.10490036010742
        },
        {
            "feature_index": 6176,
            "gpt_predictions": [
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "references to events or actions occurring after a person's death",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " into deposits of fat. During the exhumation, this fat was collected and subsequently",
                    "max_token": "hum",
                    "tokens": [
                        " into",
                        " deposits",
                        " of",
                        " fat",
                        ".",
                        " During",
                        " the",
                        " ex",
                        "hum",
                        "ation",
                        ",",
                        " this",
                        " fat",
                        " was",
                        " collected",
                        " and",
                        " subsequently"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        27.4642276763916,
                        2.723137378692627,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " Sir Henry Morton Stanley is a brilliant posthumous capstone to the Pulitzer Prize winner",
                    "max_token": "hum",
                    "tokens": [
                        " Sir",
                        " Henry",
                        " Morton",
                        " Stanley",
                        " is",
                        " a",
                        " brilliant",
                        " post",
                        "hum",
                        "ous",
                        " cap",
                        "stone",
                        " to",
                        " the",
                        " Pulitzer",
                        " Prize",
                        " winner"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        30.6468620300293,
                        4.632859706878662,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " will be friendly, outgoing, good-humoured and will enjoy helping others.\"\u010a",
                    "max_token": "hum",
                    "tokens": [
                        " will",
                        " be",
                        " friendly",
                        ",",
                        " outgoing",
                        ",",
                        " good",
                        "-",
                        "hum",
                        "oured",
                        " and",
                        " will",
                        " enjoy",
                        " helping",
                        " others",
                        ".\"",
                        "\u010a"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        23.04550743103027,
                        0.8836627006530762,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " received asylum status, while others have been repatriated to other African countries in a scheme",
                    "max_token": " repatri",
                    "tokens": [
                        " received",
                        " asylum",
                        " status",
                        ",",
                        " while",
                        " others",
                        " have",
                        " been",
                        " repatri",
                        "ated",
                        " to",
                        " other",
                        " African",
                        " countries",
                        " in",
                        " a",
                        " scheme"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        3.359622478485107,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " denies any involvement.\u010a\u010aThe exhumation and renewed allegations of Israeli involvement could",
                    "max_token": "hum",
                    "tokens": [
                        " denies",
                        " any",
                        " involvement",
                        ".",
                        "\u010a",
                        "\u010a",
                        "The",
                        " ex",
                        "hum",
                        "ation",
                        " and",
                        " renewed",
                        " allegations",
                        " of",
                        " Israeli",
                        " involvement",
                        " could"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        28.3707447052002,
                        3.195479154586792,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " most rich countries. But \u00e2\u0122\u013epremiumisation\u00e2\u0122\u013f is working. Though still",
                    "tokens": [
                        " most",
                        " rich",
                        " countries",
                        ".",
                        " But",
                        " \u00e2\u0122",
                        "\u013e",
                        "prem",
                        "ium",
                        "isation",
                        "\u00e2\u0122",
                        "\u013f",
                        " is",
                        " working",
                        ".",
                        " Though",
                        " still"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " outfit in the top 20. The Denver Nuggets reportedly spend \u00c2\u00a32,939,",
                    "tokens": [
                        " outfit",
                        " in",
                        " the",
                        " top",
                        " 20",
                        ".",
                        " The",
                        " Denver",
                        " Nuggets",
                        " reportedly",
                        " spend",
                        " \u00c2\u00a3",
                        "2",
                        ",",
                        "9",
                        "39",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "if Al-Arab al-Qathafi, where three of Colonel Gaddafi's grandchildren",
                    "tokens": [
                        "if",
                        " Al",
                        "-",
                        "Arab",
                        " al",
                        "-",
                        "Q",
                        "ath",
                        "afi",
                        ",",
                        " where",
                        " three",
                        " of",
                        " Colonel",
                        " Gaddafi",
                        "'s",
                        " grandchildren"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "2 14 Lubbock +4 20 Denver +2 23 Portland +2 7 Birmingham",
                    "tokens": [
                        "2",
                        " 14",
                        " L",
                        "ubb",
                        "ock",
                        " +",
                        "4",
                        " 20",
                        " Denver",
                        " +",
                        "2",
                        " 23",
                        " Portland",
                        " +",
                        "2",
                        " 7",
                        " Birmingham"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " has to be levied, it should be levied on the Central and state governments and the",
                    "tokens": [
                        " has",
                        " to",
                        " be",
                        " levied",
                        ",",
                        " it",
                        " should",
                        " be",
                        " levied",
                        " on",
                        " the",
                        " Central",
                        " and",
                        " state",
                        " governments",
                        " and",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 31.94747543334961
        },
        {
            "feature_index": 19321,
            "gpt_predictions": [
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "references to punk music or culture",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " grime of the city\u00e2\u0122\u013bs punk heyday are captured in gorgeous detail as",
                    "max_token": " punk",
                    "tokens": [
                        " gr",
                        "ime",
                        " of",
                        " the",
                        " city",
                        "\u00e2\u0122",
                        "\u013b",
                        "s",
                        " punk",
                        " hey",
                        "day",
                        " are",
                        " captured",
                        " in",
                        " gorgeous",
                        " detail",
                        " as"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        40.16174697875977,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " smiles and some of the best music the punk/emo/hardcore scenes have to",
                    "max_token": " punk",
                    "tokens": [
                        " smiles",
                        " and",
                        " some",
                        " of",
                        " the",
                        " best",
                        " music",
                        " the",
                        " punk",
                        "/",
                        "emo",
                        "/",
                        "hard",
                        "core",
                        " scenes",
                        " have",
                        " to"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        39.51848983764648,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " our 30\u00c2\u00b4s who play in a punk band. We are all stage hands at",
                    "max_token": " punk",
                    "tokens": [
                        " our",
                        " 30",
                        "\u00c2\u00b4",
                        "s",
                        " who",
                        " play",
                        " in",
                        " a",
                        " punk",
                        " band",
                        ".",
                        " We",
                        " are",
                        " all",
                        " stage",
                        " hands",
                        " at"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        40.52163314819336,
                        0.3162165880203247,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " interesting how Bryan doesn\u00e2\u0122\u013bt take Punk seriously, yet desperately wants to win the",
                    "max_token": " Punk",
                    "tokens": [
                        " interesting",
                        " how",
                        " Bryan",
                        " doesn",
                        "\u00e2\u0122",
                        "\u013b",
                        "t",
                        " take",
                        " Punk",
                        " seriously",
                        ",",
                        " yet",
                        " desperately",
                        " wants",
                        " to",
                        " win",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        24.00358581542969,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " - which happens to be Jason Wright's stomping grounds - and Jason has been successful",
                    "max_token": " stomp",
                    "tokens": [
                        " -",
                        " which",
                        " happens",
                        " to",
                        " be",
                        " Jason",
                        " Wright",
                        "'s",
                        " stomp",
                        "ing",
                        " grounds",
                        " -",
                        " and",
                        " Jason",
                        " has",
                        " been",
                        " successful"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        5.17390251159668,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " most rich countries. But \u00e2\u0122\u013epremiumisation\u00e2\u0122\u013f is working. Though still",
                    "tokens": [
                        " most",
                        " rich",
                        " countries",
                        ".",
                        " But",
                        " \u00e2\u0122",
                        "\u013e",
                        "prem",
                        "ium",
                        "isation",
                        "\u00e2\u0122",
                        "\u013f",
                        " is",
                        " working",
                        ".",
                        " Though",
                        " still"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " outfit in the top 20. The Denver Nuggets reportedly spend \u00c2\u00a32,939,",
                    "tokens": [
                        " outfit",
                        " in",
                        " the",
                        " top",
                        " 20",
                        ".",
                        " The",
                        " Denver",
                        " Nuggets",
                        " reportedly",
                        " spend",
                        " \u00c2\u00a3",
                        "2",
                        ",",
                        "9",
                        "39",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "if Al-Arab al-Qathafi, where three of Colonel Gaddafi's grandchildren",
                    "tokens": [
                        "if",
                        " Al",
                        "-",
                        "Arab",
                        " al",
                        "-",
                        "Q",
                        "ath",
                        "afi",
                        ",",
                        " where",
                        " three",
                        " of",
                        " Colonel",
                        " Gaddafi",
                        "'s",
                        " grandchildren"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "2 14 Lubbock +4 20 Denver +2 23 Portland +2 7 Birmingham",
                    "tokens": [
                        "2",
                        " 14",
                        " L",
                        "ubb",
                        "ock",
                        " +",
                        "4",
                        " 20",
                        " Denver",
                        " +",
                        "2",
                        " 23",
                        " Portland",
                        " +",
                        "2",
                        " 7",
                        " Birmingham"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " has to be levied, it should be levied on the Central and state governments and the",
                    "tokens": [
                        " has",
                        " to",
                        " be",
                        " levied",
                        ",",
                        " it",
                        " should",
                        " be",
                        " levied",
                        " on",
                        " the",
                        " Central",
                        " and",
                        " state",
                        " governments",
                        " and",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 41.97684478759766
        },
        {
            "feature_index": 10837,
            "gpt_predictions": [
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "phrases related to a fictional location named Equestria",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "th Soviet Army based in Transnistria at the time intervened, defeating Moldovan",
                    "max_token": "ria",
                    "tokens": [
                        "th",
                        " Soviet",
                        " Army",
                        " based",
                        " in",
                        " Trans",
                        "n",
                        "ist",
                        "ria",
                        " at",
                        " the",
                        " time",
                        " intervened",
                        ",",
                        " defeating",
                        " Mold",
                        "ovan"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        32.37870788574219,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " cannot condone these works,\u00e2\u0122\u013f says Petra Reetz, a spokeswoman for the company",
                    "max_token": " Petra",
                    "tokens": [
                        " cannot",
                        " condone",
                        " these",
                        " works",
                        ",",
                        "\u00e2\u0122",
                        "\u013f",
                        " says",
                        " Petra",
                        " Re",
                        "etz",
                        ",",
                        " a",
                        " spokeswoman",
                        " for",
                        " the",
                        " company"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        5.597376346588135,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " Stable 150.Fallout Equestria: SunriseIn the FoE Universe.",
                    "max_token": "ria",
                    "tokens": [
                        " St",
                        "able",
                        " 150",
                        ".",
                        "Fall",
                        "out",
                        " E",
                        "quest",
                        "ria",
                        ":",
                        " Sunrise",
                        "In",
                        " the",
                        " Fo",
                        "E",
                        " Universe",
                        "."
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        32.42445755004883,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " migrants who arrived at the Hungarian-Austria border by train make their way into Austria",
                    "max_token": "ria",
                    "tokens": [
                        " migrants",
                        " who",
                        " arrived",
                        " at",
                        " the",
                        " Hungarian",
                        "-",
                        "Aust",
                        "ria",
                        " border",
                        " by",
                        " train",
                        " make",
                        " their",
                        " way",
                        " into",
                        " Austria"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        14.81272506713867,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " as a result of the plight of Arrium.\u010a\u010aThe company is in voluntary",
                    "max_token": "rium",
                    "tokens": [
                        " as",
                        " a",
                        " result",
                        " of",
                        " the",
                        " plight",
                        " of",
                        " Ar",
                        "rium",
                        ".",
                        "\u010a",
                        "\u010a",
                        "The",
                        " company",
                        " is",
                        " in",
                        " voluntary"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        5.404349803924561,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " most rich countries. But \u00e2\u0122\u013epremiumisation\u00e2\u0122\u013f is working. Though still",
                    "tokens": [
                        " most",
                        " rich",
                        " countries",
                        ".",
                        " But",
                        " \u00e2\u0122",
                        "\u013e",
                        "prem",
                        "ium",
                        "isation",
                        "\u00e2\u0122",
                        "\u013f",
                        " is",
                        " working",
                        ".",
                        " Though",
                        " still"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " outfit in the top 20. The Denver Nuggets reportedly spend \u00c2\u00a32,939,",
                    "tokens": [
                        " outfit",
                        " in",
                        " the",
                        " top",
                        " 20",
                        ".",
                        " The",
                        " Denver",
                        " Nuggets",
                        " reportedly",
                        " spend",
                        " \u00c2\u00a3",
                        "2",
                        ",",
                        "9",
                        "39",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "if Al-Arab al-Qathafi, where three of Colonel Gaddafi's grandchildren",
                    "tokens": [
                        "if",
                        " Al",
                        "-",
                        "Arab",
                        " al",
                        "-",
                        "Q",
                        "ath",
                        "afi",
                        ",",
                        " where",
                        " three",
                        " of",
                        " Colonel",
                        " Gaddafi",
                        "'s",
                        " grandchildren"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "2 14 Lubbock +4 20 Denver +2 23 Portland +2 7 Birmingham",
                    "tokens": [
                        "2",
                        " 14",
                        " L",
                        "ubb",
                        "ock",
                        " +",
                        "4",
                        " 20",
                        " Denver",
                        " +",
                        "2",
                        " 23",
                        " Portland",
                        " +",
                        "2",
                        " 7",
                        " Birmingham"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " has to be levied, it should be levied on the Central and state governments and the",
                    "tokens": [
                        " has",
                        " to",
                        " be",
                        " levied",
                        ",",
                        " it",
                        " should",
                        " be",
                        " levied",
                        " on",
                        " the",
                        " Central",
                        " and",
                        " state",
                        " governments",
                        " and",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 35.09333038330078
        },
        {
            "feature_index": 20072,
            "gpt_predictions": [
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "parts of a text that suggest a narrative about following a path while avoiding obstacles or challenges",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "ah's London marker\u010a\u010aFarah's time was only four seconds<|endoftext|>\u010a\"",
                    "max_token": "'s",
                    "tokens": [
                        "ah",
                        "'s",
                        " London",
                        " marker",
                        "\u010a",
                        "\u010a",
                        "Far",
                        "ah",
                        "'s",
                        " time",
                        " was",
                        " only",
                        " four",
                        " seconds",
                        "<|endoftext|>",
                        "\u010a",
                        "\""
                    ],
                    "values": [
                        0,
                        2.103182792663574,
                        0,
                        0,
                        0,
                        0.5207162499427795,
                        0,
                        0,
                        2.852313041687012,
                        0,
                        2.556939125061035,
                        0.9348845481872559,
                        0.6472107172012329,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 13,
                    "sentence_string": " priority\u010a\u010a\u00e2\u0122\u00a2 MORE: 5 things to know about Archie Miller<|endoftext|>LOS ANGELES",
                    "max_token": "<|endoftext|>",
                    "tokens": [
                        " priority",
                        "\u010a",
                        "\u010a",
                        "\u00e2\u0122\u00a2",
                        " MORE",
                        ":",
                        " 5",
                        " things",
                        " to",
                        " know",
                        " about",
                        " Archie",
                        " Miller",
                        "<|endoftext|>",
                        "LOS",
                        " ANGEL",
                        "ES"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        1.701271295547485,
                        0.4607930779457092,
                        0,
                        2.290737152099609,
                        0.7078448534011841,
                        2.099845886230469,
                        0,
                        0,
                        3.649605274200439,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 0,
                    "sentence_string": " which sleeps<|endoftext|> \"FOLLOW\" to follow a line that fluctuates with the song",
                    "max_token": " which",
                    "tokens": [
                        " which",
                        " sleeps",
                        "<|endoftext|>",
                        " \"",
                        "FO",
                        "LLOW",
                        "\"",
                        " to",
                        " follow",
                        " a",
                        " line",
                        " that",
                        " fluct",
                        "uates",
                        " with",
                        " the",
                        " song"
                    ],
                    "values": [
                        1.486194133758545,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " fellow Spartans, Brown reflected: \"This<|endoftext|>Julian Assange just t<|endoftext|>lo,",
                    "max_token": "<|endoftext|>",
                    "tokens": [
                        " fellow",
                        " Spartans",
                        ",",
                        " Brown",
                        " reflected",
                        ":",
                        " \"",
                        "This",
                        "<|endoftext|>",
                        "Jul",
                        "ian",
                        " Assange",
                        " just",
                        " t",
                        "<|endoftext|>",
                        "lo",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        1.18003261089325,
                        1.565228223800659,
                        2.049442768096924,
                        4.031628608703613,
                        0,
                        0,
                        0,
                        1.463807344436646,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "\u010a\u010aWe Grow Bitcoins: Scam or Legitimate Operation?\u010a<|endoftext|> and pure",
                    "max_token": " or",
                    "tokens": [
                        "\u010a",
                        "\u010a",
                        "We",
                        " Grow",
                        " Bitcoins",
                        ":",
                        " Sc",
                        "am",
                        " or",
                        " Leg",
                        "itimate",
                        " Operation",
                        "?",
                        "\u010a",
                        "<|endoftext|>",
                        " and",
                        " pure"
                    ],
                    "values": [
                        0,
                        0.08960700035095215,
                        0.6679369807243347,
                        0,
                        0,
                        1.788510680198669,
                        0,
                        0,
                        3.974211692810059,
                        0,
                        0,
                        0,
                        2.637899398803711,
                        0.7483019828796387,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " most rich countries. But \u00e2\u0122\u013epremiumisation\u00e2\u0122\u013f is working. Though still",
                    "tokens": [
                        " most",
                        " rich",
                        " countries",
                        ".",
                        " But",
                        " \u00e2\u0122",
                        "\u013e",
                        "prem",
                        "ium",
                        "isation",
                        "\u00e2\u0122",
                        "\u013f",
                        " is",
                        " working",
                        ".",
                        " Though",
                        " still"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " outfit in the top 20. The Denver Nuggets reportedly spend \u00c2\u00a32,939,",
                    "tokens": [
                        " outfit",
                        " in",
                        " the",
                        " top",
                        " 20",
                        ".",
                        " The",
                        " Denver",
                        " Nuggets",
                        " reportedly",
                        " spend",
                        " \u00c2\u00a3",
                        "2",
                        ",",
                        "9",
                        "39",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "if Al-Arab al-Qathafi, where three of Colonel Gaddafi's grandchildren",
                    "tokens": [
                        "if",
                        " Al",
                        "-",
                        "Arab",
                        " al",
                        "-",
                        "Q",
                        "ath",
                        "afi",
                        ",",
                        " where",
                        " three",
                        " of",
                        " Colonel",
                        " Gaddafi",
                        "'s",
                        " grandchildren"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "2 14 Lubbock +4 20 Denver +2 23 Portland +2 7 Birmingham",
                    "tokens": [
                        "2",
                        " 14",
                        " L",
                        "ubb",
                        "ock",
                        " +",
                        "4",
                        " 20",
                        " Denver",
                        " +",
                        "2",
                        " 23",
                        " Portland",
                        " +",
                        "2",
                        " 7",
                        " Birmingham"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " has to be levied, it should be levied on the Central and state governments and the",
                    "tokens": [
                        " has",
                        " to",
                        " be",
                        " levied",
                        ",",
                        " it",
                        " should",
                        " be",
                        " levied",
                        " on",
                        " the",
                        " Central",
                        " and",
                        " state",
                        " governments",
                        " and",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 4.317410469055176
        },
        {
            "feature_index": 1745,
            "gpt_predictions": [
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "references to the Billboard Hot charts",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": ", on the \u00e2\u0122\u013eK-pop Hot 100\u00e2\u0122\u013f Billboard music chart. What",
                    "max_token": " Hot",
                    "tokens": [
                        ",",
                        " on",
                        " the",
                        " \u00e2\u0122",
                        "\u013e",
                        "K",
                        "-",
                        "pop",
                        " Hot",
                        " 100",
                        "\u00e2\u0122",
                        "\u013f",
                        " Billboard",
                        " music",
                        " chart",
                        ".",
                        " What"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        32.40377807617188,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " release for convenience to refer to Hyatt Hotels Corporation and/or one or more",
                    "max_token": " Hot",
                    "tokens": [
                        " release",
                        " for",
                        " convenience",
                        " to",
                        " refer",
                        " to",
                        " Hy",
                        "att",
                        " Hot",
                        "els",
                        " Corporation",
                        " and",
                        "/",
                        "or",
                        " one",
                        " or",
                        " more"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        31.93683433532715,
                        1.098573684692383,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " popularity of last year's Need for Speed Hot Pursuit. And i think we also",
                    "max_token": " Hot",
                    "tokens": [
                        " popularity",
                        " of",
                        " last",
                        " year",
                        "'s",
                        " Need",
                        " for",
                        " Speed",
                        " Hot",
                        " Purs",
                        "uit",
                        ".",
                        " And",
                        " i",
                        " think",
                        " we",
                        " also"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        33.0703010559082,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "BUSINESS WIRE) -- Hyatt Hotels Corporation H, -1.04",
                    "max_token": " Hot",
                    "tokens": [
                        "BUS",
                        "INESS",
                        " WI",
                        "RE",
                        ")",
                        " --",
                        " Hy",
                        "att",
                        " Hot",
                        "els",
                        " Corporation",
                        " H",
                        ",",
                        " -",
                        "1",
                        ".",
                        "04"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        30.03378486633301,
                        0.2045342326164246,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " in the face with frustration'.\u010a\u010aHotel: Gandolfini had a heart",
                    "max_token": "Hot",
                    "tokens": [
                        " in",
                        " the",
                        " face",
                        " with",
                        " frustration",
                        "'.",
                        "\u010a",
                        "\u010a",
                        "Hot",
                        "el",
                        ":",
                        " Gand",
                        "olf",
                        "ini",
                        " had",
                        " a",
                        " heart"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        22.08640480041504,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " most rich countries. But \u00e2\u0122\u013epremiumisation\u00e2\u0122\u013f is working. Though still",
                    "tokens": [
                        " most",
                        " rich",
                        " countries",
                        ".",
                        " But",
                        " \u00e2\u0122",
                        "\u013e",
                        "prem",
                        "ium",
                        "isation",
                        "\u00e2\u0122",
                        "\u013f",
                        " is",
                        " working",
                        ".",
                        " Though",
                        " still"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " outfit in the top 20. The Denver Nuggets reportedly spend \u00c2\u00a32,939,",
                    "tokens": [
                        " outfit",
                        " in",
                        " the",
                        " top",
                        " 20",
                        ".",
                        " The",
                        " Denver",
                        " Nuggets",
                        " reportedly",
                        " spend",
                        " \u00c2\u00a3",
                        "2",
                        ",",
                        "9",
                        "39",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "if Al-Arab al-Qathafi, where three of Colonel Gaddafi's grandchildren",
                    "tokens": [
                        "if",
                        " Al",
                        "-",
                        "Arab",
                        " al",
                        "-",
                        "Q",
                        "ath",
                        "afi",
                        ",",
                        " where",
                        " three",
                        " of",
                        " Colonel",
                        " Gaddafi",
                        "'s",
                        " grandchildren"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "2 14 Lubbock +4 20 Denver +2 23 Portland +2 7 Birmingham",
                    "tokens": [
                        "2",
                        " 14",
                        " L",
                        "ubb",
                        "ock",
                        " +",
                        "4",
                        " 20",
                        " Denver",
                        " +",
                        "2",
                        " 23",
                        " Portland",
                        " +",
                        "2",
                        " 7",
                        " Birmingham"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " has to be levied, it should be levied on the Central and state governments and the",
                    "tokens": [
                        " has",
                        " to",
                        " be",
                        " levied",
                        ",",
                        " it",
                        " should",
                        " be",
                        " levied",
                        " on",
                        " the",
                        " Central",
                        " and",
                        " state",
                        " governments",
                        " and",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 37.56082534790039
        },
        {
            "feature_index": 1932,
            "gpt_predictions": [
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "adjectives related to negative emotions or situations",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " opportunities. And just when things started looking bleak and you thought it was going to be",
                    "max_token": " bleak",
                    "tokens": [
                        " opportunities",
                        ".",
                        " And",
                        " just",
                        " when",
                        " things",
                        " started",
                        " looking",
                        " bleak",
                        " and",
                        " you",
                        " thought",
                        " it",
                        " was",
                        " going",
                        " to",
                        " be"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        9.892497062683105,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " the horrific and unnecessary violence that's a grim hallmark of farm attacks ostensibly staged to steal",
                    "max_token": " grim",
                    "tokens": [
                        " the",
                        " horrific",
                        " and",
                        " unnecessary",
                        " violence",
                        " that",
                        "'s",
                        " a",
                        " grim",
                        " hallmark",
                        " of",
                        " farm",
                        " attacks",
                        " ostensibly",
                        " staged",
                        " to",
                        " steal"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        44.55981063842773,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": ". The attack was the latest in a grim list of mass shootings, and the carnage",
                    "max_token": " grim",
                    "tokens": [
                        ".",
                        " The",
                        " attack",
                        " was",
                        " the",
                        " latest",
                        " in",
                        " a",
                        " grim",
                        " list",
                        " of",
                        " mass",
                        " shootings",
                        ",",
                        " and",
                        " the",
                        " carnage"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        43.40988540649414,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "\u010a\u010aIf this all sounds a little grim, it is worth reiterating that The",
                    "max_token": " grim",
                    "tokens": [
                        "\u010a",
                        "\u010a",
                        "If",
                        " this",
                        " all",
                        " sounds",
                        " a",
                        " little",
                        " grim",
                        ",",
                        " it",
                        " is",
                        " worth",
                        " reiter",
                        "ating",
                        " that",
                        " The"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        42.36167526245117,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " society, [it] isn't as bleak as it is often presented in the media",
                    "max_token": " bleak",
                    "tokens": [
                        " society",
                        ",",
                        " [",
                        "it",
                        "]",
                        " isn",
                        "'t",
                        " as",
                        " bleak",
                        " as",
                        " it",
                        " is",
                        " often",
                        " presented",
                        " in",
                        " the",
                        " media"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        9.938068389892578,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " most rich countries. But \u00e2\u0122\u013epremiumisation\u00e2\u0122\u013f is working. Though still",
                    "tokens": [
                        " most",
                        " rich",
                        " countries",
                        ".",
                        " But",
                        " \u00e2\u0122",
                        "\u013e",
                        "prem",
                        "ium",
                        "isation",
                        "\u00e2\u0122",
                        "\u013f",
                        " is",
                        " working",
                        ".",
                        " Though",
                        " still"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " outfit in the top 20. The Denver Nuggets reportedly spend \u00c2\u00a32,939,",
                    "tokens": [
                        " outfit",
                        " in",
                        " the",
                        " top",
                        " 20",
                        ".",
                        " The",
                        " Denver",
                        " Nuggets",
                        " reportedly",
                        " spend",
                        " \u00c2\u00a3",
                        "2",
                        ",",
                        "9",
                        "39",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "if Al-Arab al-Qathafi, where three of Colonel Gaddafi's grandchildren",
                    "tokens": [
                        "if",
                        " Al",
                        "-",
                        "Arab",
                        " al",
                        "-",
                        "Q",
                        "ath",
                        "afi",
                        ",",
                        " where",
                        " three",
                        " of",
                        " Colonel",
                        " Gaddafi",
                        "'s",
                        " grandchildren"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "2 14 Lubbock +4 20 Denver +2 23 Portland +2 7 Birmingham",
                    "tokens": [
                        "2",
                        " 14",
                        " L",
                        "ubb",
                        "ock",
                        " +",
                        "4",
                        " 20",
                        " Denver",
                        " +",
                        "2",
                        " 23",
                        " Portland",
                        " +",
                        "2",
                        " 7",
                        " Birmingham"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " has to be levied, it should be levied on the Central and state governments and the",
                    "tokens": [
                        " has",
                        " to",
                        " be",
                        " levied",
                        ",",
                        " it",
                        " should",
                        " be",
                        " levied",
                        " on",
                        " the",
                        " Central",
                        " and",
                        " state",
                        " governments",
                        " and",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 45.15662002563477
        },
        {
            "feature_index": 18727,
            "gpt_predictions": [
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "instinct-related words or phrases",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " villain in a frock \u00e2\u0122\u00a6 whose first instinct when his priests are caught with their pants",
                    "max_token": " instinct",
                    "tokens": [
                        " villain",
                        " in",
                        " a",
                        " fro",
                        "ck",
                        " \u00e2\u0122\u00a6",
                        " whose",
                        " first",
                        " instinct",
                        " when",
                        " his",
                        " priests",
                        " are",
                        " caught",
                        " with",
                        " their",
                        " pants"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        42.47327423095703,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 7,
                    "sentence_string": "pico was a man with an instinctive understanding of the relationship between character and destiny",
                    "max_token": " instinct",
                    "tokens": [
                        "p",
                        "ico",
                        " was",
                        " a",
                        " man",
                        " with",
                        " an",
                        " instinct",
                        "ive",
                        " understanding",
                        " of",
                        " the",
                        " relationship",
                        " between",
                        " character",
                        " and",
                        " destiny"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        43.1567497253418,
                        5.907110691070557,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " is some of the collateral damage of an instinctive DB who is looking to make plays",
                    "max_token": " instinct",
                    "tokens": [
                        " is",
                        " some",
                        " of",
                        " the",
                        " collateral",
                        " damage",
                        " of",
                        " an",
                        " instinct",
                        "ive",
                        " DB",
                        " who",
                        " is",
                        " looking",
                        " to",
                        " make",
                        " plays"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        42.02533340454102,
                        5.431779861450195,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " like AMD\u00e2\u0122\u013bs new Radeon Instinct GPUs and FGPAs for that matter",
                    "max_token": "inct",
                    "tokens": [
                        " like",
                        " AMD",
                        "\u00e2\u0122",
                        "\u013b",
                        "s",
                        " new",
                        " Radeon",
                        " Inst",
                        "inct",
                        " GPUs",
                        " and",
                        " F",
                        "GP",
                        "As",
                        " for",
                        " that",
                        " matter"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        7.794342994689941,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " into an empty net and showing the predatory instincts the Revs hope to<|endoftext|> to the",
                    "max_token": " instincts",
                    "tokens": [
                        " into",
                        " an",
                        " empty",
                        " net",
                        " and",
                        " showing",
                        " the",
                        " predatory",
                        " instincts",
                        " the",
                        " Rev",
                        "s",
                        " hope",
                        " to",
                        "<|endoftext|>",
                        " to",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        24.81212425231934,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " most rich countries. But \u00e2\u0122\u013epremiumisation\u00e2\u0122\u013f is working. Though still",
                    "tokens": [
                        " most",
                        " rich",
                        " countries",
                        ".",
                        " But",
                        " \u00e2\u0122",
                        "\u013e",
                        "prem",
                        "ium",
                        "isation",
                        "\u00e2\u0122",
                        "\u013f",
                        " is",
                        " working",
                        ".",
                        " Though",
                        " still"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " outfit in the top 20. The Denver Nuggets reportedly spend \u00c2\u00a32,939,",
                    "tokens": [
                        " outfit",
                        " in",
                        " the",
                        " top",
                        " 20",
                        ".",
                        " The",
                        " Denver",
                        " Nuggets",
                        " reportedly",
                        " spend",
                        " \u00c2\u00a3",
                        "2",
                        ",",
                        "9",
                        "39",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "if Al-Arab al-Qathafi, where three of Colonel Gaddafi's grandchildren",
                    "tokens": [
                        "if",
                        " Al",
                        "-",
                        "Arab",
                        " al",
                        "-",
                        "Q",
                        "ath",
                        "afi",
                        ",",
                        " where",
                        " three",
                        " of",
                        " Colonel",
                        " Gaddafi",
                        "'s",
                        " grandchildren"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "2 14 Lubbock +4 20 Denver +2 23 Portland +2 7 Birmingham",
                    "tokens": [
                        "2",
                        " 14",
                        " L",
                        "ubb",
                        "ock",
                        " +",
                        "4",
                        " 20",
                        " Denver",
                        " +",
                        "2",
                        " 23",
                        " Portland",
                        " +",
                        "2",
                        " 7",
                        " Birmingham"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " has to be levied, it should be levied on the Central and state governments and the",
                    "tokens": [
                        " has",
                        " to",
                        " be",
                        " levied",
                        ",",
                        " it",
                        " should",
                        " be",
                        " levied",
                        " on",
                        " the",
                        " Central",
                        " and",
                        " state",
                        " governments",
                        " and",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 44.1662712097168
        },
        {
            "feature_index": 7066,
            "gpt_predictions": [
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "mentions of academic institutions and their respective schools",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " Giveaways for Public Elementary and Pre-School Students of DepEd Makati\" 42",
                    "max_token": "School",
                    "tokens": [
                        " Give",
                        "aways",
                        " for",
                        " Public",
                        " Elementary",
                        " and",
                        " Pre",
                        "-",
                        "School",
                        " Students",
                        " of",
                        " Dep",
                        "Ed",
                        " Mak",
                        "ati",
                        "\"",
                        " 42"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        13.9683141708374,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "\u0135 Corr\u00c3\u00a1in lectures in the School of History & Geography, Dublin City",
                    "max_token": " School",
                    "tokens": [
                        "\u0135",
                        " Cor",
                        "r",
                        "\u00c3\u00a1",
                        "in",
                        " lectures",
                        " in",
                        " the",
                        " School",
                        " of",
                        " History",
                        " &",
                        " Ge",
                        "ography",
                        ",",
                        " Dublin",
                        " City"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        25.76811027526855,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " was subpoenaed.\u010a\u010aDetroit Public Schools emergency manager Darnell Earley,",
                    "max_token": " Schools",
                    "tokens": [
                        " was",
                        " subpoen",
                        "aed",
                        ".",
                        "\u010a",
                        "\u010a",
                        "Detroit",
                        " Public",
                        " Schools",
                        " emergency",
                        " manager",
                        " D",
                        "arn",
                        "ell",
                        " Ear",
                        "ley",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        7.186639785766602,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " Professor Dr Simon Lamb, from Victoria's School of Geography, Environment and Earth Sciences",
                    "max_token": " School",
                    "tokens": [
                        " Professor",
                        " Dr",
                        " Simon",
                        " Lamb",
                        ",",
                        " from",
                        " Victoria",
                        "'s",
                        " School",
                        " of",
                        " Ge",
                        "ography",
                        ",",
                        " Environment",
                        " and",
                        " Earth",
                        " Sciences"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        29.15956687927246,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " now an Honorary Research Associate with the School of Mathematics and Physics \u00e2\u0122\u0135 with its 1",
                    "max_token": " School",
                    "tokens": [
                        " now",
                        " an",
                        " Honor",
                        "ary",
                        " Research",
                        " Associate",
                        " with",
                        " the",
                        " School",
                        " of",
                        " Mathematics",
                        " and",
                        " Physics",
                        " \u00e2\u0122\u0135",
                        " with",
                        " its",
                        " 1"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        28.87985420227051,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " most rich countries. But \u00e2\u0122\u013epremiumisation\u00e2\u0122\u013f is working. Though still",
                    "tokens": [
                        " most",
                        " rich",
                        " countries",
                        ".",
                        " But",
                        " \u00e2\u0122",
                        "\u013e",
                        "prem",
                        "ium",
                        "isation",
                        "\u00e2\u0122",
                        "\u013f",
                        " is",
                        " working",
                        ".",
                        " Though",
                        " still"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " outfit in the top 20. The Denver Nuggets reportedly spend \u00c2\u00a32,939,",
                    "tokens": [
                        " outfit",
                        " in",
                        " the",
                        " top",
                        " 20",
                        ".",
                        " The",
                        " Denver",
                        " Nuggets",
                        " reportedly",
                        " spend",
                        " \u00c2\u00a3",
                        "2",
                        ",",
                        "9",
                        "39",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "if Al-Arab al-Qathafi, where three of Colonel Gaddafi's grandchildren",
                    "tokens": [
                        "if",
                        " Al",
                        "-",
                        "Arab",
                        " al",
                        "-",
                        "Q",
                        "ath",
                        "afi",
                        ",",
                        " where",
                        " three",
                        " of",
                        " Colonel",
                        " Gaddafi",
                        "'s",
                        " grandchildren"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "2 14 Lubbock +4 20 Denver +2 23 Portland +2 7 Birmingham",
                    "tokens": [
                        "2",
                        " 14",
                        " L",
                        "ubb",
                        "ock",
                        " +",
                        "4",
                        " 20",
                        " Denver",
                        " +",
                        "2",
                        " 23",
                        " Portland",
                        " +",
                        "2",
                        " 7",
                        " Birmingham"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " has to be levied, it should be levied on the Central and state governments and the",
                    "tokens": [
                        " has",
                        " to",
                        " be",
                        " levied",
                        ",",
                        " it",
                        " should",
                        " be",
                        " levied",
                        " on",
                        " the",
                        " Central",
                        " and",
                        " state",
                        " governments",
                        " and",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 31.28962326049805
        },
        {
            "feature_index": 311,
            "gpt_predictions": [
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": " the syllable \"ras\" within words",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " around with Ed Helms and John Krasinski would certainly fit in, and oddly",
                    "max_token": "ras",
                    "tokens": [
                        " around",
                        " with",
                        " Ed",
                        " Hel",
                        "ms",
                        " and",
                        " John",
                        " K",
                        "ras",
                        "inski",
                        " would",
                        " certainly",
                        " fit",
                        " in",
                        ",",
                        " and",
                        " oddly"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        31.50728225708008,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": ". The groom picked up the \u00e2\u0122\u013arasogullah\u00e2\u0122\u013b. His act ir",
                    "max_token": "ras",
                    "tokens": [
                        ".",
                        " The",
                        " groom",
                        " picked",
                        " up",
                        " the",
                        " \u00e2\u0122",
                        "\u013a",
                        "ras",
                        "og",
                        "ullah",
                        "\u00e2\u0122",
                        "\u013b",
                        ".",
                        " His",
                        " act",
                        " ir"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        27.75759315490723,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " the Indian city of Chennai (formerly Madras) hear voices, they<|endoftext|>In our",
                    "max_token": "ras",
                    "tokens": [
                        " the",
                        " Indian",
                        " city",
                        " of",
                        " Chennai",
                        " (",
                        "formerly",
                        " Mad",
                        "ras",
                        ")",
                        " hear",
                        " voices",
                        ",",
                        " they",
                        "<|endoftext|>",
                        "In",
                        " our"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0.7251411080360413,
                        0,
                        0,
                        0,
                        29.5169620513916,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " as the \u00e2\u0122\u013enew Solyndras,\u00e2\u0122\u013f which are spending \u00e2\u0122\u013e",
                    "max_token": "ras",
                    "tokens": [
                        " as",
                        " the",
                        " \u00e2\u0122",
                        "\u013e",
                        "new",
                        " So",
                        "ly",
                        "nd",
                        "ras",
                        ",",
                        "\u00e2\u0122",
                        "\u013f",
                        " which",
                        " are",
                        " spending",
                        " \u00e2\u0122",
                        "\u013e"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        30.05456352233887,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "Four such Gujarat Gaurav Yatras will begin from Central Gujarat, Saurashtra",
                    "max_token": "ras",
                    "tokens": [
                        "Four",
                        " such",
                        " Gujarat",
                        " G",
                        "aur",
                        "av",
                        " Y",
                        "at",
                        "ras",
                        " will",
                        " begin",
                        " from",
                        " Central",
                        " Gujarat",
                        ",",
                        " Saur",
                        "ashtra"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        27.1442985534668,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " most rich countries. But \u00e2\u0122\u013epremiumisation\u00e2\u0122\u013f is working. Though still",
                    "tokens": [
                        " most",
                        " rich",
                        " countries",
                        ".",
                        " But",
                        " \u00e2\u0122",
                        "\u013e",
                        "prem",
                        "ium",
                        "isation",
                        "\u00e2\u0122",
                        "\u013f",
                        " is",
                        " working",
                        ".",
                        " Though",
                        " still"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " outfit in the top 20. The Denver Nuggets reportedly spend \u00c2\u00a32,939,",
                    "tokens": [
                        " outfit",
                        " in",
                        " the",
                        " top",
                        " 20",
                        ".",
                        " The",
                        " Denver",
                        " Nuggets",
                        " reportedly",
                        " spend",
                        " \u00c2\u00a3",
                        "2",
                        ",",
                        "9",
                        "39",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "if Al-Arab al-Qathafi, where three of Colonel Gaddafi's grandchildren",
                    "tokens": [
                        "if",
                        " Al",
                        "-",
                        "Arab",
                        " al",
                        "-",
                        "Q",
                        "ath",
                        "afi",
                        ",",
                        " where",
                        " three",
                        " of",
                        " Colonel",
                        " Gaddafi",
                        "'s",
                        " grandchildren"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "2 14 Lubbock +4 20 Denver +2 23 Portland +2 7 Birmingham",
                    "tokens": [
                        "2",
                        " 14",
                        " L",
                        "ubb",
                        "ock",
                        " +",
                        "4",
                        " 20",
                        " Denver",
                        " +",
                        "2",
                        " 23",
                        " Portland",
                        " +",
                        "2",
                        " 7",
                        " Birmingham"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " has to be levied, it should be levied on the Central and state governments and the",
                    "tokens": [
                        " has",
                        " to",
                        " be",
                        " levied",
                        ",",
                        " it",
                        " should",
                        " be",
                        " levied",
                        " on",
                        " the",
                        " Central",
                        " and",
                        " state",
                        " governments",
                        " and",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 34.27663040161133
        },
        {
            "feature_index": 23106,
            "gpt_predictions": [
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "mentions of the soccer team \"Manchester United.\"",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " be reached at [email protected]<|endoftext|>United Against a Nuclear Iran (UANI),",
                    "max_token": "United",
                    "tokens": [
                        " be",
                        " reached",
                        " at",
                        " [",
                        "email",
                        " protected",
                        "]",
                        "<|endoftext|>",
                        "United",
                        " Against",
                        " a",
                        " Nuclear",
                        " Iran",
                        " (",
                        "U",
                        "ANI",
                        "),"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        14.33730602264404,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " lifted the coveted Premier League trophy. Manchester United have won it on 13 occasions, all",
                    "max_token": " United",
                    "tokens": [
                        " lifted",
                        " the",
                        " coveted",
                        " Premier",
                        " League",
                        " trophy",
                        ".",
                        " Manchester",
                        " United",
                        " have",
                        " won",
                        " it",
                        " on",
                        " 13",
                        " occasions",
                        ",",
                        " all"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        32.90697860717773,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "Joshua Roberts\u010a\u010aTies between the United States and its NATO ally have been strained",
                    "max_token": " United",
                    "tokens": [
                        "Joshua",
                        " Roberts",
                        "\u010a",
                        "\u010a",
                        "T",
                        "ies",
                        " between",
                        " the",
                        " United",
                        " States",
                        " and",
                        " its",
                        " NATO",
                        " ally",
                        " have",
                        " been",
                        " strained"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        7.539487361907959,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " to play for a huge club like Manchester United. SportBild put it directly to",
                    "max_token": " United",
                    "tokens": [
                        " to",
                        " play",
                        " for",
                        " a",
                        " huge",
                        " club",
                        " like",
                        " Manchester",
                        " United",
                        ".",
                        " Sport",
                        "B",
                        "ild",
                        " put",
                        " it",
                        " directly",
                        " to"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        35.35597610473633,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " in the second half.\u010a\u010aManchester United Manchester United Crystal Palace Crystal Palace 4 0",
                    "max_token": " United",
                    "tokens": [
                        " in",
                        " the",
                        " second",
                        " half",
                        ".",
                        "\u010a",
                        "\u010a",
                        "Manchester",
                        " United",
                        " Manchester",
                        " United",
                        " Crystal",
                        " Palace",
                        " Crystal",
                        " Palace",
                        " 4",
                        " 0"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        34.83683776855469,
                        0,
                        34.02251434326172,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " most rich countries. But \u00e2\u0122\u013epremiumisation\u00e2\u0122\u013f is working. Though still",
                    "tokens": [
                        " most",
                        " rich",
                        " countries",
                        ".",
                        " But",
                        " \u00e2\u0122",
                        "\u013e",
                        "prem",
                        "ium",
                        "isation",
                        "\u00e2\u0122",
                        "\u013f",
                        " is",
                        " working",
                        ".",
                        " Though",
                        " still"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " outfit in the top 20. The Denver Nuggets reportedly spend \u00c2\u00a32,939,",
                    "tokens": [
                        " outfit",
                        " in",
                        " the",
                        " top",
                        " 20",
                        ".",
                        " The",
                        " Denver",
                        " Nuggets",
                        " reportedly",
                        " spend",
                        " \u00c2\u00a3",
                        "2",
                        ",",
                        "9",
                        "39",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "if Al-Arab al-Qathafi, where three of Colonel Gaddafi's grandchildren",
                    "tokens": [
                        "if",
                        " Al",
                        "-",
                        "Arab",
                        " al",
                        "-",
                        "Q",
                        "ath",
                        "afi",
                        ",",
                        " where",
                        " three",
                        " of",
                        " Colonel",
                        " Gaddafi",
                        "'s",
                        " grandchildren"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "2 14 Lubbock +4 20 Denver +2 23 Portland +2 7 Birmingham",
                    "tokens": [
                        "2",
                        " 14",
                        " L",
                        "ubb",
                        "ock",
                        " +",
                        "4",
                        " 20",
                        " Denver",
                        " +",
                        "2",
                        " 23",
                        " Portland",
                        " +",
                        "2",
                        " 7",
                        " Birmingham"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " has to be levied, it should be levied on the Central and state governments and the",
                    "tokens": [
                        " has",
                        " to",
                        " be",
                        " levied",
                        ",",
                        " it",
                        " should",
                        " be",
                        " levied",
                        " on",
                        " the",
                        " Central",
                        " and",
                        " state",
                        " governments",
                        " and",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 37.48389053344727
        },
        {
            "feature_index": 23252,
            "gpt_predictions": [
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "specific references to the term \"DevOps\" with slight variations in spelling",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " I threw more of this stuff into my DeviantArt gallery. It's good business",
                    "max_token": " Dev",
                    "tokens": [
                        " I",
                        " threw",
                        " more",
                        " of",
                        " this",
                        " stuff",
                        " into",
                        " my",
                        " Dev",
                        "iant",
                        "Art",
                        " gallery",
                        ".",
                        " It",
                        "'s",
                        " good",
                        " business"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        41.60447692871094,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " it is. Right after signing up to Dev Essentials I was able to activate the",
                    "max_token": " Dev",
                    "tokens": [
                        " it",
                        " is",
                        ".",
                        " Right",
                        " after",
                        " signing",
                        " up",
                        " to",
                        " Dev",
                        " Ess",
                        "entials",
                        " I",
                        " was",
                        " able",
                        " to",
                        " activate",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        41.21787643432617,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "ly Comtois is the VP of DevOps at Hearst Business Media. Before",
                    "max_token": " Dev",
                    "tokens": [
                        "ly",
                        " Com",
                        "to",
                        "is",
                        " is",
                        " the",
                        " VP",
                        " of",
                        " Dev",
                        "Ops",
                        " at",
                        " Hear",
                        "st",
                        " Business",
                        " Media",
                        ".",
                        " Before"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        41.87052154541016,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " Solid V: Big Quiet Boss XD [DeviantArt]\u010a\u010aYou are logged",
                    "max_token": "Dev",
                    "tokens": [
                        " Solid",
                        " V",
                        ":",
                        " Big",
                        " Quiet",
                        " Boss",
                        " XD",
                        " [",
                        "Dev",
                        "iant",
                        "Art",
                        "]",
                        "\u010a",
                        "\u010a",
                        "You",
                        " are",
                        " logged"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        22.66209983825684,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " who made his name as the psychotic Tommy DeVito in mafia movie Goodfellas -",
                    "max_token": " DeV",
                    "tokens": [
                        " who",
                        " made",
                        " his",
                        " name",
                        " as",
                        " the",
                        " psychotic",
                        " Tommy",
                        " DeV",
                        "ito",
                        " in",
                        " mafia",
                        " movie",
                        " Good",
                        "fell",
                        "as",
                        " -"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        7.40012788772583,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " most rich countries. But \u00e2\u0122\u013epremiumisation\u00e2\u0122\u013f is working. Though still",
                    "tokens": [
                        " most",
                        " rich",
                        " countries",
                        ".",
                        " But",
                        " \u00e2\u0122",
                        "\u013e",
                        "prem",
                        "ium",
                        "isation",
                        "\u00e2\u0122",
                        "\u013f",
                        " is",
                        " working",
                        ".",
                        " Though",
                        " still"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " outfit in the top 20. The Denver Nuggets reportedly spend \u00c2\u00a32,939,",
                    "tokens": [
                        " outfit",
                        " in",
                        " the",
                        " top",
                        " 20",
                        ".",
                        " The",
                        " Denver",
                        " Nuggets",
                        " reportedly",
                        " spend",
                        " \u00c2\u00a3",
                        "2",
                        ",",
                        "9",
                        "39",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "if Al-Arab al-Qathafi, where three of Colonel Gaddafi's grandchildren",
                    "tokens": [
                        "if",
                        " Al",
                        "-",
                        "Arab",
                        " al",
                        "-",
                        "Q",
                        "ath",
                        "afi",
                        ",",
                        " where",
                        " three",
                        " of",
                        " Colonel",
                        " Gaddafi",
                        "'s",
                        " grandchildren"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "2 14 Lubbock +4 20 Denver +2 23 Portland +2 7 Birmingham",
                    "tokens": [
                        "2",
                        " 14",
                        " L",
                        "ubb",
                        "ock",
                        " +",
                        "4",
                        " 20",
                        " Denver",
                        " +",
                        "2",
                        " 23",
                        " Portland",
                        " +",
                        "2",
                        " 7",
                        " Birmingham"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " has to be levied, it should be levied on the Central and state governments and the",
                    "tokens": [
                        " has",
                        " to",
                        " be",
                        " levied",
                        ",",
                        " it",
                        " should",
                        " be",
                        " levied",
                        " on",
                        " the",
                        " Central",
                        " and",
                        " state",
                        " governments",
                        " and",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 43.89654541015625
        },
        {
            "feature_index": 13304,
            "gpt_predictions": [
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "references to or instructions related to baking dough",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " during that process, the fermentation of the dough further reduces the sugar in it.\u00e2\u0122",
                    "max_token": " dough",
                    "tokens": [
                        " during",
                        " that",
                        " process",
                        ",",
                        " the",
                        " fermentation",
                        " of",
                        " the",
                        " dough",
                        " further",
                        " reduces",
                        " the",
                        " sugar",
                        " in",
                        " it",
                        ".",
                        "\u00e2\u0122"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        41.5137939453125,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 9,
                    "sentence_string": ".\u010a\u010a2. Roll out your pastry dough and cut into two equal circles,",
                    "max_token": " dough",
                    "tokens": [
                        ".",
                        "\u010a",
                        "\u010a",
                        "2",
                        ".",
                        " Roll",
                        " out",
                        " your",
                        " pastry",
                        " dough",
                        " and",
                        " cut",
                        " into",
                        " two",
                        " equal",
                        " circles",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        8.014373779296875,
                        41.8764762878418,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " have little black dots mixed in with the dough, which had me worried until it became",
                    "max_token": " dough",
                    "tokens": [
                        " have",
                        " little",
                        " black",
                        " dots",
                        " mixed",
                        " in",
                        " with",
                        " the",
                        " dough",
                        ",",
                        " which",
                        " had",
                        " me",
                        " worried",
                        " until",
                        " it",
                        " became"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        46.44792938232422,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "\u010a\u010a2. Roll out your pastry dough and cut into two equal circles, enough",
                    "max_token": " dough",
                    "tokens": [
                        "\u010a",
                        "\u010a",
                        "2",
                        ".",
                        " Roll",
                        " out",
                        " your",
                        " pastry",
                        " dough",
                        " and",
                        " cut",
                        " into",
                        " two",
                        " equal",
                        " circles",
                        ",",
                        " enough"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        8.014373779296875,
                        41.8764762878418,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "\u013bt put any extra sugar in the dough. And we let our dough age for",
                    "max_token": " dough",
                    "tokens": [
                        "\u013b",
                        "t",
                        " put",
                        " any",
                        " extra",
                        " sugar",
                        " in",
                        " the",
                        " dough",
                        ".",
                        " And",
                        " we",
                        " let",
                        " our",
                        " dough",
                        " age",
                        " for"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        45.6716423034668,
                        0,
                        0,
                        0,
                        0,
                        0,
                        42.65821075439453,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " most rich countries. But \u00e2\u0122\u013epremiumisation\u00e2\u0122\u013f is working. Though still",
                    "tokens": [
                        " most",
                        " rich",
                        " countries",
                        ".",
                        " But",
                        " \u00e2\u0122",
                        "\u013e",
                        "prem",
                        "ium",
                        "isation",
                        "\u00e2\u0122",
                        "\u013f",
                        " is",
                        " working",
                        ".",
                        " Though",
                        " still"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " outfit in the top 20. The Denver Nuggets reportedly spend \u00c2\u00a32,939,",
                    "tokens": [
                        " outfit",
                        " in",
                        " the",
                        " top",
                        " 20",
                        ".",
                        " The",
                        " Denver",
                        " Nuggets",
                        " reportedly",
                        " spend",
                        " \u00c2\u00a3",
                        "2",
                        ",",
                        "9",
                        "39",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "if Al-Arab al-Qathafi, where three of Colonel Gaddafi's grandchildren",
                    "tokens": [
                        "if",
                        " Al",
                        "-",
                        "Arab",
                        " al",
                        "-",
                        "Q",
                        "ath",
                        "afi",
                        ",",
                        " where",
                        " three",
                        " of",
                        " Colonel",
                        " Gaddafi",
                        "'s",
                        " grandchildren"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "2 14 Lubbock +4 20 Denver +2 23 Portland +2 7 Birmingham",
                    "tokens": [
                        "2",
                        " 14",
                        " L",
                        "ubb",
                        "ock",
                        " +",
                        "4",
                        " 20",
                        " Denver",
                        " +",
                        "2",
                        " 23",
                        " Portland",
                        " +",
                        "2",
                        " 7",
                        " Birmingham"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " has to be levied, it should be levied on the Central and state governments and the",
                    "tokens": [
                        " has",
                        " to",
                        " be",
                        " levied",
                        ",",
                        " it",
                        " should",
                        " be",
                        " levied",
                        " on",
                        " the",
                        " Central",
                        " and",
                        " state",
                        " governments",
                        " and",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 47.20321273803711
        },
        {
            "feature_index": 18445,
            "gpt_predictions": [
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    1.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "dates written in the format \"Month Day\"",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " house.\u010a\u010aKelly dismissed the charges Feb. 19, stating the Shattucks",
                    "max_token": " Feb",
                    "tokens": [
                        " house",
                        ".",
                        "\u010a",
                        "\u010a",
                        "Kelly",
                        " dismissed",
                        " the",
                        " charges",
                        " Feb",
                        ".",
                        " 19",
                        ",",
                        " stating",
                        " the",
                        " Sh",
                        "att",
                        "ucks"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        41.18621063232422,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "Ben Kaufman Blocked Unblock Follow Following Apr 13, 2015\u010a\u010aQuirky",
                    "max_token": " Apr",
                    "tokens": [
                        "Ben",
                        " Kaufman",
                        " Bl",
                        "ocked",
                        " Un",
                        "block",
                        " Follow",
                        " Following",
                        " Apr",
                        " 13",
                        ",",
                        " 2015",
                        "\u010a",
                        "\u010a",
                        "Qu",
                        "ir",
                        "ky"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        7.843617916107178,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " open for the final two weeks before the Feb. 1 caucuses.\u010a\u010aIn recent",
                    "max_token": " Feb",
                    "tokens": [
                        " open",
                        " for",
                        " the",
                        " final",
                        " two",
                        " weeks",
                        " before",
                        " the",
                        " Feb",
                        ".",
                        " 1",
                        " caucuses",
                        ".",
                        "\u010a",
                        "\u010a",
                        "In",
                        " recent"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        41.39419937133789,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "ek (@ekotjoek) on Apr 7, 2016 at 4:32pm",
                    "max_token": " Apr",
                    "tokens": [
                        "ek",
                        " (@",
                        "ek",
                        "ot",
                        "jo",
                        "ek",
                        ")",
                        " on",
                        " Apr",
                        " 7",
                        ",",
                        " 2016",
                        " at",
                        " 4",
                        ":",
                        "32",
                        "pm"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        8.554673194885254,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "\u010a29 Willz_coar on Apr 5, 2011\u010a\u010a30 Nick Ag",
                    "max_token": " Apr",
                    "tokens": [
                        "\u010a",
                        "29",
                        " Will",
                        "z",
                        "_",
                        "co",
                        "ar",
                        " on",
                        " Apr",
                        " 5",
                        ",",
                        " 2011",
                        "\u010a",
                        "\u010a",
                        "30",
                        " Nick",
                        " Ag"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        6.211735725402832,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " most rich countries. But \u00e2\u0122\u013epremiumisation\u00e2\u0122\u013f is working. Though still",
                    "tokens": [
                        " most",
                        " rich",
                        " countries",
                        ".",
                        " But",
                        " \u00e2\u0122",
                        "\u013e",
                        "prem",
                        "ium",
                        "isation",
                        "\u00e2\u0122",
                        "\u013f",
                        " is",
                        " working",
                        ".",
                        " Though",
                        " still"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " outfit in the top 20. The Denver Nuggets reportedly spend \u00c2\u00a32,939,",
                    "tokens": [
                        " outfit",
                        " in",
                        " the",
                        " top",
                        " 20",
                        ".",
                        " The",
                        " Denver",
                        " Nuggets",
                        " reportedly",
                        " spend",
                        " \u00c2\u00a3",
                        "2",
                        ",",
                        "9",
                        "39",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "if Al-Arab al-Qathafi, where three of Colonel Gaddafi's grandchildren",
                    "tokens": [
                        "if",
                        " Al",
                        "-",
                        "Arab",
                        " al",
                        "-",
                        "Q",
                        "ath",
                        "afi",
                        ",",
                        " where",
                        " three",
                        " of",
                        " Colonel",
                        " Gaddafi",
                        "'s",
                        " grandchildren"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "2 14 Lubbock +4 20 Denver +2 23 Portland +2 7 Birmingham",
                    "tokens": [
                        "2",
                        " 14",
                        " L",
                        "ubb",
                        "ock",
                        " +",
                        "4",
                        " 20",
                        " Denver",
                        " +",
                        "2",
                        " 23",
                        " Portland",
                        " +",
                        "2",
                        " 7",
                        " Birmingham"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " has to be levied, it should be levied on the Central and state governments and the",
                    "tokens": [
                        " has",
                        " to",
                        " be",
                        " levied",
                        ",",
                        " it",
                        " should",
                        " be",
                        " levied",
                        " on",
                        " the",
                        " Central",
                        " and",
                        " state",
                        " governments",
                        " and",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 42.69929885864258
        },
        {
            "feature_index": 11519,
            "gpt_predictions": [
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    1.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "references to something being claimed, assumed, or rumored to be true",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " one Dawud Salahuddin, who supposedly had information on Rafsanjani\u00e2\u0122",
                    "max_token": " supposedly",
                    "tokens": [
                        " one",
                        " Daw",
                        "ud",
                        " Sal",
                        "ah",
                        "uddin",
                        ",",
                        " who",
                        " supposedly",
                        " had",
                        " information",
                        " on",
                        " Raf",
                        "san",
                        "j",
                        "ani",
                        "\u00e2\u0122"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        25.85694885253906,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " known to wear purple on certain days, supposedly to ward off evil.\u010a\u010aRoman",
                    "max_token": " supposedly",
                    "tokens": [
                        " known",
                        " to",
                        " wear",
                        " purple",
                        " on",
                        " certain",
                        " days",
                        ",",
                        " supposedly",
                        " to",
                        " ward",
                        " off",
                        " evil",
                        ".",
                        "\u010a",
                        "\u010a",
                        "Roman"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        27.28543472290039,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " virtually nonstop since taking office about the supposedly unfair coverage surrounding the White House, casting",
                    "max_token": " supposedly",
                    "tokens": [
                        " virtually",
                        " non",
                        "stop",
                        " since",
                        " taking",
                        " office",
                        " about",
                        " the",
                        " supposedly",
                        " unfair",
                        " coverage",
                        " surrounding",
                        " the",
                        " White",
                        " House",
                        ",",
                        " casting"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        28.94645881652832,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "s former counsel Dean Strang challenged the supposedly damning evidence used to convict Avery in 2007",
                    "max_token": " supposedly",
                    "tokens": [
                        "s",
                        " former",
                        " counsel",
                        " Dean",
                        " Str",
                        "ang",
                        " challenged",
                        " the",
                        " supposedly",
                        " damning",
                        " evidence",
                        " used",
                        " to",
                        " convict",
                        " Avery",
                        " in",
                        " 2007"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        29.70166015625,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "ama Project I was criticized for writing a supposedly \u00e2\u0122\u013ewooden critique\u00e2\u0122\u013f of",
                    "max_token": " supposedly",
                    "tokens": [
                        "ama",
                        " Project",
                        " I",
                        " was",
                        " criticized",
                        " for",
                        " writing",
                        " a",
                        " supposedly",
                        " \u00e2\u0122",
                        "\u013e",
                        "wood",
                        "en",
                        " critique",
                        "\u00e2\u0122",
                        "\u013f",
                        " of"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        30.29641151428223,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " most rich countries. But \u00e2\u0122\u013epremiumisation\u00e2\u0122\u013f is working. Though still",
                    "tokens": [
                        " most",
                        " rich",
                        " countries",
                        ".",
                        " But",
                        " \u00e2\u0122",
                        "\u013e",
                        "prem",
                        "ium",
                        "isation",
                        "\u00e2\u0122",
                        "\u013f",
                        " is",
                        " working",
                        ".",
                        " Though",
                        " still"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " outfit in the top 20. The Denver Nuggets reportedly spend \u00c2\u00a32,939,",
                    "tokens": [
                        " outfit",
                        " in",
                        " the",
                        " top",
                        " 20",
                        ".",
                        " The",
                        " Denver",
                        " Nuggets",
                        " reportedly",
                        " spend",
                        " \u00c2\u00a3",
                        "2",
                        ",",
                        "9",
                        "39",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "if Al-Arab al-Qathafi, where three of Colonel Gaddafi's grandchildren",
                    "tokens": [
                        "if",
                        " Al",
                        "-",
                        "Arab",
                        " al",
                        "-",
                        "Q",
                        "ath",
                        "afi",
                        ",",
                        " where",
                        " three",
                        " of",
                        " Colonel",
                        " Gaddafi",
                        "'s",
                        " grandchildren"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "2 14 Lubbock +4 20 Denver +2 23 Portland +2 7 Birmingham",
                    "tokens": [
                        "2",
                        " 14",
                        " L",
                        "ubb",
                        "ock",
                        " +",
                        "4",
                        " 20",
                        " Denver",
                        " +",
                        "2",
                        " 23",
                        " Portland",
                        " +",
                        "2",
                        " 7",
                        " Birmingham"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " has to be levied, it should be levied on the Central and state governments and the",
                    "tokens": [
                        " has",
                        " to",
                        " be",
                        " levied",
                        ",",
                        " it",
                        " should",
                        " be",
                        " levied",
                        " on",
                        " the",
                        " Central",
                        " and",
                        " state",
                        " governments",
                        " and",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 30.8275146484375
        },
        {
            "feature_index": 11518,
            "gpt_predictions": [
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "phrases related to ink, either metaphorical or literal",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "-faced, milkshake-drinking ginger who has epitomized American wholes",
                    "max_token": "inking",
                    "tokens": [
                        "-",
                        "faced",
                        ",",
                        " mil",
                        "ksh",
                        "ake",
                        "-",
                        "dr",
                        "inking",
                        " ginger",
                        " who",
                        " has",
                        " epit",
                        "om",
                        "ized",
                        " American",
                        " wholes"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        12.44715213775635,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 1,
                    "sentence_string": " heatsinks are particularly noticeable. Stock heatsinks are much more simplistic than the aftermarket",
                    "max_token": "inks",
                    "tokens": [
                        " heats",
                        "inks",
                        " are",
                        " particularly",
                        " noticeable",
                        ".",
                        " Stock",
                        " heats",
                        "inks",
                        " are",
                        " much",
                        " more",
                        " simplistic",
                        " than",
                        " the",
                        " after",
                        "market"
                    ],
                    "values": [
                        0,
                        16.17095375061035,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        14.54419898986816,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " league said, \u00e2\u0122\u013ewhich effectively shrinks the field, giving the umpire less",
                    "max_token": "inks",
                    "tokens": [
                        " league",
                        " said",
                        ",",
                        " \u00e2\u0122",
                        "\u013e",
                        "which",
                        " effectively",
                        " shr",
                        "inks",
                        " the",
                        " field",
                        ",",
                        " giving",
                        " the",
                        " u",
                        "mpire",
                        " less"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        14.81741619110107,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " posing with<|endoftext|>, also posted weblinks and video stills from that footage on",
                    "max_token": "inks",
                    "tokens": [
                        " posing",
                        " with",
                        "<|endoftext|>",
                        ",",
                        " also",
                        " posted",
                        " we",
                        "bl",
                        "inks",
                        " and",
                        " video",
                        " still",
                        "s",
                        " from",
                        " that",
                        " footage",
                        " on"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        21.10370254516602,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " When commitment wanes, or enthusiasm shrinks, however, it can be easy to",
                    "max_token": "inks",
                    "tokens": [
                        " When",
                        " commitment",
                        " w",
                        "anes",
                        ",",
                        " or",
                        " enthusiasm",
                        " shr",
                        "inks",
                        ",",
                        " however",
                        ",",
                        " it",
                        " can",
                        " be",
                        " easy",
                        " to"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        16.40424156188965,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " most rich countries. But \u00e2\u0122\u013epremiumisation\u00e2\u0122\u013f is working. Though still",
                    "tokens": [
                        " most",
                        " rich",
                        " countries",
                        ".",
                        " But",
                        " \u00e2\u0122",
                        "\u013e",
                        "prem",
                        "ium",
                        "isation",
                        "\u00e2\u0122",
                        "\u013f",
                        " is",
                        " working",
                        ".",
                        " Though",
                        " still"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " outfit in the top 20. The Denver Nuggets reportedly spend \u00c2\u00a32,939,",
                    "tokens": [
                        " outfit",
                        " in",
                        " the",
                        " top",
                        " 20",
                        ".",
                        " The",
                        " Denver",
                        " Nuggets",
                        " reportedly",
                        " spend",
                        " \u00c2\u00a3",
                        "2",
                        ",",
                        "9",
                        "39",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "if Al-Arab al-Qathafi, where three of Colonel Gaddafi's grandchildren",
                    "tokens": [
                        "if",
                        " Al",
                        "-",
                        "Arab",
                        " al",
                        "-",
                        "Q",
                        "ath",
                        "afi",
                        ",",
                        " where",
                        " three",
                        " of",
                        " Colonel",
                        " Gaddafi",
                        "'s",
                        " grandchildren"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "2 14 Lubbock +4 20 Denver +2 23 Portland +2 7 Birmingham",
                    "tokens": [
                        "2",
                        " 14",
                        " L",
                        "ubb",
                        "ock",
                        " +",
                        "4",
                        " 20",
                        " Denver",
                        " +",
                        "2",
                        " 23",
                        " Portland",
                        " +",
                        "2",
                        " 7",
                        " Birmingham"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " has to be levied, it should be levied on the Central and state governments and the",
                    "tokens": [
                        " has",
                        " to",
                        " be",
                        " levied",
                        ",",
                        " it",
                        " should",
                        " be",
                        " levied",
                        " on",
                        " the",
                        " Central",
                        " and",
                        " state",
                        " governments",
                        " and",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 26.40829467773438
        },
        {
            "feature_index": 8451,
            "gpt_predictions": [
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "mentions of high-tech related terms and concepts",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " government of the United States running high-tech death squads and blanketing the globe with",
                    "max_token": "tech",
                    "tokens": [
                        " government",
                        " of",
                        " the",
                        " United",
                        " States",
                        " running",
                        " high",
                        "-",
                        "tech",
                        " death",
                        " squads",
                        " and",
                        " blank",
                        "eting",
                        " the",
                        " globe",
                        " with"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        28.41677474975586,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " directly create approximately 3,000 high-tech, high-wage Intel jobs for process",
                    "max_token": "tech",
                    "tokens": [
                        " directly",
                        " create",
                        " approximately",
                        " 3",
                        ",",
                        "000",
                        " high",
                        "-",
                        "tech",
                        ",",
                        " high",
                        "-",
                        "wage",
                        " Intel",
                        " jobs",
                        " for",
                        " process"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        29.26097869873047,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "re looking for. According to Anandtech, the Android version of the media player",
                    "max_token": "tech",
                    "tokens": [
                        "re",
                        " looking",
                        " for",
                        ".",
                        " According",
                        " to",
                        " An",
                        "and",
                        "tech",
                        ",",
                        " the",
                        " Android",
                        " version",
                        " of",
                        " the",
                        " media",
                        " player"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        28.37911605834961,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": ", technology, engineering, and mathematics (STEM) fields. The false claims are examples",
                    "max_token": "STEM",
                    "tokens": [
                        ",",
                        " technology",
                        ",",
                        " engineering",
                        ",",
                        " and",
                        " mathematics",
                        " (",
                        "STEM",
                        ")",
                        " fields",
                        ".",
                        " The",
                        " false",
                        " claims",
                        " are",
                        " examples"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        5.054638385772705,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " Photo/NASA/JPL-Caltech\u010a\u010aNewly released diplomatic cables from",
                    "max_token": "tech",
                    "tokens": [
                        " Photo",
                        "/",
                        "NASA",
                        "/",
                        "J",
                        "PL",
                        "-",
                        "Cal",
                        "tech",
                        "\u010a",
                        "\u010a",
                        "New",
                        "ly",
                        " released",
                        " diplomatic",
                        " cables",
                        " from"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        20.72530937194824,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " most rich countries. But \u00e2\u0122\u013epremiumisation\u00e2\u0122\u013f is working. Though still",
                    "tokens": [
                        " most",
                        " rich",
                        " countries",
                        ".",
                        " But",
                        " \u00e2\u0122",
                        "\u013e",
                        "prem",
                        "ium",
                        "isation",
                        "\u00e2\u0122",
                        "\u013f",
                        " is",
                        " working",
                        ".",
                        " Though",
                        " still"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " outfit in the top 20. The Denver Nuggets reportedly spend \u00c2\u00a32,939,",
                    "tokens": [
                        " outfit",
                        " in",
                        " the",
                        " top",
                        " 20",
                        ".",
                        " The",
                        " Denver",
                        " Nuggets",
                        " reportedly",
                        " spend",
                        " \u00c2\u00a3",
                        "2",
                        ",",
                        "9",
                        "39",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "if Al-Arab al-Qathafi, where three of Colonel Gaddafi's grandchildren",
                    "tokens": [
                        "if",
                        " Al",
                        "-",
                        "Arab",
                        " al",
                        "-",
                        "Q",
                        "ath",
                        "afi",
                        ",",
                        " where",
                        " three",
                        " of",
                        " Colonel",
                        " Gaddafi",
                        "'s",
                        " grandchildren"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "2 14 Lubbock +4 20 Denver +2 23 Portland +2 7 Birmingham",
                    "tokens": [
                        "2",
                        " 14",
                        " L",
                        "ubb",
                        "ock",
                        " +",
                        "4",
                        " 20",
                        " Denver",
                        " +",
                        "2",
                        " 23",
                        " Portland",
                        " +",
                        "2",
                        " 7",
                        " Birmingham"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " has to be levied, it should be levied on the Central and state governments and the",
                    "tokens": [
                        " has",
                        " to",
                        " be",
                        " levied",
                        ",",
                        " it",
                        " should",
                        " be",
                        " levied",
                        " on",
                        " the",
                        " Central",
                        " and",
                        " state",
                        " governments",
                        " and",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 30.56815338134766
        },
        {
            "feature_index": 13638,
            "gpt_predictions": [
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    1.0
                ]
            ],
            "description": "edits or revisions mentioned in a text",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " won't be sold for slaughter.<|endoftext|>edit on 3/10/2013 by Kl",
                    "max_token": "edit",
                    "tokens": [
                        " won",
                        "'t",
                        " be",
                        " sold",
                        " for",
                        " slaughter",
                        ".",
                        "<|endoftext|>",
                        "edit",
                        " on",
                        " 3",
                        "/",
                        "10",
                        "/",
                        "2013",
                        " by",
                        " Kl"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        7.772970199584961,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " has yet to consider.\u010a\u010aEND EDIT\u010a\u010aHello, I am a shut",
                    "max_token": " EDIT",
                    "tokens": [
                        " has",
                        " yet",
                        " to",
                        " consider",
                        ".",
                        "\u010a",
                        "\u010a",
                        "END",
                        " EDIT",
                        "\u010a",
                        "\u010a",
                        "Hello",
                        ",",
                        " I",
                        " am",
                        " a",
                        " shut"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        12.15835952758789,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " statements this rebrander makes. [Edit: Although, if they also remove your",
                    "max_token": "Edit",
                    "tokens": [
                        " statements",
                        " this",
                        " re",
                        "br",
                        "ander",
                        " makes",
                        ".",
                        " [",
                        "Edit",
                        ":",
                        " Although",
                        ",",
                        " if",
                        " they",
                        " also",
                        " remove",
                        " your"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        15.95956134796143,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "Hauler 5\u010a\u010aRecycler 10\u010a\u010aMinesweeper 2",
                    "max_token": "cler",
                    "tokens": [
                        "H",
                        "aul",
                        "er",
                        " 5",
                        "\u010a",
                        "\u010a",
                        "Rec",
                        "y",
                        "cler",
                        " 10",
                        "\u010a",
                        "\u010a",
                        "M",
                        "ines",
                        "we",
                        "eper",
                        " 2"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        5.039724349975586,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " do think that the conversation has been oversimplified,\u00e2\u0122\u013f Obama said. \u00e2\u0122",
                    "max_token": "impl",
                    "tokens": [
                        " do",
                        " think",
                        " that",
                        " the",
                        " conversation",
                        " has",
                        " been",
                        " overs",
                        "impl",
                        "ified",
                        ",",
                        "\u00e2\u0122",
                        "\u013f",
                        " Obama",
                        " said",
                        ".",
                        " \u00e2\u0122"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        2.691575050354004,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " most rich countries. But \u00e2\u0122\u013epremiumisation\u00e2\u0122\u013f is working. Though still",
                    "tokens": [
                        " most",
                        " rich",
                        " countries",
                        ".",
                        " But",
                        " \u00e2\u0122",
                        "\u013e",
                        "prem",
                        "ium",
                        "isation",
                        "\u00e2\u0122",
                        "\u013f",
                        " is",
                        " working",
                        ".",
                        " Though",
                        " still"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " outfit in the top 20. The Denver Nuggets reportedly spend \u00c2\u00a32,939,",
                    "tokens": [
                        " outfit",
                        " in",
                        " the",
                        " top",
                        " 20",
                        ".",
                        " The",
                        " Denver",
                        " Nuggets",
                        " reportedly",
                        " spend",
                        " \u00c2\u00a3",
                        "2",
                        ",",
                        "9",
                        "39",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "if Al-Arab al-Qathafi, where three of Colonel Gaddafi's grandchildren",
                    "tokens": [
                        "if",
                        " Al",
                        "-",
                        "Arab",
                        " al",
                        "-",
                        "Q",
                        "ath",
                        "afi",
                        ",",
                        " where",
                        " three",
                        " of",
                        " Colonel",
                        " Gaddafi",
                        "'s",
                        " grandchildren"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "2 14 Lubbock +4 20 Denver +2 23 Portland +2 7 Birmingham",
                    "tokens": [
                        "2",
                        " 14",
                        " L",
                        "ubb",
                        "ock",
                        " +",
                        "4",
                        " 20",
                        " Denver",
                        " +",
                        "2",
                        " 23",
                        " Portland",
                        " +",
                        "2",
                        " 7",
                        " Birmingham"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " has to be levied, it should be levied on the Central and state governments and the",
                    "tokens": [
                        " has",
                        " to",
                        " be",
                        " levied",
                        ",",
                        " it",
                        " should",
                        " be",
                        " levied",
                        " on",
                        " the",
                        " Central",
                        " and",
                        " state",
                        " governments",
                        " and",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 21.19713592529297
        },
        {
            "feature_index": 1658,
            "gpt_predictions": [
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "verbs related to creating or defining something",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " human beings because of the human appendages carved into the stone. These images are accompanied",
                    "max_token": " carved",
                    "tokens": [
                        " human",
                        " beings",
                        " because",
                        " of",
                        " the",
                        " human",
                        " append",
                        "ages",
                        " carved",
                        " into",
                        " the",
                        " stone",
                        ".",
                        " These",
                        " images",
                        " are",
                        " accompanied"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        34.08340072631836,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " latter has pioneered a unique business model, carving out its own market niche (or nic",
                    "max_token": " carving",
                    "tokens": [
                        " latter",
                        " has",
                        " pioneered",
                        " a",
                        " unique",
                        " business",
                        " model",
                        ",",
                        " carving",
                        " out",
                        " its",
                        " own",
                        " market",
                        " niche",
                        " (",
                        "or",
                        " nic"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        35.04575347900391,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "<|endoftext|> in 1983 by Portland. He then carved out a Hall of Fame career with the",
                    "max_token": " carved",
                    "tokens": [
                        "<|endoftext|>",
                        " in",
                        " 1983",
                        " by",
                        " Portland",
                        ".",
                        " He",
                        " then",
                        " carved",
                        " out",
                        " a",
                        " Hall",
                        " of",
                        " Fame",
                        " career",
                        " with",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        35.76765060424805,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": ", and serial number of the pistol, etched in 2 or more places on the interior",
                    "max_token": " etched",
                    "tokens": [
                        ",",
                        " and",
                        " serial",
                        " number",
                        " of",
                        " the",
                        " pistol",
                        ",",
                        " etched",
                        " in",
                        " 2",
                        " or",
                        " more",
                        " places",
                        " on",
                        " the",
                        " interior"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        10.14519596099854,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " two valleys and touches several rock painting and carving sites in the pristine breathtaking environment.\u010a",
                    "max_token": " carving",
                    "tokens": [
                        " two",
                        " valleys",
                        " and",
                        " touches",
                        " several",
                        " rock",
                        " painting",
                        " and",
                        " carving",
                        " sites",
                        " in",
                        " the",
                        " pristine",
                        " breathtaking",
                        " environment",
                        ".",
                        "\u010a"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        30.90611267089844,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " most rich countries. But \u00e2\u0122\u013epremiumisation\u00e2\u0122\u013f is working. Though still",
                    "tokens": [
                        " most",
                        " rich",
                        " countries",
                        ".",
                        " But",
                        " \u00e2\u0122",
                        "\u013e",
                        "prem",
                        "ium",
                        "isation",
                        "\u00e2\u0122",
                        "\u013f",
                        " is",
                        " working",
                        ".",
                        " Though",
                        " still"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " outfit in the top 20. The Denver Nuggets reportedly spend \u00c2\u00a32,939,",
                    "tokens": [
                        " outfit",
                        " in",
                        " the",
                        " top",
                        " 20",
                        ".",
                        " The",
                        " Denver",
                        " Nuggets",
                        " reportedly",
                        " spend",
                        " \u00c2\u00a3",
                        "2",
                        ",",
                        "9",
                        "39",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "if Al-Arab al-Qathafi, where three of Colonel Gaddafi's grandchildren",
                    "tokens": [
                        "if",
                        " Al",
                        "-",
                        "Arab",
                        " al",
                        "-",
                        "Q",
                        "ath",
                        "afi",
                        ",",
                        " where",
                        " three",
                        " of",
                        " Colonel",
                        " Gaddafi",
                        "'s",
                        " grandchildren"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "2 14 Lubbock +4 20 Denver +2 23 Portland +2 7 Birmingham",
                    "tokens": [
                        "2",
                        " 14",
                        " L",
                        "ubb",
                        "ock",
                        " +",
                        "4",
                        " 20",
                        " Denver",
                        " +",
                        "2",
                        " 23",
                        " Portland",
                        " +",
                        "2",
                        " 7",
                        " Birmingham"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " has to be levied, it should be levied on the Central and state governments and the",
                    "tokens": [
                        " has",
                        " to",
                        " be",
                        " levied",
                        ",",
                        " it",
                        " should",
                        " be",
                        " levied",
                        " on",
                        " the",
                        " Central",
                        " and",
                        " state",
                        " governments",
                        " and",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 38.85968399047852
        },
        {
            "feature_index": 14190,
            "gpt_predictions": [
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "instances of the word \"lull\" or related variations",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "2017 Psychill , Ambient and Downtempo music catalog\u010a\u010aTable Fil",
                    "max_token": "ownt",
                    "tokens": [
                        "2017",
                        " Psych",
                        "ill",
                        " ,",
                        " Amb",
                        "ient",
                        " and",
                        " D",
                        "ownt",
                        "em",
                        "po",
                        " music",
                        " catalog",
                        "\u010a",
                        "\u010a",
                        "Table",
                        " Fil"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        12.55685710906982,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " minor-league life coaches giving speeches in dingy hotel ballrooms -- hardly captures the",
                    "max_token": " ding",
                    "tokens": [
                        " minor",
                        "-",
                        "league",
                        " life",
                        " coaches",
                        " giving",
                        " speeches",
                        " in",
                        " ding",
                        "y",
                        " hotel",
                        " ball",
                        "rooms",
                        " --",
                        " hardly",
                        " captures",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        4.507405757904053,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "iological Psychology.\u010a\u010aSinging traditional lullabies and nursery rhymes to babies and",
                    "max_token": " lull",
                    "tokens": [
                        "iological",
                        " Psychology",
                        ".",
                        "\u010a",
                        "\u010a",
                        "S",
                        "inging",
                        " traditional",
                        " lull",
                        "abies",
                        " and",
                        " nursery",
                        " rh",
                        "ymes",
                        " to",
                        " babies",
                        " and"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        21.20301246643066,
                        2.694263219833374,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "GB RAM, 8GB ROM expandable upto 32 GB\u010a\u010aConnectivity :",
                    "max_token": " upt",
                    "tokens": [
                        "GB",
                        " RAM",
                        ",",
                        " 8",
                        "GB",
                        " ROM",
                        " expand",
                        "able",
                        " upt",
                        "o",
                        " 32",
                        " GB",
                        "\u010a",
                        "\u010a",
                        "Connect",
                        "ivity",
                        " :"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        16.85390090942383,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " one of the hardest hit areas by the downturn in coal mining, losing 1,500",
                    "max_token": " downturn",
                    "tokens": [
                        " one",
                        " of",
                        " the",
                        " hardest",
                        " hit",
                        " areas",
                        " by",
                        " the",
                        " downturn",
                        " in",
                        " coal",
                        " mining",
                        ",",
                        " losing",
                        " 1",
                        ",",
                        "500"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        16.9974536895752,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " most rich countries. But \u00e2\u0122\u013epremiumisation\u00e2\u0122\u013f is working. Though still",
                    "tokens": [
                        " most",
                        " rich",
                        " countries",
                        ".",
                        " But",
                        " \u00e2\u0122",
                        "\u013e",
                        "prem",
                        "ium",
                        "isation",
                        "\u00e2\u0122",
                        "\u013f",
                        " is",
                        " working",
                        ".",
                        " Though",
                        " still"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " outfit in the top 20. The Denver Nuggets reportedly spend \u00c2\u00a32,939,",
                    "tokens": [
                        " outfit",
                        " in",
                        " the",
                        " top",
                        " 20",
                        ".",
                        " The",
                        " Denver",
                        " Nuggets",
                        " reportedly",
                        " spend",
                        " \u00c2\u00a3",
                        "2",
                        ",",
                        "9",
                        "39",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "if Al-Arab al-Qathafi, where three of Colonel Gaddafi's grandchildren",
                    "tokens": [
                        "if",
                        " Al",
                        "-",
                        "Arab",
                        " al",
                        "-",
                        "Q",
                        "ath",
                        "afi",
                        ",",
                        " where",
                        " three",
                        " of",
                        " Colonel",
                        " Gaddafi",
                        "'s",
                        " grandchildren"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "2 14 Lubbock +4 20 Denver +2 23 Portland +2 7 Birmingham",
                    "tokens": [
                        "2",
                        " 14",
                        " L",
                        "ubb",
                        "ock",
                        " +",
                        "4",
                        " 20",
                        " Denver",
                        " +",
                        "2",
                        " 23",
                        " Portland",
                        " +",
                        "2",
                        " 7",
                        " Birmingham"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " has to be levied, it should be levied on the Central and state governments and the",
                    "tokens": [
                        " has",
                        " to",
                        " be",
                        " levied",
                        ",",
                        " it",
                        " should",
                        " be",
                        " levied",
                        " on",
                        " the",
                        " Central",
                        " and",
                        " state",
                        " governments",
                        " and",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 21.84278297424316
        },
        {
            "feature_index": 14502,
            "gpt_predictions": [
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "mentions of royalties and related terms",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " Jemima\u00e2\u0122\u013f that entitled her to royalties, including a percentage of the proceeds,",
                    "max_token": " royalties",
                    "tokens": [
                        " Jem",
                        "ima",
                        "\u00e2\u0122",
                        "\u013f",
                        " that",
                        " entitled",
                        " her",
                        " to",
                        " royalties",
                        ",",
                        " including",
                        " a",
                        " percentage",
                        " of",
                        " the",
                        " proceeds",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        28.68813896179199,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " only to increase sales, but to increase royalties.\u00e2\u0122\u013f<|endoftext|>DURHAM,",
                    "max_token": " royalties",
                    "tokens": [
                        " only",
                        " to",
                        " increase",
                        " sales",
                        ",",
                        " but",
                        " to",
                        " increase",
                        " royalties",
                        ".",
                        "\u00e2\u0122",
                        "\u013f",
                        "<|endoftext|>",
                        "D",
                        "UR",
                        "HAM",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        29.10653495788574,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "azy shakedown artist that had been demanding royalties for Jay Z\u00e2\u0122\u013bs use of",
                    "max_token": " royalties",
                    "tokens": [
                        "azy",
                        " sh",
                        "akedown",
                        " artist",
                        " that",
                        " had",
                        " been",
                        " demanding",
                        " royalties",
                        " for",
                        " Jay",
                        " Z",
                        "\u00e2\u0122",
                        "\u013b",
                        "s",
                        " use",
                        " of"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        29.30512619018555,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " and sets of consumers. Some [intellectual properties] lend themselves to maybe a younger",
                    "max_token": "ellectual",
                    "tokens": [
                        " and",
                        " sets",
                        " of",
                        " consumers",
                        ".",
                        " Some",
                        " [",
                        "int",
                        "ellectual",
                        " properties",
                        "]",
                        " lend",
                        " themselves",
                        " to",
                        " maybe",
                        " a",
                        " younger"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        3.807226419448853,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " adding that the heirs determined they were owed royalties when they say they discovered last October that",
                    "max_token": " royalties",
                    "tokens": [
                        " adding",
                        " that",
                        " the",
                        " heirs",
                        " determined",
                        " they",
                        " were",
                        " owed",
                        " royalties",
                        " when",
                        " they",
                        " say",
                        " they",
                        " discovered",
                        " last",
                        " October",
                        " that"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        3.079203128814697,
                        0,
                        0,
                        0,
                        0.4787276387214661,
                        28.12441444396973,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " most rich countries. But \u00e2\u0122\u013epremiumisation\u00e2\u0122\u013f is working. Though still",
                    "tokens": [
                        " most",
                        " rich",
                        " countries",
                        ".",
                        " But",
                        " \u00e2\u0122",
                        "\u013e",
                        "prem",
                        "ium",
                        "isation",
                        "\u00e2\u0122",
                        "\u013f",
                        " is",
                        " working",
                        ".",
                        " Though",
                        " still"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " outfit in the top 20. The Denver Nuggets reportedly spend \u00c2\u00a32,939,",
                    "tokens": [
                        " outfit",
                        " in",
                        " the",
                        " top",
                        " 20",
                        ".",
                        " The",
                        " Denver",
                        " Nuggets",
                        " reportedly",
                        " spend",
                        " \u00c2\u00a3",
                        "2",
                        ",",
                        "9",
                        "39",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "if Al-Arab al-Qathafi, where three of Colonel Gaddafi's grandchildren",
                    "tokens": [
                        "if",
                        " Al",
                        "-",
                        "Arab",
                        " al",
                        "-",
                        "Q",
                        "ath",
                        "afi",
                        ",",
                        " where",
                        " three",
                        " of",
                        " Colonel",
                        " Gaddafi",
                        "'s",
                        " grandchildren"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "2 14 Lubbock +4 20 Denver +2 23 Portland +2 7 Birmingham",
                    "tokens": [
                        "2",
                        " 14",
                        " L",
                        "ubb",
                        "ock",
                        " +",
                        "4",
                        " 20",
                        " Denver",
                        " +",
                        "2",
                        " 23",
                        " Portland",
                        " +",
                        "2",
                        " 7",
                        " Birmingham"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " has to be levied, it should be levied on the Central and state governments and the",
                    "tokens": [
                        " has",
                        " to",
                        " be",
                        " levied",
                        ",",
                        " it",
                        " should",
                        " be",
                        " levied",
                        " on",
                        " the",
                        " Central",
                        " and",
                        " state",
                        " governments",
                        " and",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 36.37820053100586
        },
        {
            "feature_index": 8640,
            "gpt_predictions": [
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "the word \"explosion\"",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": ". Survivors recalled sensing the crash and an explosion, causing panic on the vessel.[6",
                    "max_token": " explosion",
                    "tokens": [
                        ".",
                        " Survivors",
                        " recalled",
                        " sensing",
                        " the",
                        " crash",
                        " and",
                        " an",
                        " explosion",
                        ",",
                        " causing",
                        " panic",
                        " on",
                        " the",
                        " vessel",
                        ".[",
                        "6"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        35.30068969726562,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "\u00e2\u0122\u013f, so named after the nuclear explosion they have on their labels. So,",
                    "max_token": " explosion",
                    "tokens": [
                        "\u00e2\u0122",
                        "\u013f",
                        ",",
                        " so",
                        " named",
                        " after",
                        " the",
                        " nuclear",
                        " explosion",
                        " they",
                        " have",
                        " on",
                        " their",
                        " labels",
                        ".",
                        " So",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        34.72486877441406,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " I heard people saying: \u00e2\u0122\u013aexplosion\u00e2\u0122\u013b,\u00e2\u0122\u013f one of the",
                    "max_token": "osion",
                    "tokens": [
                        " I",
                        " heard",
                        " people",
                        " saying",
                        ":",
                        " \u00e2\u0122",
                        "\u013a",
                        "expl",
                        "osion",
                        "\u00e2\u0122",
                        "\u013b",
                        ",",
                        "\u00e2\u0122",
                        "\u013f",
                        " one",
                        " of",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        1.844995379447937,
                        15.48833465576172,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " an Islamist group, claimed responsibility for the explosion. The FSA claimed its compatriots planted",
                    "max_token": " explosion",
                    "tokens": [
                        " an",
                        " Islamist",
                        " group",
                        ",",
                        " claimed",
                        " responsibility",
                        " for",
                        " the",
                        " explosion",
                        ".",
                        " The",
                        " FSA",
                        " claimed",
                        " its",
                        " compat",
                        "riots",
                        " planted"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        36.28956985473633,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " Mexico oil spill crisis, which followed an explosion on a drilling rig in April.\u010a",
                    "max_token": " explosion",
                    "tokens": [
                        " Mexico",
                        " oil",
                        " spill",
                        " crisis",
                        ",",
                        " which",
                        " followed",
                        " an",
                        " explosion",
                        " on",
                        " a",
                        " drilling",
                        " rig",
                        " in",
                        " April",
                        ".",
                        "\u010a"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        34.12986373901367,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " most rich countries. But \u00e2\u0122\u013epremiumisation\u00e2\u0122\u013f is working. Though still",
                    "tokens": [
                        " most",
                        " rich",
                        " countries",
                        ".",
                        " But",
                        " \u00e2\u0122",
                        "\u013e",
                        "prem",
                        "ium",
                        "isation",
                        "\u00e2\u0122",
                        "\u013f",
                        " is",
                        " working",
                        ".",
                        " Though",
                        " still"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " outfit in the top 20. The Denver Nuggets reportedly spend \u00c2\u00a32,939,",
                    "tokens": [
                        " outfit",
                        " in",
                        " the",
                        " top",
                        " 20",
                        ".",
                        " The",
                        " Denver",
                        " Nuggets",
                        " reportedly",
                        " spend",
                        " \u00c2\u00a3",
                        "2",
                        ",",
                        "9",
                        "39",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "if Al-Arab al-Qathafi, where three of Colonel Gaddafi's grandchildren",
                    "tokens": [
                        "if",
                        " Al",
                        "-",
                        "Arab",
                        " al",
                        "-",
                        "Q",
                        "ath",
                        "afi",
                        ",",
                        " where",
                        " three",
                        " of",
                        " Colonel",
                        " Gaddafi",
                        "'s",
                        " grandchildren"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "2 14 Lubbock +4 20 Denver +2 23 Portland +2 7 Birmingham",
                    "tokens": [
                        "2",
                        " 14",
                        " L",
                        "ubb",
                        "ock",
                        " +",
                        "4",
                        " 20",
                        " Denver",
                        " +",
                        "2",
                        " 23",
                        " Portland",
                        " +",
                        "2",
                        " 7",
                        " Birmingham"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " has to be levied, it should be levied on the Central and state governments and the",
                    "tokens": [
                        " has",
                        " to",
                        " be",
                        " levied",
                        ",",
                        " it",
                        " should",
                        " be",
                        " levied",
                        " on",
                        " the",
                        " Central",
                        " and",
                        " state",
                        " governments",
                        " and",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 38.22345733642578
        },
        {
            "feature_index": 23340,
            "gpt_predictions": [
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    1.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "proper nouns from a specific region, potentially related to politics or current events",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " program at the university\u00e2\u0122\u013bs flagship Urbana-Champaign campus.\u010a",
                    "max_token": " Ur",
                    "tokens": [
                        " program",
                        " at",
                        " the",
                        " university",
                        "\u00e2\u0122",
                        "\u013b",
                        "s",
                        " flagship",
                        " Ur",
                        "b",
                        "ana",
                        "-",
                        "Champ",
                        "aign",
                        " campus",
                        ".",
                        "\u010a"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        45.05602264404297,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "Sanjay Singh doesn\u00e2\u0122\u013bt know Urdu. But whenever his friends are in",
                    "max_token": " Ur",
                    "tokens": [
                        "San",
                        "jay",
                        " Singh",
                        " doesn",
                        "\u00e2\u0122",
                        "\u013b",
                        "t",
                        " know",
                        " Ur",
                        "du",
                        ".",
                        " But",
                        " whenever",
                        " his",
                        " friends",
                        " are",
                        " in"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        44.22660446166992,
                        4.333220481872559,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "oza Post published a lengthy story on Urrutigoity last week. C",
                    "max_token": " Ur",
                    "tokens": [
                        "o",
                        "za",
                        " Post",
                        " published",
                        " a",
                        " lengthy",
                        " story",
                        " on",
                        " Ur",
                        "r",
                        "ut",
                        "igo",
                        "ity",
                        " last",
                        " week",
                        ".",
                        " C"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        45.23063278198242,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "kIzsgP \u00e2\u0122\u0136 Robyn Urback (@RobynUrback) February",
                    "max_token": " Ur",
                    "tokens": [
                        "k",
                        "I",
                        "z",
                        "sg",
                        "P",
                        " \u00e2\u0122\u0136",
                        " Rob",
                        "yn",
                        " Ur",
                        "back",
                        " (@",
                        "Rob",
                        "yn",
                        "Ur",
                        "back",
                        ")",
                        " February"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        42.62932205200195,
                        0,
                        0,
                        0,
                        0,
                        33.71260452270508,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " spikes in property taxes.\"\u010a\u010aDe Uriarte said gentrification-fueled displacement",
                    "max_token": " Uri",
                    "tokens": [
                        " spikes",
                        " in",
                        " property",
                        " taxes",
                        ".\"",
                        "\u010a",
                        "\u010a",
                        "De",
                        " Uri",
                        "arte",
                        " said",
                        " gent",
                        "rification",
                        "-",
                        "fuel",
                        "ed",
                        " displacement"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        6.545939445495605,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " most rich countries. But \u00e2\u0122\u013epremiumisation\u00e2\u0122\u013f is working. Though still",
                    "tokens": [
                        " most",
                        " rich",
                        " countries",
                        ".",
                        " But",
                        " \u00e2\u0122",
                        "\u013e",
                        "prem",
                        "ium",
                        "isation",
                        "\u00e2\u0122",
                        "\u013f",
                        " is",
                        " working",
                        ".",
                        " Though",
                        " still"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " outfit in the top 20. The Denver Nuggets reportedly spend \u00c2\u00a32,939,",
                    "tokens": [
                        " outfit",
                        " in",
                        " the",
                        " top",
                        " 20",
                        ".",
                        " The",
                        " Denver",
                        " Nuggets",
                        " reportedly",
                        " spend",
                        " \u00c2\u00a3",
                        "2",
                        ",",
                        "9",
                        "39",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "if Al-Arab al-Qathafi, where three of Colonel Gaddafi's grandchildren",
                    "tokens": [
                        "if",
                        " Al",
                        "-",
                        "Arab",
                        " al",
                        "-",
                        "Q",
                        "ath",
                        "afi",
                        ",",
                        " where",
                        " three",
                        " of",
                        " Colonel",
                        " Gaddafi",
                        "'s",
                        " grandchildren"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "2 14 Lubbock +4 20 Denver +2 23 Portland +2 7 Birmingham",
                    "tokens": [
                        "2",
                        " 14",
                        " L",
                        "ubb",
                        "ock",
                        " +",
                        "4",
                        " 20",
                        " Denver",
                        " +",
                        "2",
                        " 23",
                        " Portland",
                        " +",
                        "2",
                        " 7",
                        " Birmingham"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " has to be levied, it should be levied on the Central and state governments and the",
                    "tokens": [
                        " has",
                        " to",
                        " be",
                        " levied",
                        ",",
                        " it",
                        " should",
                        " be",
                        " levied",
                        " on",
                        " the",
                        " Central",
                        " and",
                        " state",
                        " governments",
                        " and",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 46.50008392333984
        },
        {
            "feature_index": 7305,
            "gpt_predictions": [
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "mentions of places related to cultural or leisure activities, particularly theaters",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " Scots-connected films through its programme: theatre director Graeme Maley is bringing two",
                    "max_token": " theatre",
                    "tokens": [
                        " Scots",
                        "-",
                        "connected",
                        " films",
                        " through",
                        " its",
                        " programme",
                        ":",
                        " theatre",
                        " director",
                        " Gra",
                        "eme",
                        " Male",
                        "y",
                        " is",
                        " bringing",
                        " two"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        28.5003833770752,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " 12pm. Following the activities in the theatre, a full dress-parade,",
                    "max_token": " theatre",
                    "tokens": [
                        " 12",
                        "pm",
                        ".",
                        " Following",
                        " the",
                        " activities",
                        " in",
                        " the",
                        " theatre",
                        ",",
                        " a",
                        " full",
                        " dress",
                        "-",
                        "par",
                        "ade",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        29.78068351745605,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " Tallinn you can walk home from the theatre unmolested as late as you like",
                    "max_token": " theatre",
                    "tokens": [
                        " Tall",
                        "inn",
                        " you",
                        " can",
                        " walk",
                        " home",
                        " from",
                        " the",
                        " theatre",
                        " unm",
                        "ol",
                        "ested",
                        " as",
                        " late",
                        " as",
                        " you",
                        " like"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        28.00564002990723,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " percent.\u010a\u010aview photo essay Political Theatre: The presidential campaign of Donald Trump \u00e2\u0122",
                    "max_token": " Theatre",
                    "tokens": [
                        " percent",
                        ".",
                        "\u010a",
                        "\u010a",
                        "view",
                        " photo",
                        " essay",
                        " Political",
                        " Theatre",
                        ":",
                        " The",
                        " presidential",
                        " campaign",
                        " of",
                        " Donald",
                        " Trump",
                        " \u00e2\u0122"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        4.598790645599365,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "Awlaki wasn't in a movie theater when his life met a sudden, violent",
                    "max_token": " theater",
                    "tokens": [
                        "Aw",
                        "l",
                        "aki",
                        " wasn",
                        "'t",
                        " in",
                        " a",
                        " movie",
                        " theater",
                        " when",
                        " his",
                        " life",
                        " met",
                        " a",
                        " sudden",
                        ",",
                        " violent"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        14.46379566192627,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " most rich countries. But \u00e2\u0122\u013epremiumisation\u00e2\u0122\u013f is working. Though still",
                    "tokens": [
                        " most",
                        " rich",
                        " countries",
                        ".",
                        " But",
                        " \u00e2\u0122",
                        "\u013e",
                        "prem",
                        "ium",
                        "isation",
                        "\u00e2\u0122",
                        "\u013f",
                        " is",
                        " working",
                        ".",
                        " Though",
                        " still"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " outfit in the top 20. The Denver Nuggets reportedly spend \u00c2\u00a32,939,",
                    "tokens": [
                        " outfit",
                        " in",
                        " the",
                        " top",
                        " 20",
                        ".",
                        " The",
                        " Denver",
                        " Nuggets",
                        " reportedly",
                        " spend",
                        " \u00c2\u00a3",
                        "2",
                        ",",
                        "9",
                        "39",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "if Al-Arab al-Qathafi, where three of Colonel Gaddafi's grandchildren",
                    "tokens": [
                        "if",
                        " Al",
                        "-",
                        "Arab",
                        " al",
                        "-",
                        "Q",
                        "ath",
                        "afi",
                        ",",
                        " where",
                        " three",
                        " of",
                        " Colonel",
                        " Gaddafi",
                        "'s",
                        " grandchildren"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "2 14 Lubbock +4 20 Denver +2 23 Portland +2 7 Birmingham",
                    "tokens": [
                        "2",
                        " 14",
                        " L",
                        "ubb",
                        "ock",
                        " +",
                        "4",
                        " 20",
                        " Denver",
                        " +",
                        "2",
                        " 23",
                        " Portland",
                        " +",
                        "2",
                        " 7",
                        " Birmingham"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " has to be levied, it should be levied on the Central and state governments and the",
                    "tokens": [
                        " has",
                        " to",
                        " be",
                        " levied",
                        ",",
                        " it",
                        " should",
                        " be",
                        " levied",
                        " on",
                        " the",
                        " Central",
                        " and",
                        " state",
                        " governments",
                        " and",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 31.86441230773926
        },
        {
            "feature_index": 22431,
            "gpt_predictions": [
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "the phrase 'no one' followed by a verb",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " nine major industrialized democracies and found that every one of the parties accepts climate science except for",
                    "max_token": " one",
                    "tokens": [
                        " nine",
                        " major",
                        " industrialized",
                        " democracies",
                        " and",
                        " found",
                        " that",
                        " every",
                        " one",
                        " of",
                        " the",
                        " parties",
                        " accepts",
                        " climate",
                        " science",
                        " except",
                        " for"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        6.171360015869141,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " seems to have lost his nerve. No one really knows what he stands for or whether",
                    "max_token": " one",
                    "tokens": [
                        " seems",
                        " to",
                        " have",
                        " lost",
                        " his",
                        " nerve",
                        ".",
                        " No",
                        " one",
                        " really",
                        " knows",
                        " what",
                        " he",
                        " stands",
                        " for",
                        " or",
                        " whether"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        27.41946792602539,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "ina came under communist control, but no one much was talking, any longer, about",
                    "max_token": " one",
                    "tokens": [
                        "ina",
                        " came",
                        " under",
                        " communist",
                        " control",
                        ",",
                        " but",
                        " no",
                        " one",
                        " much",
                        " was",
                        " talking",
                        ",",
                        " any",
                        " longer",
                        ",",
                        " about"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        28.38430786132812,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " put out the smartwatch, but no one has been able to make something you'd",
                    "max_token": " one",
                    "tokens": [
                        " put",
                        " out",
                        " the",
                        " smart",
                        "watch",
                        ",",
                        " but",
                        " no",
                        " one",
                        " has",
                        " been",
                        " able",
                        " to",
                        " make",
                        " something",
                        " you",
                        "'d"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        28.60117149353027,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " Austin get justice?\u010a\u010a\"No one deserves to die like that. No one",
                    "max_token": " one",
                    "tokens": [
                        " Austin",
                        " get",
                        " justice",
                        "?",
                        "\u010a",
                        "\u010a",
                        "\"",
                        "No",
                        " one",
                        " deserves",
                        " to",
                        " die",
                        " like",
                        " that",
                        ".",
                        " No",
                        " one"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        28.41958236694336,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        24.63691139221191
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " most rich countries. But \u00e2\u0122\u013epremiumisation\u00e2\u0122\u013f is working. Though still",
                    "tokens": [
                        " most",
                        " rich",
                        " countries",
                        ".",
                        " But",
                        " \u00e2\u0122",
                        "\u013e",
                        "prem",
                        "ium",
                        "isation",
                        "\u00e2\u0122",
                        "\u013f",
                        " is",
                        " working",
                        ".",
                        " Though",
                        " still"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " outfit in the top 20. The Denver Nuggets reportedly spend \u00c2\u00a32,939,",
                    "tokens": [
                        " outfit",
                        " in",
                        " the",
                        " top",
                        " 20",
                        ".",
                        " The",
                        " Denver",
                        " Nuggets",
                        " reportedly",
                        " spend",
                        " \u00c2\u00a3",
                        "2",
                        ",",
                        "9",
                        "39",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "if Al-Arab al-Qathafi, where three of Colonel Gaddafi's grandchildren",
                    "tokens": [
                        "if",
                        " Al",
                        "-",
                        "Arab",
                        " al",
                        "-",
                        "Q",
                        "ath",
                        "afi",
                        ",",
                        " where",
                        " three",
                        " of",
                        " Colonel",
                        " Gaddafi",
                        "'s",
                        " grandchildren"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "2 14 Lubbock +4 20 Denver +2 23 Portland +2 7 Birmingham",
                    "tokens": [
                        "2",
                        " 14",
                        " L",
                        "ubb",
                        "ock",
                        " +",
                        "4",
                        " 20",
                        " Denver",
                        " +",
                        "2",
                        " 23",
                        " Portland",
                        " +",
                        "2",
                        " 7",
                        " Birmingham"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " has to be levied, it should be levied on the Central and state governments and the",
                    "tokens": [
                        " has",
                        " to",
                        " be",
                        " levied",
                        ",",
                        " it",
                        " should",
                        " be",
                        " levied",
                        " on",
                        " the",
                        " Central",
                        " and",
                        " state",
                        " governments",
                        " and",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 30.26328277587891
        },
        {
            "feature_index": 6310,
            "gpt_predictions": [
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "launchers",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "uss Cannon\u010a\u010aLauncher Precision Grenade Launcher\u010a\u010aSlashing Weapon Phasing Sab",
                    "max_token": " Launcher",
                    "tokens": [
                        "uss",
                        " Cannon",
                        "\u010a",
                        "\u010a",
                        "Laun",
                        "cher",
                        " Precision",
                        " Grenade",
                        " Launcher",
                        "\u010a",
                        "\u010a",
                        "Sl",
                        "ashing",
                        " Weapon",
                        " Ph",
                        "asing",
                        " Sab"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        16.96294784545898,
                        7.612189769744873,
                        0,
                        0.3343582451343536,
                        27.9841423034668,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": ", in which you can get a rocket launcher and light half a dozen opponents on fire",
                    "max_token": " launcher",
                    "tokens": [
                        ",",
                        " in",
                        " which",
                        " you",
                        " can",
                        " get",
                        " a",
                        " rocket",
                        " launcher",
                        " and",
                        " light",
                        " half",
                        " a",
                        " dozen",
                        " opponents",
                        " on",
                        " fire"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        28.24215316772461,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " latest Android Nougat based Nokia Z Launcher. The device will be manufactured by Fox",
                    "max_token": " Launcher",
                    "tokens": [
                        " latest",
                        " Android",
                        " N",
                        "oug",
                        "at",
                        " based",
                        " Nokia",
                        " Z",
                        " Launcher",
                        ".",
                        " The",
                        " device",
                        " will",
                        " be",
                        " manufactured",
                        " by",
                        " Fox"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        30.40438842773438,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "'s respective function. The tiles can represent launchers, running applications, or Unity lenses.",
                    "max_token": " launchers",
                    "tokens": [
                        "'s",
                        " respective",
                        " function",
                        ".",
                        " The",
                        " tiles",
                        " can",
                        " represent",
                        " launchers",
                        ",",
                        " running",
                        " applications",
                        ",",
                        " or",
                        " Unity",
                        " lenses",
                        "."
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        31.57864570617676,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " serves as a task management interface and a launcher for regularly used applications. It is functionally",
                    "max_token": " launcher",
                    "tokens": [
                        " serves",
                        " as",
                        " a",
                        " task",
                        " management",
                        " interface",
                        " and",
                        " a",
                        " launcher",
                        " for",
                        " regularly",
                        " used",
                        " applications",
                        ".",
                        " It",
                        " is",
                        " functionally"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        33.38713836669922,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " most rich countries. But \u00e2\u0122\u013epremiumisation\u00e2\u0122\u013f is working. Though still",
                    "tokens": [
                        " most",
                        " rich",
                        " countries",
                        ".",
                        " But",
                        " \u00e2\u0122",
                        "\u013e",
                        "prem",
                        "ium",
                        "isation",
                        "\u00e2\u0122",
                        "\u013f",
                        " is",
                        " working",
                        ".",
                        " Though",
                        " still"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " outfit in the top 20. The Denver Nuggets reportedly spend \u00c2\u00a32,939,",
                    "tokens": [
                        " outfit",
                        " in",
                        " the",
                        " top",
                        " 20",
                        ".",
                        " The",
                        " Denver",
                        " Nuggets",
                        " reportedly",
                        " spend",
                        " \u00c2\u00a3",
                        "2",
                        ",",
                        "9",
                        "39",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "if Al-Arab al-Qathafi, where three of Colonel Gaddafi's grandchildren",
                    "tokens": [
                        "if",
                        " Al",
                        "-",
                        "Arab",
                        " al",
                        "-",
                        "Q",
                        "ath",
                        "afi",
                        ",",
                        " where",
                        " three",
                        " of",
                        " Colonel",
                        " Gaddafi",
                        "'s",
                        " grandchildren"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "2 14 Lubbock +4 20 Denver +2 23 Portland +2 7 Birmingham",
                    "tokens": [
                        "2",
                        " 14",
                        " L",
                        "ubb",
                        "ock",
                        " +",
                        "4",
                        " 20",
                        " Denver",
                        " +",
                        "2",
                        " 23",
                        " Portland",
                        " +",
                        "2",
                        " 7",
                        " Birmingham"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " has to be levied, it should be levied on the Central and state governments and the",
                    "tokens": [
                        " has",
                        " to",
                        " be",
                        " levied",
                        ",",
                        " it",
                        " should",
                        " be",
                        " levied",
                        " on",
                        " the",
                        " Central",
                        " and",
                        " state",
                        " governments",
                        " and",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 35.11127090454102
        },
        {
            "feature_index": 11723,
            "gpt_predictions": [
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "mentions of the Sundance Film Festival",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "...no religion condones the killing of innocents...and they\u00e2\u0122\u013bre certainly",
                    "max_token": " innoc",
                    "tokens": [
                        "...",
                        "no",
                        " religion",
                        " cond",
                        "ones",
                        " the",
                        " killing",
                        " of",
                        " innoc",
                        "ents",
                        "...",
                        "and",
                        " they",
                        "\u00e2\u0122",
                        "\u013b",
                        "re",
                        " certainly"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        5.90132999420166,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " in places. A large area of the Sunda Strait and a number of places on",
                    "max_token": " Sund",
                    "tokens": [
                        " in",
                        " places",
                        ".",
                        " A",
                        " large",
                        " area",
                        " of",
                        " the",
                        " Sund",
                        "a",
                        " Strait",
                        " and",
                        " a",
                        " number",
                        " of",
                        " places",
                        " on"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        49.89335632324219,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "s to get around restrictions. Even Peter Sunde, the founder of The Pirate Bay",
                    "max_token": " Sund",
                    "tokens": [
                        "s",
                        " to",
                        " get",
                        " around",
                        " restrictions",
                        ".",
                        " Even",
                        " Peter",
                        " Sund",
                        "e",
                        ",",
                        " the",
                        " founder",
                        " of",
                        " The",
                        " Pirate",
                        " Bay"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        51.11563491821289,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " the coast of Kennebunkport sunday morning. So rising early and always",
                    "max_token": " sund",
                    "tokens": [
                        " the",
                        " coast",
                        " of",
                        " Ken",
                        "ne",
                        "b",
                        "unk",
                        "port",
                        " sund",
                        "ay",
                        " morning",
                        ".",
                        " So",
                        " rising",
                        " early",
                        " and",
                        " always"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        24.37748718261719,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 7,
                    "sentence_string": " financial interests.\u010a\u010aPosted by Sundance \u00e2\u0122\u013eMultinational Banks and Corporations",
                    "max_token": " Sund",
                    "tokens": [
                        " financial",
                        " interests",
                        ".",
                        "\u010a",
                        "\u010a",
                        "Posted",
                        " by",
                        " Sund",
                        "ance",
                        " \u00e2\u0122",
                        "\u013e",
                        "Mult",
                        "inational",
                        " Banks",
                        " and",
                        " Corpor",
                        "ations"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        48.47374725341797,
                        5.453267574310303,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " most rich countries. But \u00e2\u0122\u013epremiumisation\u00e2\u0122\u013f is working. Though still",
                    "tokens": [
                        " most",
                        " rich",
                        " countries",
                        ".",
                        " But",
                        " \u00e2\u0122",
                        "\u013e",
                        "prem",
                        "ium",
                        "isation",
                        "\u00e2\u0122",
                        "\u013f",
                        " is",
                        " working",
                        ".",
                        " Though",
                        " still"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " outfit in the top 20. The Denver Nuggets reportedly spend \u00c2\u00a32,939,",
                    "tokens": [
                        " outfit",
                        " in",
                        " the",
                        " top",
                        " 20",
                        ".",
                        " The",
                        " Denver",
                        " Nuggets",
                        " reportedly",
                        " spend",
                        " \u00c2\u00a3",
                        "2",
                        ",",
                        "9",
                        "39",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "if Al-Arab al-Qathafi, where three of Colonel Gaddafi's grandchildren",
                    "tokens": [
                        "if",
                        " Al",
                        "-",
                        "Arab",
                        " al",
                        "-",
                        "Q",
                        "ath",
                        "afi",
                        ",",
                        " where",
                        " three",
                        " of",
                        " Colonel",
                        " Gaddafi",
                        "'s",
                        " grandchildren"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "2 14 Lubbock +4 20 Denver +2 23 Portland +2 7 Birmingham",
                    "tokens": [
                        "2",
                        " 14",
                        " L",
                        "ubb",
                        "ock",
                        " +",
                        "4",
                        " 20",
                        " Denver",
                        " +",
                        "2",
                        " 23",
                        " Portland",
                        " +",
                        "2",
                        " 7",
                        " Birmingham"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " has to be levied, it should be levied on the Central and state governments and the",
                    "tokens": [
                        " has",
                        " to",
                        " be",
                        " levied",
                        ",",
                        " it",
                        " should",
                        " be",
                        " levied",
                        " on",
                        " the",
                        " Central",
                        " and",
                        " state",
                        " governments",
                        " and",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 54.25119781494141
        },
        {
            "feature_index": 2629,
            "gpt_predictions": [
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "words related to science fiction (sci-fi) and potentially fantasy",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " other cards. Gain victory points by launching fiendish attacks against rival players to force their",
                    "max_token": " fi",
                    "tokens": [
                        " other",
                        " cards",
                        ".",
                        " Gain",
                        " victory",
                        " points",
                        " by",
                        " launching",
                        " fi",
                        "endish",
                        " attacks",
                        " against",
                        " rival",
                        " players",
                        " to",
                        " force",
                        " their"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        14.66834831237793,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "\u00c3\u00a4 Lautapeliopas.fi pit\u00c3\u00a4isi l\u00c3\u00b6yty\u00c3\u00a4",
                    "max_token": "fi",
                    "tokens": [
                        "\u00c3\u00a4",
                        " L",
                        "aut",
                        "ap",
                        "el",
                        "iop",
                        "as",
                        ".",
                        "fi",
                        " pit",
                        "\u00c3\u00a4",
                        "isi",
                        " l",
                        "\u00c3\u00b6",
                        "y",
                        "ty",
                        "\u00c3\u00a4"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        22.01748275756836,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " known for playing Dave Lister in sci fi comedy Red Dwarf, but he has probably",
                    "max_token": " fi",
                    "tokens": [
                        " known",
                        " for",
                        " playing",
                        " Dave",
                        " L",
                        "ister",
                        " in",
                        " sci",
                        " fi",
                        " comedy",
                        " Red",
                        " Dwarf",
                        ",",
                        " but",
                        " he",
                        " has",
                        " probably"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        18.59081649780273,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " and Ellie; ideas for a sci-fi game; ideas for a fantasy game,",
                    "max_token": "fi",
                    "tokens": [
                        " and",
                        " Ellie",
                        ";",
                        " ideas",
                        " for",
                        " a",
                        " sci",
                        "-",
                        "fi",
                        " game",
                        ";",
                        " ideas",
                        " for",
                        " a",
                        " fantasy",
                        " game",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        25.0720043182373,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " lot of these properties as straight sci-fi if you wanted.\u010a\u010aThere really",
                    "max_token": "fi",
                    "tokens": [
                        " lot",
                        " of",
                        " these",
                        " properties",
                        " as",
                        " straight",
                        " sci",
                        "-",
                        "fi",
                        " if",
                        " you",
                        " wanted",
                        ".",
                        "\u010a",
                        "\u010a",
                        "There",
                        " really"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        23.11956214904785,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " most rich countries. But \u00e2\u0122\u013epremiumisation\u00e2\u0122\u013f is working. Though still",
                    "tokens": [
                        " most",
                        " rich",
                        " countries",
                        ".",
                        " But",
                        " \u00e2\u0122",
                        "\u013e",
                        "prem",
                        "ium",
                        "isation",
                        "\u00e2\u0122",
                        "\u013f",
                        " is",
                        " working",
                        ".",
                        " Though",
                        " still"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " outfit in the top 20. The Denver Nuggets reportedly spend \u00c2\u00a32,939,",
                    "tokens": [
                        " outfit",
                        " in",
                        " the",
                        " top",
                        " 20",
                        ".",
                        " The",
                        " Denver",
                        " Nuggets",
                        " reportedly",
                        " spend",
                        " \u00c2\u00a3",
                        "2",
                        ",",
                        "9",
                        "39",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "if Al-Arab al-Qathafi, where three of Colonel Gaddafi's grandchildren",
                    "tokens": [
                        "if",
                        " Al",
                        "-",
                        "Arab",
                        " al",
                        "-",
                        "Q",
                        "ath",
                        "afi",
                        ",",
                        " where",
                        " three",
                        " of",
                        " Colonel",
                        " Gaddafi",
                        "'s",
                        " grandchildren"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "2 14 Lubbock +4 20 Denver +2 23 Portland +2 7 Birmingham",
                    "tokens": [
                        "2",
                        " 14",
                        " L",
                        "ubb",
                        "ock",
                        " +",
                        "4",
                        " 20",
                        " Denver",
                        " +",
                        "2",
                        " 23",
                        " Portland",
                        " +",
                        "2",
                        " 7",
                        " Birmingham"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " has to be levied, it should be levied on the Central and state governments and the",
                    "tokens": [
                        " has",
                        " to",
                        " be",
                        " levied",
                        ",",
                        " it",
                        " should",
                        " be",
                        " levied",
                        " on",
                        " the",
                        " Central",
                        " and",
                        " state",
                        " governments",
                        " and",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 28.80372428894043
        },
        {
            "feature_index": 16236,
            "gpt_predictions": [
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "keywords related to critiques or reviews",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " break rules on cigarettes should be \u00e2\u0122\u013ecriticized and educated about their evil influence",
                    "max_token": "crit",
                    "tokens": [
                        " break",
                        " rules",
                        " on",
                        " cigarettes",
                        " should",
                        " be",
                        " \u00e2\u0122",
                        "\u013e",
                        "crit",
                        "ic",
                        "ized",
                        " and",
                        " educated",
                        " about",
                        " their",
                        " evil",
                        " influence"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        33.70348358154297,
                        0,
                        3.071844100952148,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " 2017]\u010a\u010aResponses and Critiques\u010a\u010aThe Conversation\u010a\u010aAbout",
                    "max_token": " Crit",
                    "tokens": [
                        " 2017",
                        "]",
                        "\u010a",
                        "\u010a",
                        "Resp",
                        "ons",
                        "es",
                        " and",
                        " Crit",
                        "iques",
                        "\u010a",
                        "\u010a",
                        "The",
                        " Conversation",
                        "\u010a",
                        "\u010a",
                        "About"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        42.16830444335938,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "\u010aWhat are your favorite movies in the Criterion Collection? \u00e2\u0122\u0136 LAist (@LA",
                    "max_token": " Crit",
                    "tokens": [
                        "\u010a",
                        "What",
                        " are",
                        " your",
                        " favorite",
                        " movies",
                        " in",
                        " the",
                        " Crit",
                        "erion",
                        " Collection",
                        "?",
                        " \u00e2\u0122\u0136",
                        " LA",
                        "ist",
                        " (@",
                        "LA"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        43.84647369384766,
                        0.2439782619476318,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " back a week, but his arthritic hip never barked and he was able",
                    "max_token": "ritic",
                    "tokens": [
                        " back",
                        " a",
                        " week",
                        ",",
                        " but",
                        " his",
                        " ar",
                        "th",
                        "ritic",
                        " hip",
                        " never",
                        " bark",
                        "ed",
                        " and",
                        " he",
                        " was",
                        " able"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        5.187674999237061,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " exasperated to hear, was roundly critized for the ostensible crime of using",
                    "max_token": " crit",
                    "tokens": [
                        " exasper",
                        "ated",
                        " to",
                        " hear",
                        ",",
                        " was",
                        " round",
                        "ly",
                        " crit",
                        "ized",
                        " for",
                        " the",
                        " ost",
                        "ensible",
                        " crime",
                        " of",
                        " using"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        18.87851142883301,
                        1.332386016845703,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " most rich countries. But \u00e2\u0122\u013epremiumisation\u00e2\u0122\u013f is working. Though still",
                    "tokens": [
                        " most",
                        " rich",
                        " countries",
                        ".",
                        " But",
                        " \u00e2\u0122",
                        "\u013e",
                        "prem",
                        "ium",
                        "isation",
                        "\u00e2\u0122",
                        "\u013f",
                        " is",
                        " working",
                        ".",
                        " Though",
                        " still"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " outfit in the top 20. The Denver Nuggets reportedly spend \u00c2\u00a32,939,",
                    "tokens": [
                        " outfit",
                        " in",
                        " the",
                        " top",
                        " 20",
                        ".",
                        " The",
                        " Denver",
                        " Nuggets",
                        " reportedly",
                        " spend",
                        " \u00c2\u00a3",
                        "2",
                        ",",
                        "9",
                        "39",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "if Al-Arab al-Qathafi, where three of Colonel Gaddafi's grandchildren",
                    "tokens": [
                        "if",
                        " Al",
                        "-",
                        "Arab",
                        " al",
                        "-",
                        "Q",
                        "ath",
                        "afi",
                        ",",
                        " where",
                        " three",
                        " of",
                        " Colonel",
                        " Gaddafi",
                        "'s",
                        " grandchildren"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "2 14 Lubbock +4 20 Denver +2 23 Portland +2 7 Birmingham",
                    "tokens": [
                        "2",
                        " 14",
                        " L",
                        "ubb",
                        "ock",
                        " +",
                        "4",
                        " 20",
                        " Denver",
                        " +",
                        "2",
                        " 23",
                        " Portland",
                        " +",
                        "2",
                        " 7",
                        " Birmingham"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " has to be levied, it should be levied on the Central and state governments and the",
                    "tokens": [
                        " has",
                        " to",
                        " be",
                        " levied",
                        ",",
                        " it",
                        " should",
                        " be",
                        " levied",
                        " on",
                        " the",
                        " Central",
                        " and",
                        " state",
                        " governments",
                        " and",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 45.61298370361328
        },
        {
            "feature_index": 3634,
            "gpt_predictions": [
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "references to the Affordable Care Act (Obamacare)",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " they should be getting.\u010a\u010aBefore Obamacare we had two major problems with health care",
                    "max_token": " Obamacare",
                    "tokens": [
                        " they",
                        " should",
                        " be",
                        " getting",
                        ".",
                        "\u010a",
                        "\u010a",
                        "Before",
                        " Obamacare",
                        " we",
                        " had",
                        " two",
                        " major",
                        " problems",
                        " with",
                        " health",
                        " care"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        45.27409362792969,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " in effect, months of earned media for Obamacare.\u010a\u010aAnother possible intangible effect is",
                    "max_token": " Obamacare",
                    "tokens": [
                        " in",
                        " effect",
                        ",",
                        " months",
                        " of",
                        " earned",
                        " media",
                        " for",
                        " Obamacare",
                        ".",
                        "\u010a",
                        "\u010a",
                        "Another",
                        " possible",
                        " intangible",
                        " effect",
                        " is"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        45.56931304931641,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " botched rollout of HealthCare.gov, Obamacare\u00e2\u0122\u013bs federal health insurance exchange is",
                    "max_token": " Obamacare",
                    "tokens": [
                        " botched",
                        " rollout",
                        " of",
                        " Health",
                        "Care",
                        ".",
                        "gov",
                        ",",
                        " Obamacare",
                        "\u00e2\u0122",
                        "\u013b",
                        "s",
                        " federal",
                        " health",
                        " insurance",
                        " exchange",
                        " is"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        45.03635406494141,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0.5538486242294312,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " It is not even universal health care. Obamacare is largely an insurance mandate that preserves the",
                    "max_token": " Obamacare",
                    "tokens": [
                        " It",
                        " is",
                        " not",
                        " even",
                        " universal",
                        " health",
                        " care",
                        ".",
                        " Obamacare",
                        " is",
                        " largely",
                        " an",
                        " insurance",
                        " mandate",
                        " that",
                        " preserves",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        40.7277946472168,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " president had the power of incumbency. ObamaCare was very attractive, particularly to those without",
                    "max_token": " ObamaCare",
                    "tokens": [
                        " president",
                        " had",
                        " the",
                        " power",
                        " of",
                        " incumb",
                        "ency",
                        ".",
                        " ObamaCare",
                        " was",
                        " very",
                        " attractive",
                        ",",
                        " particularly",
                        " to",
                        " those",
                        " without"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        21.83872222900391,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " most rich countries. But \u00e2\u0122\u013epremiumisation\u00e2\u0122\u013f is working. Though still",
                    "tokens": [
                        " most",
                        " rich",
                        " countries",
                        ".",
                        " But",
                        " \u00e2\u0122",
                        "\u013e",
                        "prem",
                        "ium",
                        "isation",
                        "\u00e2\u0122",
                        "\u013f",
                        " is",
                        " working",
                        ".",
                        " Though",
                        " still"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " outfit in the top 20. The Denver Nuggets reportedly spend \u00c2\u00a32,939,",
                    "tokens": [
                        " outfit",
                        " in",
                        " the",
                        " top",
                        " 20",
                        ".",
                        " The",
                        " Denver",
                        " Nuggets",
                        " reportedly",
                        " spend",
                        " \u00c2\u00a3",
                        "2",
                        ",",
                        "9",
                        "39",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "if Al-Arab al-Qathafi, where three of Colonel Gaddafi's grandchildren",
                    "tokens": [
                        "if",
                        " Al",
                        "-",
                        "Arab",
                        " al",
                        "-",
                        "Q",
                        "ath",
                        "afi",
                        ",",
                        " where",
                        " three",
                        " of",
                        " Colonel",
                        " Gaddafi",
                        "'s",
                        " grandchildren"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "2 14 Lubbock +4 20 Denver +2 23 Portland +2 7 Birmingham",
                    "tokens": [
                        "2",
                        " 14",
                        " L",
                        "ubb",
                        "ock",
                        " +",
                        "4",
                        " 20",
                        " Denver",
                        " +",
                        "2",
                        " 23",
                        " Portland",
                        " +",
                        "2",
                        " 7",
                        " Birmingham"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " has to be levied, it should be levied on the Central and state governments and the",
                    "tokens": [
                        " has",
                        " to",
                        " be",
                        " levied",
                        ",",
                        " it",
                        " should",
                        " be",
                        " levied",
                        " on",
                        " the",
                        " Central",
                        " and",
                        " state",
                        " governments",
                        " and",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 46.38892364501953
        },
        {
            "feature_index": 16116,
            "gpt_predictions": [
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "words related to administrative terms and institutions, such as \"administrative,\" \"administration,\" \"disintegration,\" and specific references to \"Hendrix.\"",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " g | Calories: 356 kcal | Carbohydrates: 27 g | Protein: 5",
                    "max_token": "ohyd",
                    "tokens": [
                        " g",
                        " |",
                        " Calories",
                        ":",
                        " 356",
                        " kcal",
                        " |",
                        " Carb",
                        "ohyd",
                        "rates",
                        ":",
                        " 27",
                        " g",
                        " |",
                        " Protein",
                        ":",
                        " 5"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        7.168938636779785,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " individual rehabilitation plan so she can be reintegrated back into the community once her sentence",
                    "max_token": "integ",
                    "tokens": [
                        " individual",
                        " rehabilitation",
                        " plan",
                        " so",
                        " she",
                        " can",
                        " be",
                        " re",
                        "integ",
                        "rated",
                        " back",
                        " into",
                        " the",
                        " community",
                        " once",
                        " her",
                        " sentence"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        9.158711433410645,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "forge.se/2016/08/integration-between-torchnet-",
                    "max_token": "integ",
                    "tokens": [
                        "forge",
                        ".",
                        "se",
                        "/",
                        "2016",
                        "/",
                        "08",
                        "/",
                        "integ",
                        "ration",
                        "-",
                        "between",
                        "-",
                        "tor",
                        "chn",
                        "et",
                        "-"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        8.720711708068848,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": ", Putin's regime, Russia, Russian disintegration, Russian-Chechen wars<|endoftext|>",
                    "max_token": " disinteg",
                    "tokens": [
                        ",",
                        " Putin",
                        "'s",
                        " regime",
                        ",",
                        " Russia",
                        ",",
                        " Russian",
                        " disinteg",
                        "ration",
                        ",",
                        " Russian",
                        "-",
                        "Che",
                        "chen",
                        " wars",
                        "<|endoftext|>"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        11.50121021270752,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " PSAs discussing the issues surrounding veteran reintegration. But just months after separating from",
                    "max_token": "integ",
                    "tokens": [
                        " PS",
                        "As",
                        " discussing",
                        " the",
                        " issues",
                        " surrounding",
                        " veteran",
                        " re",
                        "integ",
                        "ration",
                        ".",
                        " But",
                        " just",
                        " months",
                        " after",
                        " separating",
                        " from"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        9.34428596496582,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " most rich countries. But \u00e2\u0122\u013epremiumisation\u00e2\u0122\u013f is working. Though still",
                    "tokens": [
                        " most",
                        " rich",
                        " countries",
                        ".",
                        " But",
                        " \u00e2\u0122",
                        "\u013e",
                        "prem",
                        "ium",
                        "isation",
                        "\u00e2\u0122",
                        "\u013f",
                        " is",
                        " working",
                        ".",
                        " Though",
                        " still"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " outfit in the top 20. The Denver Nuggets reportedly spend \u00c2\u00a32,939,",
                    "tokens": [
                        " outfit",
                        " in",
                        " the",
                        " top",
                        " 20",
                        ".",
                        " The",
                        " Denver",
                        " Nuggets",
                        " reportedly",
                        " spend",
                        " \u00c2\u00a3",
                        "2",
                        ",",
                        "9",
                        "39",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "if Al-Arab al-Qathafi, where three of Colonel Gaddafi's grandchildren",
                    "tokens": [
                        "if",
                        " Al",
                        "-",
                        "Arab",
                        " al",
                        "-",
                        "Q",
                        "ath",
                        "afi",
                        ",",
                        " where",
                        " three",
                        " of",
                        " Colonel",
                        " Gaddafi",
                        "'s",
                        " grandchildren"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "2 14 Lubbock +4 20 Denver +2 23 Portland +2 7 Birmingham",
                    "tokens": [
                        "2",
                        " 14",
                        " L",
                        "ubb",
                        "ock",
                        " +",
                        "4",
                        " 20",
                        " Denver",
                        " +",
                        "2",
                        " 23",
                        " Portland",
                        " +",
                        "2",
                        " 7",
                        " Birmingham"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " has to be levied, it should be levied on the Central and state governments and the",
                    "tokens": [
                        " has",
                        " to",
                        " be",
                        " levied",
                        ",",
                        " it",
                        " should",
                        " be",
                        " levied",
                        " on",
                        " the",
                        " Central",
                        " and",
                        " state",
                        " governments",
                        " and",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 15.29079723358154
        },
        {
            "feature_index": 7613,
            "gpt_predictions": [
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "references to the Green Bay Packers football team",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "\u010a\u010aSchwartz and Green Bay Packers coach Mike McCarthy have the longest-running",
                    "max_token": " Packers",
                    "tokens": [
                        "\u010a",
                        "\u010a",
                        "Sch",
                        "w",
                        "artz",
                        " and",
                        " Green",
                        " Bay",
                        " Packers",
                        " coach",
                        " Mike",
                        " McCarthy",
                        " have",
                        " the",
                        " longest",
                        "-",
                        "running"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        40.33537292480469,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " working a preseason game between the Green Bay Packers and San Diego Chargers.\u010a\u010aW",
                    "max_token": " Packers",
                    "tokens": [
                        " working",
                        " a",
                        " preseason",
                        " game",
                        " between",
                        " the",
                        " Green",
                        " Bay",
                        " Packers",
                        " and",
                        " San",
                        " Diego",
                        " Chargers",
                        ".",
                        "\u010a",
                        "\u010a",
                        "W"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        1.494059562683105,
                        0,
                        41.06071472167969,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " technology in 2014. He first joined the Packers as a video intern in 2004 before working",
                    "max_token": " Packers",
                    "tokens": [
                        " technology",
                        " in",
                        " 2014",
                        ".",
                        " He",
                        " first",
                        " joined",
                        " the",
                        " Packers",
                        " as",
                        " a",
                        " video",
                        " intern",
                        " in",
                        " 2004",
                        " before",
                        " working"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        42.65678787231445,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "running contracts in the NFC North. Minnesota Vikings coach Leslie Frazier is due for an extension",
                    "max_token": " Vikings",
                    "tokens": [
                        "running",
                        " contracts",
                        " in",
                        " the",
                        " NFC",
                        " North",
                        ".",
                        " Minnesota",
                        " Vikings",
                        " coach",
                        " Leslie",
                        " Frazier",
                        " is",
                        " due",
                        " for",
                        " an",
                        " extension"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        5.011136531829834,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " build a<|endoftext|> ratings from the Green Bay Packers\u00e2\u0122\u013b 27-13 win over the",
                    "max_token": " Packers",
                    "tokens": [
                        " build",
                        " a",
                        "<|endoftext|>",
                        " ratings",
                        " from",
                        " the",
                        " Green",
                        " Bay",
                        " Packers",
                        "\u00e2\u0122",
                        "\u013b",
                        " 27",
                        "-",
                        "13",
                        " win",
                        " over",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        39.56654739379883,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " most rich countries. But \u00e2\u0122\u013epremiumisation\u00e2\u0122\u013f is working. Though still",
                    "tokens": [
                        " most",
                        " rich",
                        " countries",
                        ".",
                        " But",
                        " \u00e2\u0122",
                        "\u013e",
                        "prem",
                        "ium",
                        "isation",
                        "\u00e2\u0122",
                        "\u013f",
                        " is",
                        " working",
                        ".",
                        " Though",
                        " still"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " outfit in the top 20. The Denver Nuggets reportedly spend \u00c2\u00a32,939,",
                    "tokens": [
                        " outfit",
                        " in",
                        " the",
                        " top",
                        " 20",
                        ".",
                        " The",
                        " Denver",
                        " Nuggets",
                        " reportedly",
                        " spend",
                        " \u00c2\u00a3",
                        "2",
                        ",",
                        "9",
                        "39",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "if Al-Arab al-Qathafi, where three of Colonel Gaddafi's grandchildren",
                    "tokens": [
                        "if",
                        " Al",
                        "-",
                        "Arab",
                        " al",
                        "-",
                        "Q",
                        "ath",
                        "afi",
                        ",",
                        " where",
                        " three",
                        " of",
                        " Colonel",
                        " Gaddafi",
                        "'s",
                        " grandchildren"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "2 14 Lubbock +4 20 Denver +2 23 Portland +2 7 Birmingham",
                    "tokens": [
                        "2",
                        " 14",
                        " L",
                        "ubb",
                        "ock",
                        " +",
                        "4",
                        " 20",
                        " Denver",
                        " +",
                        "2",
                        " 23",
                        " Portland",
                        " +",
                        "2",
                        " 7",
                        " Birmingham"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " has to be levied, it should be levied on the Central and state governments and the",
                    "tokens": [
                        " has",
                        " to",
                        " be",
                        " levied",
                        ",",
                        " it",
                        " should",
                        " be",
                        " levied",
                        " on",
                        " the",
                        " Central",
                        " and",
                        " state",
                        " governments",
                        " and",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 45.72155380249023
        },
        {
            "feature_index": 6589,
            "gpt_predictions": [
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "words related to the process of discussing and deciding on a course of action, especially in a group setting",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " can always look back on thrilling victories and agonizing defeats.\u010a\u010aThe ability to",
                    "max_token": " agon",
                    "tokens": [
                        " can",
                        " always",
                        " look",
                        " back",
                        " on",
                        " thrilling",
                        " victories",
                        " and",
                        " agon",
                        "izing",
                        " defeats",
                        ".",
                        "\u010a",
                        "\u010a",
                        "The",
                        " ability",
                        " to"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        10.0361909866333,
                        3.474640607833862,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 7,
                    "sentence_string": "of-state operation. The panel deliberated for less than two days.\u010a\u010a",
                    "max_token": " deliber",
                    "tokens": [
                        "of",
                        "-",
                        "state",
                        " operation",
                        ".",
                        " The",
                        " panel",
                        " deliber",
                        "ated",
                        " for",
                        " less",
                        " than",
                        " two",
                        " days",
                        ".",
                        "\u010a",
                        "\u010a"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        32.43509674072266,
                        7.31651782989502,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": ". There\u00e2\u0122\u013bs a lot of strategizing, second-guessing, and",
                    "max_token": " strateg",
                    "tokens": [
                        ".",
                        " There",
                        "\u00e2\u0122",
                        "\u013b",
                        "s",
                        " a",
                        " lot",
                        " of",
                        " strateg",
                        "izing",
                        ",",
                        " second",
                        "-",
                        "gu",
                        "essing",
                        ",",
                        " and"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        6.927855968475342,
                        0.6521514058113098,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": ", would later make several goodwill visits to Brookings during the 1960s, and was even",
                    "max_token": " Brookings",
                    "tokens": [
                        ",",
                        " would",
                        " later",
                        " make",
                        " several",
                        " goodwill",
                        " visits",
                        " to",
                        " Brookings",
                        " during",
                        " the",
                        " 1960",
                        "s",
                        ",",
                        " and",
                        " was",
                        " even"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        3.982396602630615,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " U.S. influence in global economic deliberations. Over the past 21 months, European",
                    "max_token": " deliberations",
                    "tokens": [
                        " U",
                        ".",
                        "S",
                        ".",
                        " influence",
                        " in",
                        " global",
                        " economic",
                        " deliberations",
                        ".",
                        " Over",
                        " the",
                        " past",
                        " 21",
                        " months",
                        ",",
                        " European"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        16.32081031799316,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " most rich countries. But \u00e2\u0122\u013epremiumisation\u00e2\u0122\u013f is working. Though still",
                    "tokens": [
                        " most",
                        " rich",
                        " countries",
                        ".",
                        " But",
                        " \u00e2\u0122",
                        "\u013e",
                        "prem",
                        "ium",
                        "isation",
                        "\u00e2\u0122",
                        "\u013f",
                        " is",
                        " working",
                        ".",
                        " Though",
                        " still"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " outfit in the top 20. The Denver Nuggets reportedly spend \u00c2\u00a32,939,",
                    "tokens": [
                        " outfit",
                        " in",
                        " the",
                        " top",
                        " 20",
                        ".",
                        " The",
                        " Denver",
                        " Nuggets",
                        " reportedly",
                        " spend",
                        " \u00c2\u00a3",
                        "2",
                        ",",
                        "9",
                        "39",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "if Al-Arab al-Qathafi, where three of Colonel Gaddafi's grandchildren",
                    "tokens": [
                        "if",
                        " Al",
                        "-",
                        "Arab",
                        " al",
                        "-",
                        "Q",
                        "ath",
                        "afi",
                        ",",
                        " where",
                        " three",
                        " of",
                        " Colonel",
                        " Gaddafi",
                        "'s",
                        " grandchildren"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "2 14 Lubbock +4 20 Denver +2 23 Portland +2 7 Birmingham",
                    "tokens": [
                        "2",
                        " 14",
                        " L",
                        "ubb",
                        "ock",
                        " +",
                        "4",
                        " 20",
                        " Denver",
                        " +",
                        "2",
                        " 23",
                        " Portland",
                        " +",
                        "2",
                        " 7",
                        " Birmingham"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " has to be levied, it should be levied on the Central and state governments and the",
                    "tokens": [
                        " has",
                        " to",
                        " be",
                        " levied",
                        ",",
                        " it",
                        " should",
                        " be",
                        " levied",
                        " on",
                        " the",
                        " Central",
                        " and",
                        " state",
                        " governments",
                        " and",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 35.66057968139648
        },
        {
            "feature_index": 22527,
            "gpt_predictions": [
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    1.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "terms related to illicit activities or entities",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "\u010a\u010aCultural Caviar Never Speak Ill of a Hate-Crime Hoaxer",
                    "max_token": " Ill",
                    "tokens": [
                        "\u010a",
                        "\u010a",
                        "C",
                        "ultural",
                        " Cav",
                        "iar",
                        " Never",
                        " Speak",
                        " Ill",
                        " of",
                        " a",
                        " Hate",
                        "-",
                        "Crime",
                        " Ho",
                        "ax",
                        "er"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        38.69753265380859,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "\u013bs father in Stonington, Ill.\u010a\u010a[In donated suit and",
                    "max_token": " Ill",
                    "tokens": [
                        "\u013b",
                        "s",
                        " father",
                        " in",
                        " St",
                        "on",
                        "ington",
                        ",",
                        " Ill",
                        ".",
                        "\u010a",
                        "\u010a",
                        "[",
                        "In",
                        " donated",
                        " suit",
                        " and"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        39.79066848754883,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " who worked for the CIA\u00e2\u0122\u013bs Illicit Finance Group: Jablonski",
                    "max_token": " Ill",
                    "tokens": [
                        " who",
                        " worked",
                        " for",
                        " the",
                        " CIA",
                        "\u00e2\u0122",
                        "\u013b",
                        "s",
                        " Ill",
                        "icit",
                        " Finance",
                        " Group",
                        ":",
                        " J",
                        "abl",
                        "ons",
                        "ki"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        40.35406494140625,
                        2.163353681564331,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "ahil doon sa letter complaint. Nilabas po ang letter complaint upang",
                    "max_token": " Nil",
                    "tokens": [
                        "ah",
                        "il",
                        " do",
                        "on",
                        " sa",
                        " letter",
                        " complaint",
                        ".",
                        " Nil",
                        "ab",
                        "as",
                        " po",
                        " ang",
                        " letter",
                        " complaint",
                        " up",
                        "ang"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        5.388731956481934,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " Woolf in her 1926 essay On Being Ill, in which she bemoans literature",
                    "max_token": " Ill",
                    "tokens": [
                        " Wool",
                        "f",
                        " in",
                        " her",
                        " 1926",
                        " essay",
                        " On",
                        " Being",
                        " Ill",
                        ",",
                        " in",
                        " which",
                        " she",
                        " be",
                        "mo",
                        "ans",
                        " literature"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        37.43641662597656,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " most rich countries. But \u00e2\u0122\u013epremiumisation\u00e2\u0122\u013f is working. Though still",
                    "tokens": [
                        " most",
                        " rich",
                        " countries",
                        ".",
                        " But",
                        " \u00e2\u0122",
                        "\u013e",
                        "prem",
                        "ium",
                        "isation",
                        "\u00e2\u0122",
                        "\u013f",
                        " is",
                        " working",
                        ".",
                        " Though",
                        " still"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " outfit in the top 20. The Denver Nuggets reportedly spend \u00c2\u00a32,939,",
                    "tokens": [
                        " outfit",
                        " in",
                        " the",
                        " top",
                        " 20",
                        ".",
                        " The",
                        " Denver",
                        " Nuggets",
                        " reportedly",
                        " spend",
                        " \u00c2\u00a3",
                        "2",
                        ",",
                        "9",
                        "39",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "if Al-Arab al-Qathafi, where three of Colonel Gaddafi's grandchildren",
                    "tokens": [
                        "if",
                        " Al",
                        "-",
                        "Arab",
                        " al",
                        "-",
                        "Q",
                        "ath",
                        "afi",
                        ",",
                        " where",
                        " three",
                        " of",
                        " Colonel",
                        " Gaddafi",
                        "'s",
                        " grandchildren"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "2 14 Lubbock +4 20 Denver +2 23 Portland +2 7 Birmingham",
                    "tokens": [
                        "2",
                        " 14",
                        " L",
                        "ubb",
                        "ock",
                        " +",
                        "4",
                        " 20",
                        " Denver",
                        " +",
                        "2",
                        " 23",
                        " Portland",
                        " +",
                        "2",
                        " 7",
                        " Birmingham"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " has to be levied, it should be levied on the Central and state governments and the",
                    "tokens": [
                        " has",
                        " to",
                        " be",
                        " levied",
                        ",",
                        " it",
                        " should",
                        " be",
                        " levied",
                        " on",
                        " the",
                        " Central",
                        " and",
                        " state",
                        " governments",
                        " and",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 41.94285202026367
        },
        {
            "feature_index": 4668,
            "gpt_predictions": [
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "names and terms related to titanium and medical conditions",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " the concepts of \"race\" and \"racism\" are modern inventions. They arose",
                    "max_token": "rac",
                    "tokens": [
                        " the",
                        " concepts",
                        " of",
                        " \"",
                        "race",
                        "\"",
                        " and",
                        " \"",
                        "rac",
                        "ism",
                        "\"",
                        " are",
                        " modern",
                        " inventions",
                        ".",
                        " They",
                        " arose"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        8.553641319274902,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " r\u00c3\u00a2nd, acest gard nu este ermetic, exist\u00c4\u0125 palest",
                    "max_token": " este",
                    "tokens": [
                        " r",
                        "\u00c3\u00a2",
                        "nd",
                        ",",
                        " ac",
                        "est",
                        " gard",
                        " nu",
                        " este",
                        " er",
                        "metic",
                        ",",
                        " exist",
                        "\u00c4",
                        "\u0125",
                        " pal",
                        "est"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        6.090804576873779,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": ".\u010a\u010aThe pattern of the woven stainless steel appears to be exactly the same as",
                    "max_token": " stainless",
                    "tokens": [
                        ".",
                        "\u010a",
                        "\u010a",
                        "The",
                        " pattern",
                        " of",
                        " the",
                        " woven",
                        " stainless",
                        " steel",
                        " appears",
                        " to",
                        " be",
                        " exactly",
                        " the",
                        " same",
                        " as"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        5.956213474273682,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " and intelligent conversation by using words like 'racism,'\" said long-time Chinatown activist",
                    "max_token": "rac",
                    "tokens": [
                        " and",
                        " intelligent",
                        " conversation",
                        " by",
                        " using",
                        " words",
                        " like",
                        " '",
                        "rac",
                        "ism",
                        ",'\"",
                        " said",
                        " long",
                        "-",
                        "time",
                        " Chinatown",
                        " activist"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        9.385993957519531,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " on the sideline after post-play extracurricular activity between running back Joe McK",
                    "max_token": "rac",
                    "tokens": [
                        " on",
                        " the",
                        " sideline",
                        " after",
                        " post",
                        "-",
                        "play",
                        " ext",
                        "rac",
                        "ur",
                        "ricular",
                        " activity",
                        " between",
                        " running",
                        " back",
                        " Joe",
                        " McK"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        7.099631309509277,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " most rich countries. But \u00e2\u0122\u013epremiumisation\u00e2\u0122\u013f is working. Though still",
                    "tokens": [
                        " most",
                        " rich",
                        " countries",
                        ".",
                        " But",
                        " \u00e2\u0122",
                        "\u013e",
                        "prem",
                        "ium",
                        "isation",
                        "\u00e2\u0122",
                        "\u013f",
                        " is",
                        " working",
                        ".",
                        " Though",
                        " still"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " outfit in the top 20. The Denver Nuggets reportedly spend \u00c2\u00a32,939,",
                    "tokens": [
                        " outfit",
                        " in",
                        " the",
                        " top",
                        " 20",
                        ".",
                        " The",
                        " Denver",
                        " Nuggets",
                        " reportedly",
                        " spend",
                        " \u00c2\u00a3",
                        "2",
                        ",",
                        "9",
                        "39",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "if Al-Arab al-Qathafi, where three of Colonel Gaddafi's grandchildren",
                    "tokens": [
                        "if",
                        " Al",
                        "-",
                        "Arab",
                        " al",
                        "-",
                        "Q",
                        "ath",
                        "afi",
                        ",",
                        " where",
                        " three",
                        " of",
                        " Colonel",
                        " Gaddafi",
                        "'s",
                        " grandchildren"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "2 14 Lubbock +4 20 Denver +2 23 Portland +2 7 Birmingham",
                    "tokens": [
                        "2",
                        " 14",
                        " L",
                        "ubb",
                        "ock",
                        " +",
                        "4",
                        " 20",
                        " Denver",
                        " +",
                        "2",
                        " 23",
                        " Portland",
                        " +",
                        "2",
                        " 7",
                        " Birmingham"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " has to be levied, it should be levied on the Central and state governments and the",
                    "tokens": [
                        " has",
                        " to",
                        " be",
                        " levied",
                        ",",
                        " it",
                        " should",
                        " be",
                        " levied",
                        " on",
                        " the",
                        " Central",
                        " and",
                        " state",
                        " governments",
                        " and",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 9.534293174743652
        },
        {
            "feature_index": 23740,
            "gpt_predictions": [
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    1.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "references to specific weeks in sports contexts",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " to trend higher for May.\u010a\u010aWeek on week volume has been growing for the",
                    "max_token": "Week",
                    "tokens": [
                        " to",
                        " trend",
                        " higher",
                        " for",
                        " May",
                        ".",
                        "\u010a",
                        "\u010a",
                        "Week",
                        " on",
                        " week",
                        " volume",
                        " has",
                        " been",
                        " growing",
                        " for",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        18.23539924621582,
                        0,
                        0.6559840440750122,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " statistical \"power ranking\" going into imaginary Week 18:\u010a\u010aWe see Cleveland's",
                    "max_token": " Week",
                    "tokens": [
                        " statistical",
                        " \"",
                        "power",
                        " ranking",
                        "\"",
                        " going",
                        " into",
                        " imaginary",
                        " Week",
                        " 18",
                        ":",
                        "\u010a",
                        "\u010a",
                        "We",
                        " see",
                        " Cleveland",
                        "'s"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        30.43268585205078,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "100.org.\u010a\u010aBike Month continues\u010a\u010aIn other beer news,",
                    "max_token": " Month",
                    "tokens": [
                        "100",
                        ".",
                        "org",
                        ".",
                        "\u010a",
                        "\u010a",
                        "B",
                        "ike",
                        " Month",
                        " continues",
                        "\u010a",
                        "\u010a",
                        "In",
                        " other",
                        " beer",
                        " news",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        6.972320556640625,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " plus-four takeaway/giveaway in Week 1 against Houston and won by 22 points",
                    "max_token": " Week",
                    "tokens": [
                        " plus",
                        "-",
                        "four",
                        " takeaway",
                        "/",
                        "give",
                        "away",
                        " in",
                        " Week",
                        " 1",
                        " against",
                        " Houston",
                        " and",
                        " won",
                        " by",
                        " 22",
                        " points"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        32.97760391235352,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " was cut short by an ACL tear in Week 14. He then spent the entire 2011",
                    "max_token": " Week",
                    "tokens": [
                        " was",
                        " cut",
                        " short",
                        " by",
                        " an",
                        " ACL",
                        " tear",
                        " in",
                        " Week",
                        " 14",
                        ".",
                        " He",
                        " then",
                        " spent",
                        " the",
                        " entire",
                        " 2011"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        32.55839538574219,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " most rich countries. But \u00e2\u0122\u013epremiumisation\u00e2\u0122\u013f is working. Though still",
                    "tokens": [
                        " most",
                        " rich",
                        " countries",
                        ".",
                        " But",
                        " \u00e2\u0122",
                        "\u013e",
                        "prem",
                        "ium",
                        "isation",
                        "\u00e2\u0122",
                        "\u013f",
                        " is",
                        " working",
                        ".",
                        " Though",
                        " still"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " outfit in the top 20. The Denver Nuggets reportedly spend \u00c2\u00a32,939,",
                    "tokens": [
                        " outfit",
                        " in",
                        " the",
                        " top",
                        " 20",
                        ".",
                        " The",
                        " Denver",
                        " Nuggets",
                        " reportedly",
                        " spend",
                        " \u00c2\u00a3",
                        "2",
                        ",",
                        "9",
                        "39",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "if Al-Arab al-Qathafi, where three of Colonel Gaddafi's grandchildren",
                    "tokens": [
                        "if",
                        " Al",
                        "-",
                        "Arab",
                        " al",
                        "-",
                        "Q",
                        "ath",
                        "afi",
                        ",",
                        " where",
                        " three",
                        " of",
                        " Colonel",
                        " Gaddafi",
                        "'s",
                        " grandchildren"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "2 14 Lubbock +4 20 Denver +2 23 Portland +2 7 Birmingham",
                    "tokens": [
                        "2",
                        " 14",
                        " L",
                        "ubb",
                        "ock",
                        " +",
                        "4",
                        " 20",
                        " Denver",
                        " +",
                        "2",
                        " 23",
                        " Portland",
                        " +",
                        "2",
                        " 7",
                        " Birmingham"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " has to be levied, it should be levied on the Central and state governments and the",
                    "tokens": [
                        " has",
                        " to",
                        " be",
                        " levied",
                        ",",
                        " it",
                        " should",
                        " be",
                        " levied",
                        " on",
                        " the",
                        " Central",
                        " and",
                        " state",
                        " governments",
                        " and",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 34.17937088012695
        },
        {
            "feature_index": 1740,
            "gpt_predictions": [
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "references to screen resolutions, specifically focusing on the 1080p resolution",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " points on the extreme preset using the GTX 1070 card.\u010a\u010aThe Core i7",
                    "max_token": " 1070",
                    "tokens": [
                        " points",
                        " on",
                        " the",
                        " extreme",
                        " preset",
                        " using",
                        " the",
                        " GTX",
                        " 1070",
                        " card",
                        ".",
                        "\u010a",
                        "\u010a",
                        "The",
                        " Core",
                        " i",
                        "7"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        13.16747856140137,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "rapixels or lasers? QHD or 1080p? It's time to choose between",
                    "max_token": " 1080",
                    "tokens": [
                        "rap",
                        "ixels",
                        " or",
                        " lasers",
                        "?",
                        " Q",
                        "HD",
                        " or",
                        " 1080",
                        "p",
                        "?",
                        " It",
                        "'s",
                        " time",
                        " to",
                        " choose",
                        " between"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        41.12287139892578,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " ARM processor and went with the Acer C720 4GB. Most of my laptop time",
                    "max_token": "720",
                    "tokens": [
                        " ARM",
                        " processor",
                        " and",
                        " went",
                        " with",
                        " the",
                        " Acer",
                        " C",
                        "720",
                        " 4",
                        "GB",
                        ".",
                        " Most",
                        " of",
                        " my",
                        " laptop",
                        " time"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        10.65873622894287,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": ", so some issues remain. GeForce GTX 780 Ti cards in SLI have no screen output",
                    "max_token": " 780",
                    "tokens": [
                        ",",
                        " so",
                        " some",
                        " issues",
                        " remain",
                        ".",
                        " GeForce",
                        " GTX",
                        " 780",
                        " Ti",
                        " cards",
                        " in",
                        " SLI",
                        " have",
                        " no",
                        " screen",
                        " output"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        4.528342723846436,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " 19, 2016 @ 12:42pm 1440 x 900 31 Unique Visitors 0 Current Favor",
                    "max_token": " 1440",
                    "tokens": [
                        " 19",
                        ",",
                        " 2016",
                        " @",
                        " 12",
                        ":",
                        "42",
                        "pm",
                        " 1440",
                        " x",
                        " 900",
                        " 31",
                        " Unique",
                        " Visitors",
                        " 0",
                        " Current",
                        " Favor"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        15.47753429412842,
                        0,
                        3.673592567443848,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " most rich countries. But \u00e2\u0122\u013epremiumisation\u00e2\u0122\u013f is working. Though still",
                    "tokens": [
                        " most",
                        " rich",
                        " countries",
                        ".",
                        " But",
                        " \u00e2\u0122",
                        "\u013e",
                        "prem",
                        "ium",
                        "isation",
                        "\u00e2\u0122",
                        "\u013f",
                        " is",
                        " working",
                        ".",
                        " Though",
                        " still"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " outfit in the top 20. The Denver Nuggets reportedly spend \u00c2\u00a32,939,",
                    "tokens": [
                        " outfit",
                        " in",
                        " the",
                        " top",
                        " 20",
                        ".",
                        " The",
                        " Denver",
                        " Nuggets",
                        " reportedly",
                        " spend",
                        " \u00c2\u00a3",
                        "2",
                        ",",
                        "9",
                        "39",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "if Al-Arab al-Qathafi, where three of Colonel Gaddafi's grandchildren",
                    "tokens": [
                        "if",
                        " Al",
                        "-",
                        "Arab",
                        " al",
                        "-",
                        "Q",
                        "ath",
                        "afi",
                        ",",
                        " where",
                        " three",
                        " of",
                        " Colonel",
                        " Gaddafi",
                        "'s",
                        " grandchildren"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "2 14 Lubbock +4 20 Denver +2 23 Portland +2 7 Birmingham",
                    "tokens": [
                        "2",
                        " 14",
                        " L",
                        "ubb",
                        "ock",
                        " +",
                        "4",
                        " 20",
                        " Denver",
                        " +",
                        "2",
                        " 23",
                        " Portland",
                        " +",
                        "2",
                        " 7",
                        " Birmingham"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " has to be levied, it should be levied on the Central and state governments and the",
                    "tokens": [
                        " has",
                        " to",
                        " be",
                        " levied",
                        ",",
                        " it",
                        " should",
                        " be",
                        " levied",
                        " on",
                        " the",
                        " Central",
                        " and",
                        " state",
                        " governments",
                        " and",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 43.85476684570312
        },
        {
            "feature_index": 10581,
            "gpt_predictions": [
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "references to wealthy individuals or families",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 11,
                    "sentence_string": " cut taxes for everyone, but especially the richest of the rich. If enacted, the",
                    "max_token": " rich",
                    "tokens": [
                        " cut",
                        " taxes",
                        " for",
                        " everyone",
                        ",",
                        " but",
                        " especially",
                        " the",
                        " richest",
                        " of",
                        " the",
                        " rich",
                        ".",
                        " If",
                        " enacted",
                        ",",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        13.00230884552002,
                        0,
                        0,
                        33.13735198974609,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "ma is the ancient city known for its rich mosaics in the southeastern province of Gaz",
                    "max_token": " rich",
                    "tokens": [
                        "ma",
                        " is",
                        " the",
                        " ancient",
                        " city",
                        " known",
                        " for",
                        " its",
                        " rich",
                        " mosa",
                        "ics",
                        " in",
                        " the",
                        " southeastern",
                        " province",
                        " of",
                        " Gaz"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        33.55019378662109,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "They taught and performed in the homes of rich people. Aristocrats and the upper middle",
                    "max_token": " rich",
                    "tokens": [
                        "They",
                        " taught",
                        " and",
                        " performed",
                        " in",
                        " the",
                        " homes",
                        " of",
                        " rich",
                        " people",
                        ".",
                        " Arist",
                        "ocrats",
                        " and",
                        " the",
                        " upper",
                        " middle"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        36.1803092956543,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": ". \u00e2\u0122\u013eThe city\u00e2\u0122\u013bs rich musical heritage is famous the world over,",
                    "max_token": " rich",
                    "tokens": [
                        ".",
                        " \u00e2\u0122",
                        "\u013e",
                        "The",
                        " city",
                        "\u00e2\u0122",
                        "\u013b",
                        "s",
                        " rich",
                        " musical",
                        " heritage",
                        " is",
                        " famous",
                        " the",
                        " world",
                        " over",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        36.6363525390625,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " the Constitution as an instrument crafted by the rich to protect their selfish interests (J.",
                    "max_token": " rich",
                    "tokens": [
                        " the",
                        " Constitution",
                        " as",
                        " an",
                        " instrument",
                        " crafted",
                        " by",
                        " the",
                        " rich",
                        " to",
                        " protect",
                        " their",
                        " selfish",
                        " interests",
                        " (",
                        "J",
                        "."
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        37.06306457519531,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " most rich countries. But \u00e2\u0122\u013epremiumisation\u00e2\u0122\u013f is working. Though still",
                    "tokens": [
                        " most",
                        " rich",
                        " countries",
                        ".",
                        " But",
                        " \u00e2\u0122",
                        "\u013e",
                        "prem",
                        "ium",
                        "isation",
                        "\u00e2\u0122",
                        "\u013f",
                        " is",
                        " working",
                        ".",
                        " Though",
                        " still"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " outfit in the top 20. The Denver Nuggets reportedly spend \u00c2\u00a32,939,",
                    "tokens": [
                        " outfit",
                        " in",
                        " the",
                        " top",
                        " 20",
                        ".",
                        " The",
                        " Denver",
                        " Nuggets",
                        " reportedly",
                        " spend",
                        " \u00c2\u00a3",
                        "2",
                        ",",
                        "9",
                        "39",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "if Al-Arab al-Qathafi, where three of Colonel Gaddafi's grandchildren",
                    "tokens": [
                        "if",
                        " Al",
                        "-",
                        "Arab",
                        " al",
                        "-",
                        "Q",
                        "ath",
                        "afi",
                        ",",
                        " where",
                        " three",
                        " of",
                        " Colonel",
                        " Gaddafi",
                        "'s",
                        " grandchildren"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "2 14 Lubbock +4 20 Denver +2 23 Portland +2 7 Birmingham",
                    "tokens": [
                        "2",
                        " 14",
                        " L",
                        "ubb",
                        "ock",
                        " +",
                        "4",
                        " 20",
                        " Denver",
                        " +",
                        "2",
                        " 23",
                        " Portland",
                        " +",
                        "2",
                        " 7",
                        " Birmingham"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " has to be levied, it should be levied on the Central and state governments and the",
                    "tokens": [
                        " has",
                        " to",
                        " be",
                        " levied",
                        ",",
                        " it",
                        " should",
                        " be",
                        " levied",
                        " on",
                        " the",
                        " Central",
                        " and",
                        " state",
                        " governments",
                        " and",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 37.36577224731445
        },
        {
            "feature_index": 14290,
            "gpt_predictions": [
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    0,
                    1.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    1.0
                ],
                [
                    0,
                    1.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "characters such as symbols and specific names",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " times when this movie does a Lion King/Finding Nemo impersonation. So I",
                    "max_token": "/",
                    "tokens": [
                        " times",
                        " when",
                        " this",
                        " movie",
                        " does",
                        " a",
                        " Lion",
                        " King",
                        "/",
                        "Finding",
                        " Nem",
                        "o",
                        " imperson",
                        "ation",
                        ".",
                        " So",
                        " I"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        3.602503299713135,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0.1711462140083313,
                        0,
                        0.4004577398300171
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " was right. https://t.co/ZzZxTpr8r",
                    "max_token": "/",
                    "tokens": [
                        " was",
                        " right",
                        ".",
                        " https",
                        "://",
                        "t",
                        ".",
                        "co",
                        "/",
                        "Z",
                        "z",
                        "Z",
                        "x",
                        "T",
                        "pr",
                        "8",
                        "r"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        2.456857919692993,
                        0,
                        3.019504070281982,
                        0,
                        5.188126087188721,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " considered to be what\u00e2\u0122\u013bs known as the Karman line which marks the edge",
                    "max_token": " as",
                    "tokens": [
                        " considered",
                        " to",
                        " be",
                        " what",
                        "\u00e2\u0122",
                        "\u013b",
                        "s",
                        " known",
                        " as",
                        " the",
                        " Kar",
                        "man",
                        " line",
                        " which",
                        " marks",
                        " the",
                        " edge"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        1.605423212051392,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " Brexit negotiator Guy Verhofstadt Getty 6/8 Britain's Prime Minister Theresa May Getty",
                    "max_token": "/",
                    "tokens": [
                        " Brexit",
                        " negotiator",
                        " Guy",
                        " Ver",
                        "hof",
                        "stadt",
                        " Getty",
                        " 6",
                        "/",
                        "8",
                        " Britain",
                        "'s",
                        " Prime",
                        " Minister",
                        " Theresa",
                        " May",
                        " Getty"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0.8918322920799255,
                        5.766378402709961,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " in numerous years at that position \u00c3\u00a2\u00c2\u0122\u00c2\u0135 quality starts.\u010a\u010a",
                    "max_token": "\u00c2",
                    "tokens": [
                        " in",
                        " numerous",
                        " years",
                        " at",
                        " that",
                        " position",
                        " \u00c3",
                        "\u00a2",
                        "\u00c2",
                        "\u0122",
                        "\u00c2",
                        "\u0135",
                        " quality",
                        " starts",
                        ".",
                        "\u010a",
                        "\u010a"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0.6146529316902161,
                        3.014097452163696,
                        5.642220497131348,
                        4.703259468078613,
                        4.751832008361816,
                        5.266077041625977,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " most rich countries. But \u00e2\u0122\u013epremiumisation\u00e2\u0122\u013f is working. Though still",
                    "tokens": [
                        " most",
                        " rich",
                        " countries",
                        ".",
                        " But",
                        " \u00e2\u0122",
                        "\u013e",
                        "prem",
                        "ium",
                        "isation",
                        "\u00e2\u0122",
                        "\u013f",
                        " is",
                        " working",
                        ".",
                        " Though",
                        " still"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " outfit in the top 20. The Denver Nuggets reportedly spend \u00c2\u00a32,939,",
                    "tokens": [
                        " outfit",
                        " in",
                        " the",
                        " top",
                        " 20",
                        ".",
                        " The",
                        " Denver",
                        " Nuggets",
                        " reportedly",
                        " spend",
                        " \u00c2\u00a3",
                        "2",
                        ",",
                        "9",
                        "39",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "if Al-Arab al-Qathafi, where three of Colonel Gaddafi's grandchildren",
                    "tokens": [
                        "if",
                        " Al",
                        "-",
                        "Arab",
                        " al",
                        "-",
                        "Q",
                        "ath",
                        "afi",
                        ",",
                        " where",
                        " three",
                        " of",
                        " Colonel",
                        " Gaddafi",
                        "'s",
                        " grandchildren"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "2 14 Lubbock +4 20 Denver +2 23 Portland +2 7 Birmingham",
                    "tokens": [
                        "2",
                        " 14",
                        " L",
                        "ubb",
                        "ock",
                        " +",
                        "4",
                        " 20",
                        " Denver",
                        " +",
                        "2",
                        " 23",
                        " Portland",
                        " +",
                        "2",
                        " 7",
                        " Birmingham"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " has to be levied, it should be levied on the Central and state governments and the",
                    "tokens": [
                        " has",
                        " to",
                        " be",
                        " levied",
                        ",",
                        " it",
                        " should",
                        " be",
                        " levied",
                        " on",
                        " the",
                        " Central",
                        " and",
                        " state",
                        " governments",
                        " and",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 6.435142993927002
        },
        {
            "feature_index": 12845,
            "gpt_predictions": [
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "words with repeated letters like \"umm\" and \"Cummings\"",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " scene. It was the legendary photographer Kevin Cummins who captured James, Sean, R",
                    "max_token": " Cumm",
                    "tokens": [
                        " scene",
                        ".",
                        " It",
                        " was",
                        " the",
                        " legendary",
                        " photographer",
                        " Kevin",
                        " Cumm",
                        "ins",
                        " who",
                        " captured",
                        " James",
                        ",",
                        " Sean",
                        ",",
                        " R"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        5.131272792816162,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " capitalized) OpenBizaar (umm)\u010a\u010aCORRECT USAGE\u010a",
                    "max_token": "umm",
                    "tokens": [
                        " capital",
                        "ized",
                        ")",
                        " Open",
                        "B",
                        "iza",
                        "ar",
                        " (",
                        "umm",
                        ")",
                        "\u010a",
                        "\u010a",
                        "COR",
                        "RECT",
                        " US",
                        "AGE",
                        "\u010a"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        46.1492805480957,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " that hungover than having your ears pummelled and body shaken by one of the",
                    "max_token": "umm",
                    "tokens": [
                        " that",
                        " hung",
                        "over",
                        " than",
                        " having",
                        " your",
                        " ears",
                        " p",
                        "umm",
                        "elled",
                        " and",
                        " body",
                        " shaken",
                        " by",
                        " one",
                        " of",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        47.41109085083008,
                        1.455972194671631,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "us couldn\u00e2\u0122\u013bt hate. Katie Cummins Recently, I was scrolling through Twitter",
                    "max_token": " Cumm",
                    "tokens": [
                        "us",
                        " couldn",
                        "\u00e2\u0122",
                        "\u013b",
                        "t",
                        " hate",
                        ".",
                        " Katie",
                        " Cumm",
                        "ins",
                        " Recently",
                        ",",
                        " I",
                        " was",
                        " scrolling",
                        " through",
                        " Twitter"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        5.762070178985596,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " on the Oversight panel.\u010a\u010aCummings has accused Issa of mucking around",
                    "max_token": "umm",
                    "tokens": [
                        " on",
                        " the",
                        " Oversight",
                        " panel",
                        ".",
                        "\u010a",
                        "\u010a",
                        "C",
                        "umm",
                        "ings",
                        " has",
                        " accused",
                        " Issa",
                        " of",
                        " m",
                        "ucking",
                        " around"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        43.33551788330078,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " most rich countries. But \u00e2\u0122\u013epremiumisation\u00e2\u0122\u013f is working. Though still",
                    "tokens": [
                        " most",
                        " rich",
                        " countries",
                        ".",
                        " But",
                        " \u00e2\u0122",
                        "\u013e",
                        "prem",
                        "ium",
                        "isation",
                        "\u00e2\u0122",
                        "\u013f",
                        " is",
                        " working",
                        ".",
                        " Though",
                        " still"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " outfit in the top 20. The Denver Nuggets reportedly spend \u00c2\u00a32,939,",
                    "tokens": [
                        " outfit",
                        " in",
                        " the",
                        " top",
                        " 20",
                        ".",
                        " The",
                        " Denver",
                        " Nuggets",
                        " reportedly",
                        " spend",
                        " \u00c2\u00a3",
                        "2",
                        ",",
                        "9",
                        "39",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "if Al-Arab al-Qathafi, where three of Colonel Gaddafi's grandchildren",
                    "tokens": [
                        "if",
                        " Al",
                        "-",
                        "Arab",
                        " al",
                        "-",
                        "Q",
                        "ath",
                        "afi",
                        ",",
                        " where",
                        " three",
                        " of",
                        " Colonel",
                        " Gaddafi",
                        "'s",
                        " grandchildren"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "2 14 Lubbock +4 20 Denver +2 23 Portland +2 7 Birmingham",
                    "tokens": [
                        "2",
                        " 14",
                        " L",
                        "ubb",
                        "ock",
                        " +",
                        "4",
                        " 20",
                        " Denver",
                        " +",
                        "2",
                        " 23",
                        " Portland",
                        " +",
                        "2",
                        " 7",
                        " Birmingham"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " has to be levied, it should be levied on the Central and state governments and the",
                    "tokens": [
                        " has",
                        " to",
                        " be",
                        " levied",
                        ",",
                        " it",
                        " should",
                        " be",
                        " levied",
                        " on",
                        " the",
                        " Central",
                        " and",
                        " state",
                        " governments",
                        " and",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 48.23351669311523
        },
        {
            "feature_index": 9135,
            "gpt_predictions": [
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "words related to halving or cutting in half",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " smelly breath. This guy literally invented halitosis, a problem<|endoftext|>Guilty",
                    "max_token": " hal",
                    "tokens": [
                        " sm",
                        "elly",
                        " breath",
                        ".",
                        " This",
                        " guy",
                        " literally",
                        " invented",
                        " hal",
                        "it",
                        "osis",
                        ",",
                        " a",
                        " problem",
                        "<|endoftext|>",
                        "Gu",
                        "ilty"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        37.95607757568359,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "is have concluded that there is no real halachic problem with a woman carrying a",
                    "max_token": " hal",
                    "tokens": [
                        "is",
                        " have",
                        " concluded",
                        " that",
                        " there",
                        " is",
                        " no",
                        " real",
                        " hal",
                        "ach",
                        "ic",
                        " problem",
                        " with",
                        " a",
                        " woman",
                        " carrying",
                        " a"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        37.96692276000977,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " normal, which is in contradiction to normative halachic opinion.[9] Some models",
                    "max_token": " hal",
                    "tokens": [
                        " normal",
                        ",",
                        " which",
                        " is",
                        " in",
                        " contradiction",
                        " to",
                        " normative",
                        " hal",
                        "ach",
                        "ic",
                        " opinion",
                        ".[",
                        "9",
                        "]",
                        " Some",
                        " models"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        39.00234985351562,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " members instead approved an austerity plan aimed at halving deficits in three years.\u010a\u010a",
                    "max_token": " hal",
                    "tokens": [
                        " members",
                        " instead",
                        " approved",
                        " an",
                        " austerity",
                        " plan",
                        " aimed",
                        " at",
                        " hal",
                        "ving",
                        " deficits",
                        " in",
                        " three",
                        " years",
                        ".",
                        "\u010a",
                        "\u010a"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        38.42134857177734,
                        4.885172367095947,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " both miners and the market with block reward halving which acts as a rudimentary decay model",
                    "max_token": " hal",
                    "tokens": [
                        " both",
                        " miners",
                        " and",
                        " the",
                        " market",
                        " with",
                        " block",
                        " reward",
                        " hal",
                        "ving",
                        " which",
                        " acts",
                        " as",
                        " a",
                        " rudimentary",
                        " decay",
                        " model"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        38.98554611206055,
                        4.216987133026123,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " most rich countries. But \u00e2\u0122\u013epremiumisation\u00e2\u0122\u013f is working. Though still",
                    "tokens": [
                        " most",
                        " rich",
                        " countries",
                        ".",
                        " But",
                        " \u00e2\u0122",
                        "\u013e",
                        "prem",
                        "ium",
                        "isation",
                        "\u00e2\u0122",
                        "\u013f",
                        " is",
                        " working",
                        ".",
                        " Though",
                        " still"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " outfit in the top 20. The Denver Nuggets reportedly spend \u00c2\u00a32,939,",
                    "tokens": [
                        " outfit",
                        " in",
                        " the",
                        " top",
                        " 20",
                        ".",
                        " The",
                        " Denver",
                        " Nuggets",
                        " reportedly",
                        " spend",
                        " \u00c2\u00a3",
                        "2",
                        ",",
                        "9",
                        "39",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "if Al-Arab al-Qathafi, where three of Colonel Gaddafi's grandchildren",
                    "tokens": [
                        "if",
                        " Al",
                        "-",
                        "Arab",
                        " al",
                        "-",
                        "Q",
                        "ath",
                        "afi",
                        ",",
                        " where",
                        " three",
                        " of",
                        " Colonel",
                        " Gaddafi",
                        "'s",
                        " grandchildren"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "2 14 Lubbock +4 20 Denver +2 23 Portland +2 7 Birmingham",
                    "tokens": [
                        "2",
                        " 14",
                        " L",
                        "ubb",
                        "ock",
                        " +",
                        "4",
                        " 20",
                        " Denver",
                        " +",
                        "2",
                        " 23",
                        " Portland",
                        " +",
                        "2",
                        " 7",
                        " Birmingham"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " has to be levied, it should be levied on the Central and state governments and the",
                    "tokens": [
                        " has",
                        " to",
                        " be",
                        " levied",
                        ",",
                        " it",
                        " should",
                        " be",
                        " levied",
                        " on",
                        " the",
                        " Central",
                        " and",
                        " state",
                        " governments",
                        " and",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 41.45129013061523
        },
        {
            "feature_index": 19836,
            "gpt_predictions": [
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "mentions of the term \"Transatlantic\"",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "\u010a0:00 The three latest Intercontinental Ballistic Missile tests by North Korea. 00",
                    "max_token": "continental",
                    "tokens": [
                        "\u010a",
                        "0",
                        ":",
                        "00",
                        " The",
                        " three",
                        " latest",
                        " Inter",
                        "continental",
                        " Ballistic",
                        " Missile",
                        " tests",
                        " by",
                        " North",
                        " Korea",
                        ".",
                        " 00"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        7.053068161010742,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " cash and property<|endoftext|> busts on the interstate.\u010a\u010a\"It is disappointing when",
                    "max_token": " interstate",
                    "tokens": [
                        " cash",
                        " and",
                        " property",
                        "<|endoftext|>",
                        " bust",
                        "s",
                        " on",
                        " the",
                        " interstate",
                        ".",
                        "\u010a",
                        "\u010a",
                        "\"",
                        "It",
                        " is",
                        " disappointing",
                        " when"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        7.261377334594727,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " Davis RyanBrexit and exit: A transatlantic comparison Five takeaways from McCabe\u00e2\u0122\u013b",
                    "max_token": "atlantic",
                    "tokens": [
                        " Davis",
                        " Ryan",
                        "Brexit",
                        " and",
                        " exit",
                        ":",
                        " A",
                        " trans",
                        "atlantic",
                        " comparison",
                        " Five",
                        " take",
                        "aways",
                        " from",
                        " McCabe",
                        "\u00e2\u0122",
                        "\u013b"
                    ],
                    "values": [
                        0,
                        0,
                        3.713936805725098,
                        0,
                        0,
                        0,
                        0,
                        0,
                        40.92080688476562,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " interdiction unit, which deals with interstate crimes and is responsible for a large number",
                    "max_token": " interstate",
                    "tokens": [
                        " inter",
                        "d",
                        "iction",
                        " unit",
                        ",",
                        " which",
                        " deals",
                        " with",
                        " interstate",
                        " crimes",
                        " and",
                        " is",
                        " responsible",
                        " for",
                        " a",
                        " large",
                        " number"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        7.305352687835693,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " the editor or write to letters@theatlantic.com.<|endoftext|>Chinese Stock Market Pl",
                    "max_token": "atlantic",
                    "tokens": [
                        " the",
                        " editor",
                        " or",
                        " write",
                        " to",
                        " letters",
                        "@",
                        "the",
                        "atlantic",
                        ".",
                        "com",
                        ".",
                        "<|endoftext|>",
                        "Chinese",
                        " Stock",
                        " Market",
                        " Pl"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        38.66107559204102,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " most rich countries. But \u00e2\u0122\u013epremiumisation\u00e2\u0122\u013f is working. Though still",
                    "tokens": [
                        " most",
                        " rich",
                        " countries",
                        ".",
                        " But",
                        " \u00e2\u0122",
                        "\u013e",
                        "prem",
                        "ium",
                        "isation",
                        "\u00e2\u0122",
                        "\u013f",
                        " is",
                        " working",
                        ".",
                        " Though",
                        " still"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " outfit in the top 20. The Denver Nuggets reportedly spend \u00c2\u00a32,939,",
                    "tokens": [
                        " outfit",
                        " in",
                        " the",
                        " top",
                        " 20",
                        ".",
                        " The",
                        " Denver",
                        " Nuggets",
                        " reportedly",
                        " spend",
                        " \u00c2\u00a3",
                        "2",
                        ",",
                        "9",
                        "39",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "if Al-Arab al-Qathafi, where three of Colonel Gaddafi's grandchildren",
                    "tokens": [
                        "if",
                        " Al",
                        "-",
                        "Arab",
                        " al",
                        "-",
                        "Q",
                        "ath",
                        "afi",
                        ",",
                        " where",
                        " three",
                        " of",
                        " Colonel",
                        " Gaddafi",
                        "'s",
                        " grandchildren"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "2 14 Lubbock +4 20 Denver +2 23 Portland +2 7 Birmingham",
                    "tokens": [
                        "2",
                        " 14",
                        " L",
                        "ubb",
                        "ock",
                        " +",
                        "4",
                        " 20",
                        " Denver",
                        " +",
                        "2",
                        " 23",
                        " Portland",
                        " +",
                        "2",
                        " 7",
                        " Birmingham"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " has to be levied, it should be levied on the Central and state governments and the",
                    "tokens": [
                        " has",
                        " to",
                        " be",
                        " levied",
                        ",",
                        " it",
                        " should",
                        " be",
                        " levied",
                        " on",
                        " the",
                        " Central",
                        " and",
                        " state",
                        " governments",
                        " and",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 41.81784439086914
        },
        {
            "feature_index": 15823,
            "gpt_predictions": [
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "the surname \"Chow\"",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " cut ties with Russia-bound forward Vladimir Sobotka just yet.\u010a\u010aThe",
                    "max_token": " Sob",
                    "tokens": [
                        " cut",
                        " ties",
                        " with",
                        " Russia",
                        "-",
                        "bound",
                        " forward",
                        " Vladimir",
                        " Sob",
                        "ot",
                        "ka",
                        " just",
                        " yet",
                        ".",
                        "\u010a",
                        "\u010a",
                        "The"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        5.891452312469482,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "Up\u00e2\u0122\u013b? Co-host Lisa Chow speaks with Geektime\u010a\u010aFor Geek",
                    "max_token": " Chow",
                    "tokens": [
                        "Up",
                        "\u00e2\u0122",
                        "\u013b",
                        "?",
                        " Co",
                        "-",
                        "host",
                        " Lisa",
                        " Chow",
                        " speaks",
                        " with",
                        " Geek",
                        "time",
                        "\u010a",
                        "\u010a",
                        "For",
                        " Geek"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        27.63457298278809,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " full observation of the court,\u00e2\u0122\u013f Chowdhury said. \u00e2\u0122\u013eWe have",
                    "max_token": " Chow",
                    "tokens": [
                        " full",
                        " observation",
                        " of",
                        " the",
                        " court",
                        ",",
                        "\u00e2\u0122",
                        "\u013f",
                        " Chow",
                        "dh",
                        "ury",
                        " said",
                        ".",
                        " \u00e2\u0122",
                        "\u013e",
                        "We",
                        " have"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        26.48699569702148,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " Qasim Jaan in Chandni Chowk\u00e2\u0122\u013bs rundown neighbourhood of Ball",
                    "max_token": " Chow",
                    "tokens": [
                        " Q",
                        "as",
                        "im",
                        " Ja",
                        "an",
                        " in",
                        " Chand",
                        "ni",
                        " Chow",
                        "k",
                        "\u00e2\u0122",
                        "\u013b",
                        "s",
                        " rundown",
                        " neighbourhood",
                        " of",
                        " Ball"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        25.48556137084961,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "pt<|endoftext|> video.Police Commission President Steve Soboroff said the independent inspector general and",
                    "max_token": " Sob",
                    "tokens": [
                        "pt",
                        "<|endoftext|>",
                        " video",
                        ".",
                        "Police",
                        " Commission",
                        " President",
                        " Steve",
                        " Sob",
                        "or",
                        "off",
                        " said",
                        " the",
                        " independent",
                        " inspector",
                        " general",
                        " and"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        5.939386367797852,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " most rich countries. But \u00e2\u0122\u013epremiumisation\u00e2\u0122\u013f is working. Though still",
                    "tokens": [
                        " most",
                        " rich",
                        " countries",
                        ".",
                        " But",
                        " \u00e2\u0122",
                        "\u013e",
                        "prem",
                        "ium",
                        "isation",
                        "\u00e2\u0122",
                        "\u013f",
                        " is",
                        " working",
                        ".",
                        " Though",
                        " still"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " outfit in the top 20. The Denver Nuggets reportedly spend \u00c2\u00a32,939,",
                    "tokens": [
                        " outfit",
                        " in",
                        " the",
                        " top",
                        " 20",
                        ".",
                        " The",
                        " Denver",
                        " Nuggets",
                        " reportedly",
                        " spend",
                        " \u00c2\u00a3",
                        "2",
                        ",",
                        "9",
                        "39",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "if Al-Arab al-Qathafi, where three of Colonel Gaddafi's grandchildren",
                    "tokens": [
                        "if",
                        " Al",
                        "-",
                        "Arab",
                        " al",
                        "-",
                        "Q",
                        "ath",
                        "afi",
                        ",",
                        " where",
                        " three",
                        " of",
                        " Colonel",
                        " Gaddafi",
                        "'s",
                        " grandchildren"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "2 14 Lubbock +4 20 Denver +2 23 Portland +2 7 Birmingham",
                    "tokens": [
                        "2",
                        " 14",
                        " L",
                        "ubb",
                        "ock",
                        " +",
                        "4",
                        " 20",
                        " Denver",
                        " +",
                        "2",
                        " 23",
                        " Portland",
                        " +",
                        "2",
                        " 7",
                        " Birmingham"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " has to be levied, it should be levied on the Central and state governments and the",
                    "tokens": [
                        " has",
                        " to",
                        " be",
                        " levied",
                        ",",
                        " it",
                        " should",
                        " be",
                        " levied",
                        " on",
                        " the",
                        " Central",
                        " and",
                        " state",
                        " governments",
                        " and",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 28.54742240905762
        },
        {
            "feature_index": 4427,
            "gpt_predictions": [
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    1.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "instances of the abbreviation 'SB'",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " rugby has lost its dignity\u010a\u010a* SBW: No form dip for 'on",
                    "max_token": " SB",
                    "tokens": [
                        " rugby",
                        " has",
                        " lost",
                        " its",
                        " dignity",
                        "\u010a",
                        "\u010a",
                        "*",
                        " SB",
                        "W",
                        ":",
                        " No",
                        " form",
                        " dip",
                        " for",
                        " '",
                        "on"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        40.79538345336914,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "<|endoftext|>Bikes like the new Yeti SB5c pack a ton of technology,",
                    "max_token": " SB",
                    "tokens": [
                        "<|endoftext|>",
                        "B",
                        "ikes",
                        " like",
                        " the",
                        " new",
                        " Yet",
                        "i",
                        " SB",
                        "5",
                        "c",
                        " pack",
                        " a",
                        " ton",
                        " of",
                        " technology",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        39.97500610351562,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " NBC Sports president of programming Jon Miller told SB Nation.\u010a\u010aAmong the numerous ideas",
                    "max_token": " SB",
                    "tokens": [
                        " NBC",
                        " Sports",
                        " president",
                        " of",
                        " programming",
                        " Jon",
                        " Miller",
                        " told",
                        " SB",
                        " Nation",
                        ".",
                        "\u010a",
                        "\u010a",
                        "Among",
                        " the",
                        " numerous",
                        " ideas"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        41.41357421875,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " College Football Hall of Fame\u010a\u010a\u00e2\u0122\u00a2 SB Nation spends NFL Draft night with a first",
                    "max_token": " SB",
                    "tokens": [
                        " College",
                        " Football",
                        " Hall",
                        " of",
                        " Fame",
                        "\u010a",
                        "\u010a",
                        "\u00e2\u0122\u00a2",
                        " SB",
                        " Nation",
                        " spends",
                        " NFL",
                        " Draft",
                        " night",
                        " with",
                        " a",
                        " first"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        37.34194946289062,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "AMTHEdvb2dsZSBJbnRl [...]\u010a\u010aIf",
                    "max_token": "SB",
                    "tokens": [
                        "AM",
                        "TH",
                        "Ed",
                        "v",
                        "b",
                        "2",
                        "ds",
                        "Z",
                        "SB",
                        "J",
                        "bn",
                        "R",
                        "l",
                        " [...]",
                        "\u010a",
                        "\u010a",
                        "If"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        25.46121597290039,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " most rich countries. But \u00e2\u0122\u013epremiumisation\u00e2\u0122\u013f is working. Though still",
                    "tokens": [
                        " most",
                        " rich",
                        " countries",
                        ".",
                        " But",
                        " \u00e2\u0122",
                        "\u013e",
                        "prem",
                        "ium",
                        "isation",
                        "\u00e2\u0122",
                        "\u013f",
                        " is",
                        " working",
                        ".",
                        " Though",
                        " still"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " outfit in the top 20. The Denver Nuggets reportedly spend \u00c2\u00a32,939,",
                    "tokens": [
                        " outfit",
                        " in",
                        " the",
                        " top",
                        " 20",
                        ".",
                        " The",
                        " Denver",
                        " Nuggets",
                        " reportedly",
                        " spend",
                        " \u00c2\u00a3",
                        "2",
                        ",",
                        "9",
                        "39",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "if Al-Arab al-Qathafi, where three of Colonel Gaddafi's grandchildren",
                    "tokens": [
                        "if",
                        " Al",
                        "-",
                        "Arab",
                        " al",
                        "-",
                        "Q",
                        "ath",
                        "afi",
                        ",",
                        " where",
                        " three",
                        " of",
                        " Colonel",
                        " Gaddafi",
                        "'s",
                        " grandchildren"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "2 14 Lubbock +4 20 Denver +2 23 Portland +2 7 Birmingham",
                    "tokens": [
                        "2",
                        " 14",
                        " L",
                        "ubb",
                        "ock",
                        " +",
                        "4",
                        " 20",
                        " Denver",
                        " +",
                        "2",
                        " 23",
                        " Portland",
                        " +",
                        "2",
                        " 7",
                        " Birmingham"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " has to be levied, it should be levied on the Central and state governments and the",
                    "tokens": [
                        " has",
                        " to",
                        " be",
                        " levied",
                        ",",
                        " it",
                        " should",
                        " be",
                        " levied",
                        " on",
                        " the",
                        " Central",
                        " and",
                        " state",
                        " governments",
                        " and",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 42.57390213012695
        },
        {
            "feature_index": 18084,
            "gpt_predictions": [
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "time-related information, specifically total time measurements",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " Tasnim news agency.\u010a\u010aTotal was the first Western energy company to sign",
                    "max_token": "Total",
                    "tokens": [
                        " Tas",
                        "n",
                        "im",
                        " news",
                        " agency",
                        ".",
                        "\u010a",
                        "\u010a",
                        "Total",
                        " was",
                        " the",
                        " first",
                        " Western",
                        " energy",
                        " company",
                        " to",
                        " sign"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        19.54041290283203,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " what we\u00e2\u0122\u013bre doing with [Total War developer] Creative Assembly. Of course",
                    "max_token": "Total",
                    "tokens": [
                        " what",
                        " we",
                        "\u00e2\u0122",
                        "\u013b",
                        "re",
                        " doing",
                        " with",
                        " [",
                        "Total",
                        " War",
                        " developer",
                        "]",
                        " Creative",
                        " Assembly",
                        ".",
                        " Of",
                        " course"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        21.87173461914062,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " giants such as Exxon Mobil, Chevron, Total and Gasprom.\u010a\u010aK",
                    "max_token": " Total",
                    "tokens": [
                        " giants",
                        " such",
                        " as",
                        " Exxon",
                        " Mobil",
                        ",",
                        " Chevron",
                        ",",
                        " Total",
                        " and",
                        " G",
                        "asp",
                        "rom",
                        ".",
                        "\u010a",
                        "\u010a",
                        "K"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        21.53940391540527,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " salads and more. Prep Time 5 minutes Total Time 5 minutes Servings 12 Oun",
                    "max_token": " Total",
                    "tokens": [
                        " salads",
                        " and",
                        " more",
                        ".",
                        " Prep",
                        " Time",
                        " 5",
                        " minutes",
                        " Total",
                        " Time",
                        " 5",
                        " minutes",
                        " Serv",
                        "ings",
                        " 12",
                        " O",
                        "un"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        24.9442081451416,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "-book at $9.99. Total revenue at $14.99 would be",
                    "max_token": " Total",
                    "tokens": [
                        "-",
                        "book",
                        " at",
                        " $",
                        "9",
                        ".",
                        "99",
                        ".",
                        " Total",
                        " revenue",
                        " at",
                        " $",
                        "14",
                        ".",
                        "99",
                        " would",
                        " be"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        22.87176513671875,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " most rich countries. But \u00e2\u0122\u013epremiumisation\u00e2\u0122\u013f is working. Though still",
                    "tokens": [
                        " most",
                        " rich",
                        " countries",
                        ".",
                        " But",
                        " \u00e2\u0122",
                        "\u013e",
                        "prem",
                        "ium",
                        "isation",
                        "\u00e2\u0122",
                        "\u013f",
                        " is",
                        " working",
                        ".",
                        " Though",
                        " still"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " outfit in the top 20. The Denver Nuggets reportedly spend \u00c2\u00a32,939,",
                    "tokens": [
                        " outfit",
                        " in",
                        " the",
                        " top",
                        " 20",
                        ".",
                        " The",
                        " Denver",
                        " Nuggets",
                        " reportedly",
                        " spend",
                        " \u00c2\u00a3",
                        "2",
                        ",",
                        "9",
                        "39",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "if Al-Arab al-Qathafi, where three of Colonel Gaddafi's grandchildren",
                    "tokens": [
                        "if",
                        " Al",
                        "-",
                        "Arab",
                        " al",
                        "-",
                        "Q",
                        "ath",
                        "afi",
                        ",",
                        " where",
                        " three",
                        " of",
                        " Colonel",
                        " Gaddafi",
                        "'s",
                        " grandchildren"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "2 14 Lubbock +4 20 Denver +2 23 Portland +2 7 Birmingham",
                    "tokens": [
                        "2",
                        " 14",
                        " L",
                        "ubb",
                        "ock",
                        " +",
                        "4",
                        " 20",
                        " Denver",
                        " +",
                        "2",
                        " 23",
                        " Portland",
                        " +",
                        "2",
                        " 7",
                        " Birmingham"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " has to be levied, it should be levied on the Central and state governments and the",
                    "tokens": [
                        " has",
                        " to",
                        " be",
                        " levied",
                        ",",
                        " it",
                        " should",
                        " be",
                        " levied",
                        " on",
                        " the",
                        " Central",
                        " and",
                        " state",
                        " governments",
                        " and",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 26.81548690795898
        },
        {
            "feature_index": 20163,
            "gpt_predictions": [
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    1.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "the mention of specific names, especially related to sports and entertainment",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " no. It\u00e2\u0122\u013bs honest. Browne was notable as much for the<|endoftext|> world",
                    "max_token": " Browne",
                    "tokens": [
                        " no",
                        ".",
                        " It",
                        "\u00e2\u0122",
                        "\u013b",
                        "s",
                        " honest",
                        ".",
                        " Browne",
                        " was",
                        " notable",
                        " as",
                        " much",
                        " for",
                        " the",
                        "<|endoftext|>",
                        " world"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        16.55380821228027,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": ", and had never tapped out someone at Barnett's level.\u010a\u010aThe main take",
                    "max_token": " Barnett",
                    "tokens": [
                        ",",
                        " and",
                        " had",
                        " never",
                        " tapped",
                        " out",
                        " someone",
                        " at",
                        " Barnett",
                        "'s",
                        " level",
                        ".",
                        "\u010a",
                        "\u010a",
                        "The",
                        " main",
                        " take"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        21.12952423095703,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " puts it:\u010a\u010aSylvia Browne, a self-proclaimed psychic\u00e2\u0122\u00a6\u010a",
                    "max_token": " Browne",
                    "tokens": [
                        " puts",
                        " it",
                        ":",
                        "\u010a",
                        "\u010a",
                        "S",
                        "yl",
                        "via",
                        " Browne",
                        ",",
                        " a",
                        " self",
                        "-",
                        "proclaimed",
                        " psychic",
                        "\u00e2\u0122\u00a6",
                        "\u010a"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        16.26649475097656,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " polo match between Greenwich High School and Brunswick School at Greenwich High School, Conn.,",
                    "max_token": " Brunswick",
                    "tokens": [
                        " pol",
                        "o",
                        " match",
                        " between",
                        " Greenwich",
                        " High",
                        " School",
                        " and",
                        " Brunswick",
                        " School",
                        " at",
                        " Greenwich",
                        " High",
                        " School",
                        ",",
                        " Conn",
                        ".,"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        3.593460321426392,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": ".\u010a\u010aInstead, police say, Norris took the animal itself \u00e2\u0122\u0136 but neither the",
                    "max_token": " Norris",
                    "tokens": [
                        ".",
                        "\u010a",
                        "\u010a",
                        "Instead",
                        ",",
                        " police",
                        " say",
                        ",",
                        " Norris",
                        " took",
                        " the",
                        " animal",
                        " itself",
                        " \u00e2\u0122\u0136",
                        " but",
                        " neither",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        7.564048767089844,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " most rich countries. But \u00e2\u0122\u013epremiumisation\u00e2\u0122\u013f is working. Though still",
                    "tokens": [
                        " most",
                        " rich",
                        " countries",
                        ".",
                        " But",
                        " \u00e2\u0122",
                        "\u013e",
                        "prem",
                        "ium",
                        "isation",
                        "\u00e2\u0122",
                        "\u013f",
                        " is",
                        " working",
                        ".",
                        " Though",
                        " still"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " outfit in the top 20. The Denver Nuggets reportedly spend \u00c2\u00a32,939,",
                    "tokens": [
                        " outfit",
                        " in",
                        " the",
                        " top",
                        " 20",
                        ".",
                        " The",
                        " Denver",
                        " Nuggets",
                        " reportedly",
                        " spend",
                        " \u00c2\u00a3",
                        "2",
                        ",",
                        "9",
                        "39",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "if Al-Arab al-Qathafi, where three of Colonel Gaddafi's grandchildren",
                    "tokens": [
                        "if",
                        " Al",
                        "-",
                        "Arab",
                        " al",
                        "-",
                        "Q",
                        "ath",
                        "afi",
                        ",",
                        " where",
                        " three",
                        " of",
                        " Colonel",
                        " Gaddafi",
                        "'s",
                        " grandchildren"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "2 14 Lubbock +4 20 Denver +2 23 Portland +2 7 Birmingham",
                    "tokens": [
                        "2",
                        " 14",
                        " L",
                        "ubb",
                        "ock",
                        " +",
                        "4",
                        " 20",
                        " Denver",
                        " +",
                        "2",
                        " 23",
                        " Portland",
                        " +",
                        "2",
                        " 7",
                        " Birmingham"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " has to be levied, it should be levied on the Central and state governments and the",
                    "tokens": [
                        " has",
                        " to",
                        " be",
                        " levied",
                        ",",
                        " it",
                        " should",
                        " be",
                        " levied",
                        " on",
                        " the",
                        " Central",
                        " and",
                        " state",
                        " governments",
                        " and",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 21.6140308380127
        },
        {
            "feature_index": 22548,
            "gpt_predictions": [
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "common English pronouns",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " Sword of the Chevalier and Doctor Who - Cold Vengeance - can be bought individually",
                    "max_token": " Who",
                    "tokens": [
                        " Sword",
                        " of",
                        " the",
                        " Che",
                        "val",
                        "ier",
                        " and",
                        " Doctor",
                        " Who",
                        " -",
                        " Cold",
                        " Vengeance",
                        " -",
                        " can",
                        " be",
                        " bought",
                        " individually"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        18.86075782775879,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "\u00e2\u0122\u013f\u010a\u010a\u00e2\u0122\u013eComing. Who is it\u00e2\u0122\u013f.\u00e2\u0122\u013f\u010a",
                    "max_token": " Who",
                    "tokens": [
                        "\u00e2\u0122",
                        "\u013f",
                        "\u010a",
                        "\u010a",
                        "\u00e2\u0122",
                        "\u013e",
                        "Coming",
                        ".",
                        " Who",
                        " is",
                        " it",
                        "\u00e2\u0122",
                        "\u013f",
                        ".",
                        "\u00e2\u0122",
                        "\u013f",
                        "\u010a"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        29.19223594665527,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " that noise? Hmm\u00e2\u0122\u00a6\u010a\u010aHey who\u00e2\u0122\u013bs footprints are these? Oh",
                    "max_token": " who",
                    "tokens": [
                        " that",
                        " noise",
                        "?",
                        " Hmm",
                        "\u00e2\u0122\u00a6",
                        "\u010a",
                        "\u010a",
                        "Hey",
                        " who",
                        "\u00e2\u0122",
                        "\u013b",
                        "s",
                        " footprints",
                        " are",
                        " these",
                        "?",
                        " Oh"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        7.062452793121338,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " smokers that you will find.\u010a\u010aWho Is This Guide For?\u010a\u010aThis",
                    "max_token": "Who",
                    "tokens": [
                        " smokers",
                        " that",
                        " you",
                        " will",
                        " find",
                        ".",
                        "\u010a",
                        "\u010a",
                        "Who",
                        " Is",
                        " This",
                        " Guide",
                        " For",
                        "?",
                        "\u010a",
                        "\u010a",
                        "This"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        32.13818359375,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " these features at any time.\u010a\u010aWho are the third parties mentioned by Samsung and",
                    "max_token": "Who",
                    "tokens": [
                        " these",
                        " features",
                        " at",
                        " any",
                        " time",
                        ".",
                        "\u010a",
                        "\u010a",
                        "Who",
                        " are",
                        " the",
                        " third",
                        " parties",
                        " mentioned",
                        " by",
                        " Samsung",
                        " and"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        31.57158470153809,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " most rich countries. But \u00e2\u0122\u013epremiumisation\u00e2\u0122\u013f is working. Though still",
                    "tokens": [
                        " most",
                        " rich",
                        " countries",
                        ".",
                        " But",
                        " \u00e2\u0122",
                        "\u013e",
                        "prem",
                        "ium",
                        "isation",
                        "\u00e2\u0122",
                        "\u013f",
                        " is",
                        " working",
                        ".",
                        " Though",
                        " still"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " outfit in the top 20. The Denver Nuggets reportedly spend \u00c2\u00a32,939,",
                    "tokens": [
                        " outfit",
                        " in",
                        " the",
                        " top",
                        " 20",
                        ".",
                        " The",
                        " Denver",
                        " Nuggets",
                        " reportedly",
                        " spend",
                        " \u00c2\u00a3",
                        "2",
                        ",",
                        "9",
                        "39",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "if Al-Arab al-Qathafi, where three of Colonel Gaddafi's grandchildren",
                    "tokens": [
                        "if",
                        " Al",
                        "-",
                        "Arab",
                        " al",
                        "-",
                        "Q",
                        "ath",
                        "afi",
                        ",",
                        " where",
                        " three",
                        " of",
                        " Colonel",
                        " Gaddafi",
                        "'s",
                        " grandchildren"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "2 14 Lubbock +4 20 Denver +2 23 Portland +2 7 Birmingham",
                    "tokens": [
                        "2",
                        " 14",
                        " L",
                        "ubb",
                        "ock",
                        " +",
                        "4",
                        " 20",
                        " Denver",
                        " +",
                        "2",
                        " 23",
                        " Portland",
                        " +",
                        "2",
                        " 7",
                        " Birmingham"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " has to be levied, it should be levied on the Central and state governments and the",
                    "tokens": [
                        " has",
                        " to",
                        " be",
                        " levied",
                        ",",
                        " it",
                        " should",
                        " be",
                        " levied",
                        " on",
                        " the",
                        " Central",
                        " and",
                        " state",
                        " governments",
                        " and",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 33.35800933837891
        },
        {
            "feature_index": 15632,
            "gpt_predictions": [
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    1.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "references to financial terms related to money and transactions",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " and spend their cryptocurrency wealth in a still fiat-dominated world.\u010a\u010aIts founding",
                    "max_token": " fiat",
                    "tokens": [
                        " and",
                        " spend",
                        " their",
                        " cryptocurrency",
                        " wealth",
                        " in",
                        " a",
                        " still",
                        " fiat",
                        "-",
                        "dominated",
                        " world",
                        ".",
                        "\u010a",
                        "\u010a",
                        "Its",
                        " founding"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        8.506644248962402,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "IO has successfully completed the full circle of fiat money flows to and from Visa and Master",
                    "max_token": " fiat",
                    "tokens": [
                        "IO",
                        " has",
                        " successfully",
                        " completed",
                        " the",
                        " full",
                        " circle",
                        " of",
                        " fiat",
                        " money",
                        " flows",
                        " to",
                        " and",
                        " from",
                        " Visa",
                        " and",
                        " Master"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        9.060073852539062,
                        0,
                        2.104674816131592,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " capital through a digital token, investors get liquidity, transparency, clear view on value and",
                    "max_token": " liquidity",
                    "tokens": [
                        " capital",
                        " through",
                        " a",
                        " digital",
                        " token",
                        ",",
                        " investors",
                        " get",
                        " liquidity",
                        ",",
                        " transparency",
                        ",",
                        " clear",
                        " view",
                        " on",
                        " value",
                        " and"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        42.56310653686523,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " business, either through direct control or by fiat.\u010a\u010aDon\u00e2\u0122\u013bt take",
                    "max_token": " fiat",
                    "tokens": [
                        " business",
                        ",",
                        " either",
                        " through",
                        " direct",
                        " control",
                        " or",
                        " by",
                        " fiat",
                        ".",
                        "\u010a",
                        "\u010a",
                        "Don",
                        "\u00e2\u0122",
                        "\u013b",
                        "t",
                        " take"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        9.405376434326172,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " the bitcoin exchange can now make withdrawals of fiat currencies directly to their payment cards; Visa",
                    "max_token": " fiat",
                    "tokens": [
                        " the",
                        " bitcoin",
                        " exchange",
                        " can",
                        " now",
                        " make",
                        " withdrawals",
                        " of",
                        " fiat",
                        " currencies",
                        " directly",
                        " to",
                        " their",
                        " payment",
                        " cards",
                        ";",
                        " Visa"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        9.706098556518555,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " most rich countries. But \u00e2\u0122\u013epremiumisation\u00e2\u0122\u013f is working. Though still",
                    "tokens": [
                        " most",
                        " rich",
                        " countries",
                        ".",
                        " But",
                        " \u00e2\u0122",
                        "\u013e",
                        "prem",
                        "ium",
                        "isation",
                        "\u00e2\u0122",
                        "\u013f",
                        " is",
                        " working",
                        ".",
                        " Though",
                        " still"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " outfit in the top 20. The Denver Nuggets reportedly spend \u00c2\u00a32,939,",
                    "tokens": [
                        " outfit",
                        " in",
                        " the",
                        " top",
                        " 20",
                        ".",
                        " The",
                        " Denver",
                        " Nuggets",
                        " reportedly",
                        " spend",
                        " \u00c2\u00a3",
                        "2",
                        ",",
                        "9",
                        "39",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "if Al-Arab al-Qathafi, where three of Colonel Gaddafi's grandchildren",
                    "tokens": [
                        "if",
                        " Al",
                        "-",
                        "Arab",
                        " al",
                        "-",
                        "Q",
                        "ath",
                        "afi",
                        ",",
                        " where",
                        " three",
                        " of",
                        " Colonel",
                        " Gaddafi",
                        "'s",
                        " grandchildren"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "2 14 Lubbock +4 20 Denver +2 23 Portland +2 7 Birmingham",
                    "tokens": [
                        "2",
                        " 14",
                        " L",
                        "ubb",
                        "ock",
                        " +",
                        "4",
                        " 20",
                        " Denver",
                        " +",
                        "2",
                        " 23",
                        " Portland",
                        " +",
                        "2",
                        " 7",
                        " Birmingham"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " has to be levied, it should be levied on the Central and state governments and the",
                    "tokens": [
                        " has",
                        " to",
                        " be",
                        " levied",
                        ",",
                        " it",
                        " should",
                        " be",
                        " levied",
                        " on",
                        " the",
                        " Central",
                        " and",
                        " state",
                        " governments",
                        " and",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 46.32057571411133
        },
        {
            "feature_index": 9089,
            "gpt_predictions": [
                [
                    1,
                    0.0
                ],
                [
                    1,
                    1.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "phrases related to music charts and award nominations",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " like nothing less than a \u00e2\u0122\u013aBritish Melody Festival\u00e2\u0122\u013a will satisfy them). If",
                    "max_token": " Melody",
                    "tokens": [
                        " like",
                        " nothing",
                        " less",
                        " than",
                        " a",
                        " \u00e2\u0122",
                        "\u013a",
                        "British",
                        " Melody",
                        " Festival",
                        "\u00e2\u0122",
                        "\u013a",
                        " will",
                        " satisfy",
                        " them",
                        ").",
                        " If"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        5.428154945373535,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " climbed as high as number 33 on the Billboard Country charts.\u010a\u010aThe song broke",
                    "max_token": " Billboard",
                    "tokens": [
                        " climbed",
                        " as",
                        " high",
                        " as",
                        " number",
                        " 33",
                        " on",
                        " the",
                        " Billboard",
                        " Country",
                        " charts",
                        ".",
                        "\u010a",
                        "\u010a",
                        "The",
                        " song",
                        " broke"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        46.26802444458008,
                        0,
                        4.00025749206543,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " upward price momentum wanes, according to BMI Research, a unit of Fitch Group",
                    "max_token": " BMI",
                    "tokens": [
                        " upward",
                        " price",
                        " momentum",
                        " w",
                        "anes",
                        ",",
                        " according",
                        " to",
                        " BMI",
                        " Research",
                        ",",
                        " a",
                        " unit",
                        " of",
                        " F",
                        "itch",
                        " Group"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        7.050563812255859,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " real problem.\"\u010a\u010aThe rapper told Billboard he has several issues with the new president",
                    "max_token": " Billboard",
                    "tokens": [
                        " real",
                        " problem",
                        ".\"",
                        "\u010a",
                        "\u010a",
                        "The",
                        " rapper",
                        " told",
                        " Billboard",
                        " he",
                        " has",
                        " several",
                        " issues",
                        " with",
                        " the",
                        " new",
                        " president"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        46.39287185668945,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " a recent interview with Danielle Bacher of Billboard magazine, Cho, 46, dropped a",
                    "max_token": " Billboard",
                    "tokens": [
                        " a",
                        " recent",
                        " interview",
                        " with",
                        " Danielle",
                        " B",
                        "acher",
                        " of",
                        " Billboard",
                        " magazine",
                        ",",
                        " Cho",
                        ",",
                        " 46",
                        ",",
                        " dropped",
                        " a"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        44.11013412475586,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " most rich countries. But \u00e2\u0122\u013epremiumisation\u00e2\u0122\u013f is working. Though still",
                    "tokens": [
                        " most",
                        " rich",
                        " countries",
                        ".",
                        " But",
                        " \u00e2\u0122",
                        "\u013e",
                        "prem",
                        "ium",
                        "isation",
                        "\u00e2\u0122",
                        "\u013f",
                        " is",
                        " working",
                        ".",
                        " Though",
                        " still"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " outfit in the top 20. The Denver Nuggets reportedly spend \u00c2\u00a32,939,",
                    "tokens": [
                        " outfit",
                        " in",
                        " the",
                        " top",
                        " 20",
                        ".",
                        " The",
                        " Denver",
                        " Nuggets",
                        " reportedly",
                        " spend",
                        " \u00c2\u00a3",
                        "2",
                        ",",
                        "9",
                        "39",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "if Al-Arab al-Qathafi, where three of Colonel Gaddafi's grandchildren",
                    "tokens": [
                        "if",
                        " Al",
                        "-",
                        "Arab",
                        " al",
                        "-",
                        "Q",
                        "ath",
                        "afi",
                        ",",
                        " where",
                        " three",
                        " of",
                        " Colonel",
                        " Gaddafi",
                        "'s",
                        " grandchildren"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "2 14 Lubbock +4 20 Denver +2 23 Portland +2 7 Birmingham",
                    "tokens": [
                        "2",
                        " 14",
                        " L",
                        "ubb",
                        "ock",
                        " +",
                        "4",
                        " 20",
                        " Denver",
                        " +",
                        "2",
                        " 23",
                        " Portland",
                        " +",
                        "2",
                        " 7",
                        " Birmingham"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " has to be levied, it should be levied on the Central and state governments and the",
                    "tokens": [
                        " has",
                        " to",
                        " be",
                        " levied",
                        ",",
                        " it",
                        " should",
                        " be",
                        " levied",
                        " on",
                        " the",
                        " Central",
                        " and",
                        " state",
                        " governments",
                        " and",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 48.72696685791016
        },
        {
            "feature_index": 8194,
            "gpt_predictions": [
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    1,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ],
                [
                    0,
                    0.0
                ]
            ],
            "description": "words related to explosives or powerful forces",
            "test_sentences": [
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " inquiry.\u010a\u010a\u00e2\u0122\u013e#ExxonKnew the truth about fossil fuels and",
                    "max_token": "xon",
                    "tokens": [
                        " inquiry",
                        ".",
                        "\u010a",
                        "\u010a",
                        "\u00e2\u0122",
                        "\u013e",
                        "#",
                        "Ex",
                        "xon",
                        "K",
                        "new",
                        " the",
                        " truth",
                        " about",
                        " fossil",
                        " fuels",
                        " and"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        4.496387004852295,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": "\u013bs match this Saturday against the Houston Dynamo, Minnesota United has announced it has loan",
                    "max_token": " Dynamo",
                    "tokens": [
                        "\u013b",
                        "s",
                        " match",
                        " this",
                        " Saturday",
                        " against",
                        " the",
                        " Houston",
                        " Dynamo",
                        ",",
                        " Minnesota",
                        " United",
                        " has",
                        " announced",
                        " it",
                        " has",
                        " loan"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        11.35913562774658,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " friend, BYU alum and director of Napoleon Dynamite Jared Hess to produce a music video",
                    "max_token": " Dynam",
                    "tokens": [
                        " friend",
                        ",",
                        " BYU",
                        " alum",
                        " and",
                        " director",
                        " of",
                        " Napoleon",
                        " Dynam",
                        "ite",
                        " Jared",
                        " Hess",
                        " to",
                        " produce",
                        " a",
                        " music",
                        " video"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        27.02700996398926,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " in King B\u00c3\u00a9la\u00e2\u0122\u013bs dynastic policy. He wrote in a letter",
                    "max_token": " dyn",
                    "tokens": [
                        " in",
                        " King",
                        " B",
                        "\u00c3\u00a9",
                        "la",
                        "\u00e2\u0122",
                        "\u013b",
                        "s",
                        " dyn",
                        "astic",
                        " policy",
                        ".",
                        " He",
                        " wrote",
                        " in",
                        " a",
                        " letter"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        7.791844844818115,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "high",
                    "max_value_token_index": 8,
                    "sentence_string": " watch teams' fortunes rise and fall as dynasties emerge, crumble, and eventually",
                    "max_token": " dyn",
                    "tokens": [
                        " watch",
                        " teams",
                        "'",
                        " fortunes",
                        " rise",
                        " and",
                        " fall",
                        " as",
                        " dyn",
                        "ast",
                        "ies",
                        " emerge",
                        ",",
                        " crumble",
                        ",",
                        " and",
                        " eventually"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        8.660067558288574,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " most rich countries. But \u00e2\u0122\u013epremiumisation\u00e2\u0122\u013f is working. Though still",
                    "tokens": [
                        " most",
                        " rich",
                        " countries",
                        ".",
                        " But",
                        " \u00e2\u0122",
                        "\u013e",
                        "prem",
                        "ium",
                        "isation",
                        "\u00e2\u0122",
                        "\u013f",
                        " is",
                        " working",
                        ".",
                        " Though",
                        " still"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " outfit in the top 20. The Denver Nuggets reportedly spend \u00c2\u00a32,939,",
                    "tokens": [
                        " outfit",
                        " in",
                        " the",
                        " top",
                        " 20",
                        ".",
                        " The",
                        " Denver",
                        " Nuggets",
                        " reportedly",
                        " spend",
                        " \u00c2\u00a3",
                        "2",
                        ",",
                        "9",
                        "39",
                        ","
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "if Al-Arab al-Qathafi, where three of Colonel Gaddafi's grandchildren",
                    "tokens": [
                        "if",
                        " Al",
                        "-",
                        "Arab",
                        " al",
                        "-",
                        "Q",
                        "ath",
                        "afi",
                        ",",
                        " where",
                        " three",
                        " of",
                        " Colonel",
                        " Gaddafi",
                        "'s",
                        " grandchildren"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": "2 14 Lubbock +4 20 Denver +2 23 Portland +2 7 Birmingham",
                    "tokens": [
                        "2",
                        " 14",
                        " L",
                        "ubb",
                        "ock",
                        " +",
                        "4",
                        " 20",
                        " Denver",
                        " +",
                        "2",
                        " 23",
                        " Portland",
                        " +",
                        "2",
                        " 7",
                        " Birmingham"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                },
                {
                    "max_value": "low",
                    "sentence_string": " has to be levied, it should be levied on the Central and state governments and the",
                    "tokens": [
                        " has",
                        " to",
                        " be",
                        " levied",
                        ",",
                        " it",
                        " should",
                        " be",
                        " levied",
                        " on",
                        " the",
                        " Central",
                        " and",
                        " state",
                        " governments",
                        " and",
                        " the"
                    ],
                    "values": [
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0,
                        0
                    ]
                }
            ],
            "show_sentences": [],
            "highest_activation": 42.99213409423828
        }
    ],
    "timestamp": 1715492451.978493
}